# -*- coding: utf-8 -*-
import os
import time
import scrapy
import requests
from datetime import datetime, timedelta
from urllib.parse import urlencode


class ArxivSpider(scrapy.Spider):
    name = "arxiv_weekly"

    def wait_until_data_ready(self):
        """
        ç­‰å¾… arXiv ä¸Šå‘¨æ•°æ®æ›´æ–°å®Œæˆã€‚
        æ£€æŸ¥é€»è¾‘ï¼šæ£€æµ‹ä¸Šå‘¨æ—¥çš„æ•°æ®æ˜¯å¦å·²å¯ç”¨ã€‚
        """
        check_interval = int(os.environ.get("CHECK_INTERVAL", 1800))  # é»˜è®¤30åˆ†é’Ÿ
        max_retry = int(os.environ.get("MAX_RETRY", 12))  # é»˜è®¤6å°æ—¶å†…å°è¯•12æ¬¡
        base_url = "https://export.arxiv.org/api/query?"

        for attempt in range(1, max_retry + 1):
            if self.is_last_week_data_ready(base_url):
                self.logger.info("âœ… ä¸Šå‘¨æ•°æ®å·²æ›´æ–°ï¼Œå¼€å§‹çˆ¬å–ã€‚")
                return True
            else:
                self.logger.warning(f"âš ï¸ ç¬¬ {attempt} æ¬¡æ£€æµ‹ï¼šä¸Šå‘¨æ•°æ®å°šæœªæ›´æ–°ï¼Œ{check_interval/60:.0f} åˆ†é’Ÿåé‡è¯•...")
                time.sleep(check_interval)

        self.logger.error("âŒ è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä¸Šå‘¨æ•°æ®ä»æœªæ›´æ–°ã€‚ç»ˆæ­¢çˆ¬å–ã€‚")
        return False

    def is_last_week_data_ready(self, base_url: str) -> bool:
        """
        æ£€æŸ¥ä¸Šå‘¨æ•°æ®æ˜¯å¦æ›´æ–°å®Œæ¯•ï¼ˆä¸Šå‘¨æ—¥çš„æ•°æ®æ˜¯å¦å¯ç”¨ï¼‰ã€‚
        """
        last_sunday = (datetime.utcnow().date() - timedelta(days=datetime.utcnow().weekday() + 1))
        params = {
            "search_query": "all",
            "sortBy": "submittedDate",
            "sortOrder": "descending",
            "max_results": 5,
        }
        try:
            url = base_url + urlencode(params)
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
            if str(last_sunday) in resp.text:
                self.logger.info(f"æ£€æµ‹åˆ°ä¸Šå‘¨æ—¥ ({last_sunday}) å‡ºç°åœ¨æœ€æ–°ç»“æœä¸­ï¼Œæ•°æ®å·²æ›´æ–°ã€‚")
                return True
            return False
        except Exception as e:
            self.logger.warning(f"æ£€æµ‹ä¸Šå‘¨æ•°æ®å¤±è´¥ï¼š{e}")
            return False

    def start_requests(self):
        """
        æ¯å‘¨ä¸€è¿è¡Œï¼Œè‡ªåŠ¨çˆ¬å–ä¸Šå‘¨ä¸€ï½ä¸Šå‘¨æ—¥çš„æ•°æ®ã€‚
        """
        # === ç­‰å¾…ä¸Šå‘¨æ•°æ®æ›´æ–° ===
        if not self.wait_until_data_ready():
            return  # æ•°æ®æœªå‡†å¤‡å¥½åˆ™é€€å‡º

        # === è·å–å…³é”®è¯ ===
        keywords_str = os.environ.get('KEYWORDS')
        if not keywords_str:
            self.logger.error("é”™è¯¯ï¼šæœªè®¾ç½® KEYWORDS ç¯å¢ƒå˜é‡ï¼ˆä¾‹å¦‚ï¼š'Robotics,Model Quantization'ï¼‰")
            return

        keywords = [k.strip().strip('"').strip("'") for k in keywords_str.split(',') if k.strip()]
        if not keywords:
            self.logger.error("é”™è¯¯ï¼šKEYWORDS ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ã€‚")
            return

        self.logger.info(f"æˆåŠŸåŠ è½½å…³é”®è¯: {keywords}")

        # === æ„å»ºæœç´¢æŸ¥è¯¢ ===
        query_parts = [f'(ti:"{k}" OR abs:"{k}")' for k in keywords]
        self.search_query = ' OR '.join(query_parts)

        # === è®¾ç½®ä¸Šå‘¨æ—¶é—´èŒƒå›´ ===
        today = datetime.utcnow().date()
        self.start_date = today - timedelta(days=today.weekday() + 7)   # ä¸Šå‘¨ä¸€
        self.end_date = self.start_date + timedelta(days=6)             # ä¸Šå‘¨æ—¥
        self.logger.info(f"ğŸ“… å°†çˆ¬å–ä¸Šå‘¨ ({self.start_date} ~ {self.end_date}) çš„è®ºæ–‡")

        # === å‚æ•°åˆå§‹åŒ– ===
        self.base_url = "https://export.arxiv.org/api/query?"
        self.per_page = int(os.environ.get('PER_PAGE', 200))
        self.max_pages = int(os.environ.get('MAX_PAGES', 10))
        self.date_field = os.environ.get('DATE_FIELD', 'published').strip().lower()
        if self.date_field not in ('published', 'updated'):
            self.logger.warning("DATE_FIELD éé¢„æœŸï¼Œæ”¹å› 'published'")
            self.date_field = 'published'

        # === èµ·å§‹è¯·æ±‚ ===
        params = {
            'search_query': self.search_query,
            'start': 0,
            'max_results': self.per_page,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        url = self.base_url + urlencode(params)
        yield scrapy.Request(url, callback=self.parse_api_response, meta={'start': 0, 'page': 1})

    def parse_api_response(self, response):
        """
        è§£ææ¯é¡µçš„ API è¿”å›ï¼Œè¿‡æ»¤å‡ºä¸Šå‘¨çš„è®ºæ–‡ã€‚
        """
        start = response.meta.get('start', 0)
        page = response.meta.get('page', 1)

        entries = response.xpath('//*[local-name()="entry"]')
        if not entries:
            self.logger.warning(f"ç¬¬ {page} é¡µæœªè¿”å› entryï¼Œå¯èƒ½ API å“åº”ç»“æ„æ”¹å˜æˆ–å…³é”®è¯æ— åŒ¹é…ã€‚")
            return

        found_in_page = 0
        stop_paging = False

        for entry in entries:
            date_text = entry.xpath(f'*[local-name()="{self.date_field}"]/text()').get()
            if not date_text:
                continue

            try:
                pub_dt = datetime.fromisoformat(date_text.replace('Z', '+00:00'))
            except Exception:
                continue
            pub_date = pub_dt.date()

            if pub_date > self.end_date:
                continue  # ä»æ˜¯æœ¬å‘¨æˆ–æœªæ¥æ•°æ®
            elif self.start_date <= pub_date <= self.end_date:
                found_in_page += 1
                arxiv_id_url = entry.xpath('*[local-name()="id"]/text()').get() or ''
                arxiv_id = arxiv_id_url.split('/')[-1] if arxiv_id_url else None
                title = entry.xpath('*[local-name()="title"]/text()').get()
                summary = entry.xpath('*[local-name()="summary"]/text()').get()
                authors = entry.xpath('*[local-name()="author"]/*[local-name()="name"]/text()').getall()
                pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf" if arxiv_id else None

                yield {
                    "id": arxiv_id,
                    "title": title.strip() if title else None,
                    "summary": summary.strip() if summary else None,
                    "authors": authors,
                    self.date_field: date_text,
                    "pdf": pdf_url,
                }
            else:
                # æ—©äºä¸Šå‘¨æ•°æ®ï¼Œåœæ­¢ç¿»é¡µ
                stop_paging = True
                break

        self.logger.info(f"ç¬¬ {page} é¡µæ‰¾åˆ° {found_in_page} æ¡ä¸Šå‘¨è®ºæ–‡")

        # ç¿»é¡µé€»è¾‘
        if not stop_paging and len(entries) == self.per_page:
            next_start = start + self.per_page
            next_page = page + 1
            if next_page > self.max_pages:
                self.logger.warning(f"å·²åˆ°è¾¾ max_pages ({self.max_pages})ï¼Œåœæ­¢ç¿»é¡µã€‚")
                return

            params = {
                'search_query': self.search_query,
                'start': next_start,
                'max_results': self.per_page,
                'sortBy': 'submittedDate',
                'sortOrder': 'descending'
            }
            next_url = self.base_url + urlencode(params)
            self.logger.info(f"ç»§ç»­ç¿»é¡µï¼šè¯·æ±‚ start={next_start} (page {next_page})")
            yield scrapy.Request(next_url, callback=self.parse_api_response, meta={'start': next_start, 'page': next_page})
        else:
            if stop_paging:
                self.logger.info(f"ğŸ“š å·²é‡åˆ°æ—©äº {self.start_date} çš„æ¡ç›®ï¼Œåœæ­¢ç¿»é¡µã€‚")
            else:
                self.logger.info("ğŸ›‘ å·²åˆ°è¾¾ç»“æœæœ«å°¾ï¼Œåœæ­¢ç¿»é¡µã€‚")
