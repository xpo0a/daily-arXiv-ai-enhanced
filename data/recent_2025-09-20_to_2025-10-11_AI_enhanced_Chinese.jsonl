{"id": "2510.08572v1", "title": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation", "summary": "Scaling data and models has played a pivotal role in the remarkable progress\nof computer vision and language. Inspired by these domains, recent efforts in\nrobotics have similarly focused on scaling both data and model size to develop\nmore generalizable and robust policies. However, unlike vision and language,\nrobotics lacks access to internet-scale demonstrations across diverse robotic\ntasks and environments. As a result, the scale of existing datasets typically\nsuffers from the need for manual data collection and curation. To address this\nproblem, here we propose BLAZER, a framework that learns manipulation policies\nfrom automatically generated training data. We build on the zero-shot\ncapabilities of LLM planners and automatically generate demonstrations for\ndiverse manipulation tasks in simulation. Successful examples are then used to\nfinetune an LLM and to improve its planning capabilities without human\nsupervision. Notably, while BLAZER training requires access to the simulator's\nstate, we demonstrate direct transfer of acquired skills to sensor-based\nmanipulation. Through extensive experiments, we show BLAZER to significantly\nimprove zero-shot manipulation in both simulated and real environments.\nMoreover, BLAZER improves on tasks outside of its training pool and enables\ndownscaling of LLM models. Our code and data will be made publicly available on\nthe project page.", "authors": ["Rocktim Jyoti Das", "Harsh Singh", "Diana Turmakhan", "Muhammad Abdullah Sohail", "Mingfei Han", "Preslav Nakov", "Fabio Pizzati", "Ivan Laptev"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-10-09T17:59:58Z", "pdf": "https://arxiv.org/pdf/2510.08572v1", "abs": "https://arxiv.org/abs/2510.08572v1", "comment": "11 pages, 8 figures", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08571v1", "title": "Scalable Offline Metrics for Autonomous Driving", "summary": "Real-World evaluation of perception-based planning models for robotic\nsystems, such as autonomous vehicles, can be safely and inexpensively conducted\noffline, i.e., by computing model prediction error over a pre-collected\nvalidation dataset with ground-truth annotations. However, extrapolating from\noffline model performance to online settings remains a challenge. In these\nsettings, seemingly minor errors can compound and result in test-time\ninfractions or collisions. This relationship is understudied, particularly\nacross diverse closed-loop metrics and complex urban maneuvers. In this work,\nwe revisit this undervalued question in policy evaluation through an extensive\nset of experiments across diverse conditions and metrics. Based on analysis in\nsimulation, we find an even worse correlation between offline and online\nsettings than reported by prior studies, casting doubts on the validity of\ncurrent evaluation practices and metrics for driving policies. Next, we bridge\nthe gap between offline and online evaluation. We investigate an offline metric\nbased on epistemic uncertainty, which aims to capture events that are likely to\ncause errors in closed-loop settings. The resulting metric achieves over 13%\nimprovement in correlation compared to previous offline metrics. We further\nvalidate the generalization of our findings beyond the simulation environment\nin real-world settings, where even greater gains are observed.", "authors": ["Animikh Aich", "Adwait Kulkarni", "Eshed Ohn-Bar"], "categories": ["cs.RO", "cs.CV"], "published": "2025-10-09T17:59:57Z", "pdf": "https://arxiv.org/pdf/2510.08571v1", "abs": "https://arxiv.org/abs/2510.08571v1", "comment": "Accepted at IROS 2025 (IEEE/RSJ International Conference on\n  Intelligent Robots and Systems)", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08568v1", "title": "NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos", "summary": "Enabling robots to execute novel manipulation tasks zero-shot is a central\ngoal in robotics. Most existing methods assume in-distribution tasks or rely on\nfine-tuning with embodiment-matched data, limiting transfer across platforms.\nWe present NovaFlow, an autonomous manipulation framework that converts a task\ndescription into an actionable plan for a target robot without any\ndemonstrations. Given a task description, NovaFlow synthesizes a video using a\nvideo generation model and distills it into 3D actionable object flow using\noff-the-shelf perception modules. From the object flow, it computes relative\nposes for rigid objects and realizes them as robot actions via grasp proposals\nand trajectory optimization. For deformable objects, this flow serves as a\ntracking objective for model-based planning with a particle-based dynamics\nmodel. By decoupling task understanding from low-level control, NovaFlow\nnaturally transfers across embodiments. We validate on rigid, articulated, and\ndeformable object manipulation tasks using a table-top Franka arm and a Spot\nquadrupedal mobile robot, and achieve effective zero-shot execution without\ndemonstrations or embodiment-specific training. Project website:\nhttps://novaflow.lhy.xyz/.", "authors": ["Hongyu Li", "Lingfeng Sun", "Yafei Hu", "Duy Ta", "Jennifer Barry", "George Konidaris", "Jiahui Fu"], "categories": ["cs.RO", "cs.AI", "cs.CV"], "published": "2025-10-09T17:59:55Z", "pdf": "https://arxiv.org/pdf/2510.08568v1", "abs": "https://arxiv.org/abs/2510.08568v1", "comment": null, "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08567v1", "title": "MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning", "summary": "Vision language models (VLMs) are increasingly deployed as controllers with\naccess to external tools for complex reasoning and decision-making, yet their\neffectiveness remains limited by the scarcity of high-quality multimodal\ntrajectories and the cost of manual annotation. We address this challenge with\na vision-centric agent tuning framework that automatically synthesizes\nmultimodal trajectories, generates step-wise preference pairs, and trains a VLM\ncontroller for robust tool-use reasoning. Our pipeline first constructs\nM-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified\ntrajectories, enabling imitation-based trajectory tuning. Building on this, we\ndevelop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool\nreasoning. To achieve finer alignment, we further introduce Pref-X, a set of\n11K automatically generated preference pairs, and optimize MATRIX on it via\nstep-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,\nMATRIX consistently surpasses both open- and closed-source VLMs, demonstrating\nscalable and effective multimodal tool use. Our data and code is avaliable at\nhttps://github.com/mbzuai-oryx/MATRIX.", "authors": ["Tajamul Ashraf", "Umair Nawaz", "Abdelrahman M. Shaker", "Rao Anwer", "Philip Torr", "Fahad Shahbaz Khan", "Salman Khan"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-10-09T17:59:54Z", "pdf": "https://arxiv.org/pdf/2510.08567v1", "abs": "https://arxiv.org/abs/2510.08567v1", "comment": null, "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08564v1", "title": "How to Teach Large Multimodal Models New Skills", "summary": "How can we teach large multimodal models (LMMs) new skills without erasing\nprior abilities? We study sequential fine-tuning on five target skills while\nmonitoring general ability on eight held-out benchmarks across three model\nfamilies. We observe that apparent \"forgetting\" on held-out tasks after narrow\nfine-tuning can partly recover at later stages. We trace this behavior to a\nmeasurable shift in the output token distribution, manifested through a simple\ncounting-bias probe that co-varies with forgetting. Guided by this picture, we\nidentify two simple, robust tuning recipes that learn strongly while limiting\ndrift: (i) updating only the self-attention projection layers, and (ii)\nupdating only the MLP Gate&Up while freezing the Down projection. Across models\nand tasks, these choices deliver strong target gains while largely preserving\nheld-out performance. Code is available at\nhttps://github.com/jessemelpolio/LMM_CL", "authors": ["Zhen Zhu", "Yiming Gong", "Yao Xiao", "Yaoyao Liu", "Derek Hoiem"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "published": "2025-10-09T17:59:37Z", "pdf": "https://arxiv.org/pdf/2510.08564v1", "abs": "https://arxiv.org/abs/2510.08564v1", "comment": "In submission. Code is available at\n  https://github.com/jessemelpolio/LMM_CL", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08565v1", "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints", "summary": "Compositional training has been the de-facto paradigm in existing Multimodal\nLarge Language Models (MLLMs), where pre-trained vision encoders are connected\nwith pre-trained LLMs through continuous multimodal pre-training. However, the\nmultimodal scaling property of this paradigm remains difficult to explore due\nto the separated training. In this paper, we focus on the native training of\nMLLMs in an end-to-end manner and systematically study its design space and\nscaling property under a practical setting, i.e., data constraint. Through\ncareful study of various choices in MLLM, we obtain the optimal\nmeta-architecture that best balances performance and training cost. After that,\nwe further explore the scaling properties of the native MLLM and indicate the\npositively correlated scaling relationship between visual encoders and LLMs.\nBased on these findings, we propose a native MLLM called NaViL, combined with a\nsimple and cost-effective recipe. Experimental results on 14 multimodal\nbenchmarks confirm the competitive performance of NaViL against existing MLLMs.\nBesides that, our findings and results provide in-depth insights for the future\nstudy of native MLLMs.", "authors": ["Changyao Tian", "Hao Li", "Gen Luo", "Xizhou Zhu", "Weijie Su", "Hanming Deng", "Jinguo Zhu", "Jie Shao", "Ziran Zhu", "Yunpeng Liu", "Lewei Lu", "Wenhai Wang", "Hongsheng Li", "Jifeng Dai"], "categories": ["cs.CV"], "published": "2025-10-09T17:59:37Z", "pdf": "https://arxiv.org/pdf/2510.08565v1", "abs": "https://arxiv.org/abs/2510.08565v1", "comment": "Accepted by NeurIPS 2025. 22 pages, link:\n  https://github.com/OpenGVLab/NaViL", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08559v1", "title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models", "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress across\nvarious capabilities; however, complex video reasoning in the scientific domain\nremains a significant and challenging frontier. Current video benchmarks\npredominantly target general scenarios where perception/recognition is heavily\nrelied on, while with relatively simple reasoning tasks, leading to saturation\nand thus failing to effectively evaluate advanced multimodal cognitive skills.\nTo address this critical gap, we introduce SciVideoBench, a rigorous benchmark\nspecifically designed to assess advanced video reasoning in scientific\ncontexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice\nquestions derived from cutting-edge scientific experimental videos spanning\nover 25 specialized academic subjects and verified by a semi-automatic system.\nEach question demands sophisticated domain-specific knowledge, precise\nspatiotemporal perception, and intricate logical reasoning, effectively\nchallenging models' higher-order cognitive abilities. Our evaluation highlights\nsignificant performance deficits in state-of-the-art proprietary and\nopen-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating\nsubstantial room for advancement in video reasoning capabilities. Detailed\nanalyses of critical factors such as reasoning complexity and visual grounding\nprovide valuable insights and clear direction for future developments in LMMs,\ndriving the evolution of truly capable multimodal AI co-scientists. We hope\nSciVideoBench could fit the interests of the community and help to push the\nboundary of cutting-edge AI for border science.", "authors": ["Andong Deng", "Taojiannan Yang", "Shoubin Yu", "Lincoln Spencer", "Mohit Bansal", "Chen Chen", "Serena Yeung-Levy", "Xiaohan Wang"], "categories": ["cs.CV", "cs.AI"], "published": "2025-10-09T17:59:23Z", "pdf": "https://arxiv.org/pdf/2510.08559v1", "abs": "https://arxiv.org/abs/2510.08559v1", "comment": null, "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08556v1", "title": "DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model", "summary": "Achieving generalized in-hand object rotation remains a significant challenge\nin robotics, largely due to the difficulty of transferring policies from\nsimulation to the real world. The complex, contact-rich dynamics of dexterous\nmanipulation create a \"reality gap\" that has limited prior work to constrained\nscenarios involving simple geometries, limited object sizes and aspect ratios,\nconstrained wrist poses, or customized hands. We address this sim-to-real\nchallenge with a novel framework that enables a single policy, trained in\nsimulation, to generalize to a wide variety of objects and conditions in the\nreal world. The core of our method is a joint-wise dynamics model that learns\nto bridge the reality gap by effectively fitting limited amount of real-world\ncollected data and then adapting the sim policy's actions accordingly. The\nmodel is highly data-efficient and generalizable across different whole-hand\ninteraction distributions by factorizing dynamics across joints, compressing\nsystem-wide influences into low-dimensional variables, and learning each\njoint's evolution from its own dynamic profile, implicitly capturing these net\neffects. We pair this with a fully autonomous data collection strategy that\ngathers diverse, real-world interaction data with minimal human intervention.\nOur complete pipeline demonstrates unprecedented generality: a single policy\nsuccessfully rotates challenging objects with complex shapes (e.g., animals),\nhigh aspect ratios (up to 5.33), and small sizes, all while handling diverse\nwrist orientations and rotation axes. Comprehensive real-world evaluations and\na teleoperation application for complex tasks validate the effectiveness and\nrobustness of our approach. Website: https://meowuu7.github.io/DexNDM/", "authors": ["Xueyi Liu", "He Wang", "Li Yi"], "categories": ["cs.RO", "cs.CV"], "published": "2025-10-09T17:59:11Z", "pdf": "https://arxiv.org/pdf/2510.08556v1", "abs": "https://arxiv.org/abs/2510.08556v1", "comment": "Project Website: https://meowuu7.github.io/DexNDM/ Video:\n  https://youtu.be/tU2Mv8vWftU", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08553v1", "title": "Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation", "summary": "Vision-and-Language Navigation (VLN) requires agents to follow natural\nlanguage instructions through environments, with memory-persistent variants\ndemanding progressive improvement through accumulated experience. Existing\napproaches for memory-persistent VLN face critical limitations: they lack\neffective memory access mechanisms, instead relying on entire memory\nincorporation or fixed-horizon lookup, and predominantly store only\nenvironmental observations while neglecting navigation behavioral patterns that\nencode valuable decision-making strategies. We present Memoir, which employs\nimagination as a retrieval mechanism grounded by explicit memory: a world model\nimagines future navigation states as queries to selectively retrieve relevant\nenvironmental observations and behavioral histories. The approach comprises: 1)\na language-conditioned world model that imagines future states serving dual\npurposes: encoding experiences for storage and generating retrieval queries; 2)\nHybrid Viewpoint-Level Memory that anchors both observations and behavioral\npatterns to viewpoints, enabling hybrid retrieval; and 3) an\nexperience-augmented navigation model that integrates retrieved knowledge\nthrough specialized encoders. Extensive evaluation across diverse\nmemory-persistent VLN benchmarks with 10 distinctive testing scenarios\ndemonstrates Memoir's effectiveness: significant improvements across all\nscenarios, with 5.4% SPL gains on IR2R over the best memory-persistent\nbaseline, accompanied by 8.3x training speedup and 74% inference memory\nreduction. The results validate that predictive retrieval of both environmental\nand behavioral memories enables more effective navigation, with analysis\nindicating substantial headroom (73.3% vs 93.4% upper bound) for this\nimagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.", "authors": ["Yunzhe Xu", "Yiyuan Pan", "Zhe Liu"], "categories": ["cs.CV", "cs.AI", "cs.RO"], "published": "2025-10-09T17:58:01Z", "pdf": "https://arxiv.org/pdf/2510.08553v1", "abs": "https://arxiv.org/abs/2510.08553v1", "comment": "14 pages, 6 figures, 13 tables", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08551v1", "title": "ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation", "summary": "On-the-fly 3D reconstruction from monocular image sequences is a\nlong-standing challenge in computer vision, critical for applications such as\nreal-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff:\nper-scene optimization yields high fidelity but is computationally expensive,\nwhereas feed-forward foundation models enable real-time inference but struggle\nwith accuracy and robustness. In this work, we propose ARTDECO, a unified\nframework that combines the efficiency of feed-forward models with the\nreliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose\nestimation and point prediction, coupled with a Gaussian decoder that\ntransforms multi-scale features into structured 3D Gaussians. To sustain both\nfidelity and efficiency at scale, we design a hierarchical Gaussian\nrepresentation with a LoD-aware rendering strategy, which improves rendering\nfidelity while reducing redundancy. Experiments on eight diverse indoor and\noutdoor benchmarks show that ARTDECO delivers interactive performance\ncomparable to SLAM, robustness similar to feed-forward systems, and\nreconstruction quality close to per-scene optimization, providing a practical\npath toward on-the-fly digitization of real-world environments with both\naccurate geometry and high visual fidelity. Explore more demos on our project\npage: https://city-super.github.io/artdeco/.", "authors": ["Guanghao Li", "Kerui Ren", "Linning Xu", "Zhewen Zheng", "Changjian Jiang", "Xin Gao", "Bo Dai", "Jian Pu", "Mulin Yu", "Jiangmiao Pang"], "categories": ["cs.CV"], "published": "2025-10-09T17:57:38Z", "pdf": "https://arxiv.org/pdf/2510.08551v1", "abs": "https://arxiv.org/abs/2510.08551v1", "comment": null, "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08547v1", "title": "R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation", "summary": "Towards the aim of generalized robotic manipulation, spatial generalization\nis the most fundamental capability that requires the policy to work robustly\nunder different spatial distribution of objects, environment and agent itself.\nTo achieve this, substantial human demonstrations need to be collected to cover\ndifferent spatial configurations for training a generalized visuomotor policy\nvia imitation learning. Prior works explore a promising direction that\nleverages data generation to acquire abundant spatially diverse data from\nminimal source demonstrations. However, most approaches face significant\nsim-to-real gap and are often limited to constrained settings, such as\nfixed-base scenarios and predefined camera viewpoints. In this paper, we\npropose a real-to-real 3D data generation framework (R2RGen) that directly\naugments the pointcloud observation-action pairs to generate real-world data.\nR2RGen is simulator- and rendering-free, thus being efficient and\nplug-and-play. Specifically, given a single source demonstration, we introduce\nan annotation mechanism for fine-grained parsing of scene and trajectory. A\ngroup-wise augmentation strategy is proposed to handle complex multi-object\ncompositions and diverse task constraints. We further present camera-aware\nprocessing to align the distribution of generated data with real-world 3D\nsensor. Empirically, R2RGen substantially enhances data efficiency on extensive\nexperiments and demonstrates strong potential for scaling and application on\nmobile manipulation.", "authors": ["Xiuwei Xu", "Angyuan Ma", "Hankun Li", "Bingyao Yu", "Zheng Zhu", "Jie Zhou", "Jiwen Lu"], "categories": ["cs.RO", "cs.CV"], "published": "2025-10-09T17:55:44Z", "pdf": "https://arxiv.org/pdf/2510.08547v1", "abs": "https://arxiv.org/abs/2510.08547v1", "comment": "Project page: https://r2rgen.github.io/", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08540v1", "title": "MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization", "summary": "While current Multimodal Large Language Models (MLLMs) have demonstrated\nproficiency in reasoning tasks such as mathematics and logic, their capacity\nfor long-chain reflective reasoning, a prerequisite for solving complex\nreal-world problems, remains largely underexplored. In this work, we first\nconduct an extensive empirical investigation to evaluate this capability.\nLeveraging a carefully designed data synthesis engine, we construct MM-HELIX, a\nmultimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks\nthat require iterative thinking and backtracking. Empirical results on this\nbenchmark reveal that existing MLLMs exhibit significant performance deficits\nin long-chain reflective reasoning. To address this limitation, we generate\npost-training data and further explore learning paradigms for exploiting such\ndata. We first develop the Step-Elicited Response Generation pipeline to create\nMM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning\ntraces for instruction-tuning stage. Given that standard Reinforcement Learning\nfails on complex tasks due to sparse reward signals and catastrophic forgetting\nafter Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization\n(AHPO), a novel training strategy that dynamically unifies offline supervision\nand online optimization into a single stage. This strategy enables the model to\nlearn from expert data when rewards are sparse and conduct independent\nexploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our\nmethod achieves a +18.6\\% accuracy improvement on MM-HELIX benchmark and\ndemonstrates strong generalization with a +5.7\\% average performance gain on\ngeneral mathematic and logic tasks. Our work demonstrate that reflective\nreasoning in MLLMs can be effectively learned and generalized, paving the way\nfor developing more capable MLLMs.", "authors": ["Xiangyu Zhao", "Junming Lin", "Tianhao Liang", "Yifan Zhou", "Wenhao Chai", "Yuzhe Gu", "Weiyun Wang", "Kai Chen", "Gen Luo", "Wenwei Zhang", "Junchi Yan", "Hua Yang", "Haodong Duan", "Xue Yang"], "categories": ["cs.CV"], "published": "2025-10-09T17:53:58Z", "pdf": "https://arxiv.org/pdf/2510.08540v1", "abs": "https://arxiv.org/abs/2510.08540v1", "comment": null, "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08531v1", "title": "SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models", "summary": "Spatial reasoning remains a fundamental challenge for Vision-Language Models\n(VLMs), with current approaches struggling to achieve robust performance\ndespite recent advances. We identify that this limitation stems from a critical\ngap: existing methods attempt to learn spatial reasoning directly without\nestablishing the hierarchical foundations of perception and understanding. To\naddress this challenge, we present a comprehensive methodology for building\nspatial intelligence progressively. We introduce SpatialLadder-26k, a\nmultimodal dataset containing 26,610 samples spanning object localization,\nsingle image, multi-view, and video spatial reasoning tasks, constructed\nthrough a standardized pipeline that ensures systematic coverage across\nmodalities. Building on this dataset, we design a three-stage progressive\ntraining framework that (1) establishes spatial perception through object\nlocalization, (2) develops spatial understanding through multi-dimensional\nspatial tasks, and (3) strengthens complex reasoning via reinforcement learning\nwith verifiable rewards. This approach yields SpatialLadder, a 3B-parameter\nmodel that achieves state-of-the-art performance on spatial reasoning\nbenchmarks, with 23.4% average improvement over the base model, surpassing\nGPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains\nstrong generalization with 7.2% improvement on out-of-domain benchmarks,\ndemonstrating that progressive training from perception to reasoning is\nessential for robust spatial intelligence.", "authors": ["Hongxing Li", "Dingming Li", "Zixuan Wang", "Yuchen Yan", "Hang Wu", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-10-09T17:50:54Z", "pdf": "https://arxiv.org/pdf/2510.08531v1", "abs": "https://arxiv.org/abs/2510.08531v1", "comment": "Project Page: https://zju-real.github.io/SpatialLadder/ Code:\n  https://github.com/ZJU-REAL/SpatialLadder", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08530v1", "title": "X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering", "summary": "We present X2Video, the first diffusion model for rendering photorealistic\nvideos guided by intrinsic channels including albedo, normal, roughness,\nmetallicity, and irradiance, while supporting intuitive multi-modal controls\nwith reference images and text prompts for both global and local regions. The\nintrinsic guidance allows accurate manipulation of color, material, geometry,\nand lighting, while reference images and text prompts provide intuitive\nadjustments in the absence of intrinsic information. To enable these\nfunctionalities, we extend the intrinsic-guided image generation model XRGB to\nvideo generation by employing a novel and efficient Hybrid Self-Attention,\nwhich ensures temporal consistency across video frames and also enhances\nfidelity to reference images. We further develop a Masked Cross-Attention to\ndisentangle global and local text prompts, applying them effectively onto\nrespective local and global regions. For generating long videos, our novel\nRecursive Sampling method incorporates progressive frame sampling, combining\nkeyframe prediction and frame interpolation to maintain long-range temporal\nconsistency while preventing error accumulation. To support the training of\nX2Video, we assembled a video dataset named InteriorVideo, featuring 1,154\nrooms from 295 interior scenes, complete with reliable ground-truth intrinsic\nchannel sequences and smooth camera trajectories. Both qualitative and\nquantitative evaluations demonstrate that X2Video can produce long, temporally\nconsistent, and photorealistic videos guided by intrinsic conditions.\nAdditionally, X2Video effectively accommodates multi-modal controls with\nreference images, global and local text prompts, and simultaneously supports\nediting on color, material, geometry, and lighting through parametric tuning.\nProject page: https://luckyhzt.github.io/x2video", "authors": ["Zhitong Huang", "Mohan Zhang", "Renhan Wang", "Rui Tang", "Hao Zhu", "Jing Liao"], "categories": ["cs.GR", "cs.CV", "68U05", "I.3.3; I.3.6"], "published": "2025-10-09T17:50:31Z", "pdf": "https://arxiv.org/pdf/2510.08530v1", "abs": "https://arxiv.org/abs/2510.08530v1", "comment": "Code, model, and dataset will be released at project page soon:\n  https://luckyhzt.github.io/x2video", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08527v1", "title": "FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control", "summary": "We present FlexTraj, a framework for image-to-video generation with flexible\npoint trajectory control. FlexTraj introduces a unified point-based motion\nrepresentation that encodes each point with a segmentation ID, a temporally\nconsistent trajectory ID, and an optional color channel for appearance cues,\nenabling both dense and sparse trajectory control. Instead of injecting\ntrajectory conditions into the video generator through token concatenation or\nControlNet, FlexTraj employs an efficient sequence-concatenation scheme that\nachieves faster convergence, stronger controllability, and more efficient\ninference, while maintaining robustness under unaligned conditions. To train\nsuch a unified point trajectory-controlled video generator, FlexTraj adopts an\nannealing training strategy that gradually reduces reliance on complete\nsupervision and aligned condition. Experimental results demonstrate that\nFlexTraj enables multi-granularity, alignment-agnostic trajectory control for\nvideo generation, supporting various applications such as motion cloning,\ndrag-based image-to-video, motion interpolation, camera redirection, flexible\naction control and mesh animations.", "authors": ["Zhiyuan Zhang", "Can Wang", "Dongdong Chen", "Jing Liao"], "categories": ["cs.CV"], "published": "2025-10-09T17:50:22Z", "pdf": "https://arxiv.org/pdf/2510.08527v1", "abs": "https://arxiv.org/abs/2510.08527v1", "comment": "Project Page: https://bestzzhang.github.io/FlexTraj", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08525v1", "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression", "summary": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results.", "authors": ["Wenjie Du", "Li Jiang", "Keda Tao", "Xue Liu", "Huan Wang"], "categories": ["cs.CL"], "published": "2025-10-09T17:50:00Z", "pdf": "https://arxiv.org/pdf/2510.08525v1", "abs": "https://arxiv.org/abs/2510.08525v1", "comment": null, "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08512v1", "title": "Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression", "summary": "Efficient transmission of 3D point cloud data is critical for advanced\nperception in centralized and decentralized multi-agent robotic systems,\nespecially nowadays with the growing reliance on edge and cloud-based\nprocessing. However, the large and complex nature of point clouds creates\nchallenges under bandwidth constraints and intermittent connectivity, often\ndegrading system performance. We propose a deep compression framework based on\nsemantic scene graphs. The method decomposes point clouds into semantically\ncoherent patches and encodes them into compact latent representations with\nsemantic-aware encoders conditioned by Feature-wise Linear Modulation (FiLM). A\nfolding-based decoder, guided by latent features and graph node attributes,\nenables structurally accurate reconstruction. Experiments on the SemanticKITTI\nand nuScenes datasets show that the framework achieves state-of-the-art\ncompression rates, reducing data size by up to 98% while preserving both\nstructural and semantic fidelity. In addition, it supports downstream\napplications such as multi-robot pose graph optimization and map merging,\nachieving trajectory accuracy and map alignment comparable to those obtained\nwith raw LiDAR scans.", "authors": ["Nikolaos Stathoulopoulos", "Christoforos Kanellakis", "George Nikolakopoulos"], "categories": ["cs.CV", "cs.RO"], "published": "2025-10-09T17:45:09Z", "pdf": "https://arxiv.org/pdf/2510.08512v1", "abs": "https://arxiv.org/abs/2510.08512v1", "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L). 8 pages, 6 figures", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08508v1", "title": "MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration", "summary": "Real-world videos often suffer from complex degradations, such as noise,\ncompression artifacts, and low-light distortions, due to diverse acquisition\nand transmission conditions. Existing restoration methods typically require\nprofessional manual selection of specialized models or rely on monolithic\narchitectures that fail to generalize across varying degradations. Inspired by\nexpert experience, we propose MoA-VR, the first\n\\underline{M}ixture-\\underline{o}f-\\underline{A}gents \\underline{V}ideo\n\\underline{R}estoration system that mimics the reasoning and processing\nprocedures of human professionals through three coordinated agents: Degradation\nIdentification, Routing and Restoration, and Restoration Quality Assessment.\nSpecifically, we construct a large-scale and high-resolution video degradation\nrecognition benchmark and build a vision-language model (VLM) driven\ndegradation identifier. We further introduce a self-adaptive router powered by\nlarge language models (LLMs), which autonomously learns effective restoration\nstrategies by observing tool usage patterns. To assess intermediate and final\nprocessed video quality, we construct the \\underline{Res}tored\n\\underline{V}ideo \\underline{Q}uality (Res-VQ) dataset and design a dedicated\nVLM-based video quality assessment (VQA) model tailored for restoration tasks.\nExtensive experiments demonstrate that MoA-VR effectively handles diverse and\ncompound degradations, consistently outperforming existing baselines in terms\nof both objective metrics and perceptual quality. These results highlight the\npotential of integrating multimodal intelligence and modular reasoning in\ngeneral-purpose video restoration systems.", "authors": ["Lu Liu", "Chunlei Cai", "Shaocheng Shen", "Jianfeng Liang", "Weimin Ouyang", "Tianxiao Ye", "Jian Mao", "Huiyu Duan", "Jiangchao Yao", "Xiaoyun Zhang", "Qiang Hu", "Guangtao Zhai"], "categories": ["cs.CV"], "published": "2025-10-09T17:42:51Z", "pdf": "https://arxiv.org/pdf/2510.08508v1", "abs": "https://arxiv.org/abs/2510.08508v1", "comment": null, "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08494v1", "title": "Quartic quantum speedups for community detection", "summary": "Community detection is a foundational problem in data science. Its natural\nextension to hypergraphs captures higher-order correlations beyond pairwise\ninteractions. In this work, we develop a quantum algorithm for hypergraph\ncommunity detection that achieves a quartic quantum speedup over the best known\nclassical algorithm, along with superpolynomial savings in space. Our algorithm\nis based on the Kikuchi method, which we extend beyond previously considered\nproblems such as Tensor PCA and $p$XORSAT to a broad family of generalized\nstochastic block models. To demonstrate (near) optimality of this method, we\nprove matching lower bounds (up to logarithmic factors) in the low-degree\nframework, showing that the algorithm saturates a smooth\nstatistical-computational tradeoff. The quantum speedup arises from a quantized\nversion of the Kikuchi method and is based on the efficient preparation of a\nguiding state correlated with the underlying community structure. Our work\nsuggests that prior quantum speedups using the Kikuchi method are sufficiently\nrobust to encompass a broader set of problems than previously believed; we\nconjecture that a quantity known as marginal order characterizes the existence\nof these quantum speedups.", "authors": ["Alexander Schmidhuber", "Alexander Zlokapa"], "categories": ["quant-ph", "cs.DS"], "published": "2025-10-09T17:35:17Z", "pdf": "https://arxiv.org/pdf/2510.08494v1", "abs": "https://arxiv.org/abs/2510.08494v1", "comment": "40 pages", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08492v1", "title": "Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models", "summary": "Traditional multimodal learners find unified representations for tasks like\nvisual question answering, but rely heavily on paired datasets. However, an\noverlooked yet potentially powerful question is: can one leverage auxiliary\nunpaired multimodal data to directly enhance representation learning in a\ntarget modality? We introduce UML: Unpaired Multimodal Learner, a\nmodality-agnostic training paradigm in which a single model alternately\nprocesses inputs from different modalities while sharing parameters across\nthem. This design exploits the assumption that different modalities are\nprojections of a shared underlying reality, allowing the model to benefit from\ncross-modal structure without requiring explicit pairs. Theoretically, under\nlinear data-generating assumptions, we show that unpaired auxiliary data can\nyield representations strictly more informative about the data-generating\nprocess than unimodal training. Empirically, we show that using unpaired data\nfrom auxiliary modalities -- such as text, audio, or images -- consistently\nimproves downstream performance across diverse unimodal targets such as image\nand audio. Our project page: https://unpaired-multimodal.github.io/", "authors": ["Sharut Gupta", "Shobhita Sundaram", "Chenyu Wang", "Stefanie Jegelka", "Phillip Isola"], "categories": ["cs.LG", "cs.CV"], "published": "2025-10-09T17:32:23Z", "pdf": "https://arxiv.org/pdf/2510.08492v1", "abs": "https://arxiv.org/abs/2510.08492v1", "comment": "63 pages, 29 tables, and 47 figures", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08485v1", "title": "InstructX: Towards Unified Visual Editing with MLLM Guidance", "summary": "With recent advances in Multimodal Large Language Models (MLLMs) showing\nstrong visual understanding and reasoning, interest is growing in using them to\nimprove the editing performance of diffusion models. Despite rapid progress,\nmost studies lack an in-depth analysis of MLLM design choices. Moreover, the\nintegration of MLLMs and diffusion models remains an open challenge in some\ndifficult tasks, such as video editing. In this paper, we present InstructX, a\nunified framework for image and video editing. Specifically, we conduct a\ncomprehensive study on integrating MLLMs and diffusion models for\ninstruction-driven editing across diverse tasks. Building on this study, we\nanalyze the cooperation and distinction between images and videos in unified\nmodeling. (1) We show that training on image data can lead to emergent video\nediting capabilities without explicit supervision, thereby alleviating the\nconstraints imposed by scarce video training data. (2) By incorporating\nmodality-specific MLLM features, our approach effectively unifies image and\nvideo editing tasks within a single model. Extensive experiments demonstrate\nthat our method can handle a broad range of image and video editing tasks and\nachieves state-of-the-art performance.", "authors": ["Chong Mou", "Qichao Sun", "Yanze Wu", "Pengze Zhang", "Xinghui Li", "Fulong Ye", "Songtao Zhao", "Qian He"], "categories": ["cs.CV"], "published": "2025-10-09T17:26:09Z", "pdf": "https://arxiv.org/pdf/2510.08485v1", "abs": "https://arxiv.org/abs/2510.08485v1", "comment": null, "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08482v1", "title": "The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping", "summary": "Iconicity, the resemblance between linguistic form and meaning, is pervasive\nin signed languages, offering a natural testbed for visual grounding. For\nvision-language models (VLMs), the challenge is to recover such essential\nmappings from dynamic human motion rather than static context. We introduce the\n\\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts\npsycholinguistic measures to evaluate VLMs on three tasks: (i) phonological\nsign-form prediction (e.g., handshape, location), (ii) transparency (inferring\nmeaning from visual form), and (iii) graded iconicity ratings. We assess $13$\nstate-of-the-art VLMs in zero- and few-shot settings on Sign Language of the\nNetherlands and compare them to human baselines. On \\textit{phonological form\nprediction}, VLMs recover some handshape and location detail but remain below\nhuman performance; on \\textit{transparency}, they are far from human baselines;\nand only top models correlate moderately with human \\textit{iconicity ratings}.\nInterestingly, \\textit{models with stronger phonological form prediction\ncorrelate better with human iconicity judgment}, indicating shared sensitivity\nto visually grounded structure. Our findings validate these diagnostic tasks\nand motivate human-centric signals and embodied learning methods for modelling\niconicity and improving visual grounding in multimodal models.", "authors": ["Onur Kele\u015f", "Asl\u0131 \u00d6zy\u00fcrek", "Gerardo Ortega", "Kadir G\u00f6kg\u00f6", "Esam Ghaleb"], "categories": ["cs.CV", "cs.CL"], "published": "2025-10-09T17:21:59Z", "pdf": "https://arxiv.org/pdf/2510.08482v1", "abs": "https://arxiv.org/abs/2510.08482v1", "comment": null, "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08480v1", "title": "Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools", "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\npotential in bridging visual and textual reasoning, yet their reliance on\ntext-centric priors often limits their ability to disentangle semantically\nsimilar actions in open-vocabulary scenarios. To address this, we propose\nVideo-STAR, a framework that harmonizes contextual sub-motion decomposition\nwith tool-augmented reinforcement learning for open-vocabulary action\nrecognition (OVAR). Unlike prior methods that treat actions as monolithic\nentities, our approach innovatively decomposes actions into discriminative\nsub-motions for fine-grained matching while dynamically invoking\ndomain-specific tools for cross-modal interleaving, thereby enabling\ncategory-specific reasoning capacity and reducing cross-modal hallucination.\nMoreover, by designing a hierarchical reward that balances tool-usage\nefficiency, sub-motion relevance, and structural coherence in reasoning, our\nmethod autonomously leverages external tools to prioritize sub-motion patterns\nwithout explicit supervision, transmitting from text-centric reasoning to\nvisually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2,\nKinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art\nperformance, outperforming existing methods in distinguishing fine-grained\nactions and handling cross-modal hallucination, validating our excellent\nrobustness and generalization.", "authors": ["Zhenlong Yuan", "Xiangyan Qu", "Chengxuan Qian", "Rui Chen", "Jing Tang", "Lei Sun", "Xiangxiang Chu", "Dapeng Zhang", "Yiwei Wang", "Yujun Cai", "Shuo Li"], "categories": ["cs.CV"], "published": "2025-10-09T17:20:44Z", "pdf": "https://arxiv.org/pdf/2510.08480v1", "abs": "https://arxiv.org/abs/2510.08480v1", "comment": null, "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08475v1", "title": "DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos", "summary": "We present DexMan, an automated framework that converts human visual\ndemonstrations into bimanual dexterous manipulation skills for humanoid robots\nin simulation. Operating directly on third-person videos of humans manipulating\nrigid objects, DexMan eliminates the need for camera calibration, depth\nsensors, scanned 3D object assets, or ground-truth hand and object motion\nannotations. Unlike prior approaches that consider only simplified floating\nhands, it directly controls a humanoid robot and leverages novel contact-based\nrewards to improve policy learning from noisy hand-object poses estimated from\nin-the-wild videos.\n  DexMan achieves state-of-the-art performance in object pose estimation on the\nTACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.\nMeanwhile, its reinforcement learning policy surpasses previous methods by 19%\nin success rate on OakInk-v2. Furthermore, DexMan can generate skills from both\nreal and synthetic videos, without the need for manual data collection and\ncostly motion capture, and enabling the creation of large-scale, diverse\ndatasets for training generalist dexterous manipulation.", "authors": ["Jhen Hsieh", "Kuan-Hsun Tu", "Kuo-Han Hung", "Tsung-Wei Ke"], "categories": ["cs.RO", "cs.CV", "cs.LG"], "published": "2025-10-09T17:17:05Z", "pdf": "https://arxiv.org/pdf/2510.08475v1", "abs": "https://arxiv.org/abs/2510.08475v1", "comment": "Video results are available at:\n  https://embodiedai-ntu.github.io/dexman/index.html", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.08470v1", "title": "Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling", "summary": "Training vision-language models on cognitively-plausible amounts of data\nrequires rethinking how models integrate multimodal information. Within the\nconstraints of the Vision track for the BabyLM Challenge 2025, we propose a\nlightweight decoder-based architecture with (1) token-wise dynamic gating for\nadaptive fusion of linguistic and visual cues, (2) feature modulation and\nchannel attention to maximise the utility of limited visual information and (3)\nauxiliary contrastive objectives for visual grounding. Evaluation on five\nbenchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows\ncompetitive or superior performance to multimodal baselines. More notably, our\ndynamic gate discovers interpretable patterns without explicit supervision,\nfavouring visual cues for content words and linguistic cues for function words.\nWhile we identify limitations in the Challenge constraints, such as the\ninformation bottleneck created by global image embeddings and training\ninstability from the dataset split, our findings establish dynamic gating as a\npowerful tool for efficient multimodal learning, offering both interpretability\nand performance even under severe constraints.", "authors": ["Bianca-Mihaela Ganescu", "Suchir Salhan", "Andrew Caines", "Paula Buttery"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-10-09T17:10:36Z", "pdf": "https://arxiv.org/pdf/2510.08470v1", "abs": "https://arxiv.org/abs/2510.08470v1", "comment": "Accepted to the EMNLP 2025 BabyLM Workshop", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
