{"id": "2510.09607v1", "title": "VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation", "summary": "Vision-Language Action (VLA) models significantly advance robotic\nmanipulation by leveraging the strong perception capabilities of pretrained\nvision-language models (VLMs). By integrating action modules into these\npretrained models, VLA methods exhibit improved generalization. However,\ntraining them from scratch is costly. In this work, we propose a simple yet\neffective distillation-based framework that equips VLMs with action-execution\ncapability by transferring knowledge from pretrained small action models. Our\narchitecture retains the original VLM structure, adding only an action token\nand a state encoder to incorporate physical inputs. To distill action\nknowledge, we adopt a two-stage training strategy. First, we perform\nlightweight alignment by mapping VLM hidden states into the action space of the\nsmall action model, enabling effective reuse of its pretrained action decoder\nand avoiding expensive pretraining. Second, we selectively fine-tune the\nlanguage model, state encoder, and action modules, enabling the system to\nintegrate multimodal inputs with precise action generation. Specifically, the\naction token provides the VLM with a direct handle for predicting future\nactions, while the state encoder allows the model to incorporate robot dynamics\nnot captured by vision alone. This design yields substantial efficiency gains\nover training large VLA models from scratch. Compared with previous\nstate-of-the-art methods, our method achieves 97.3% average success rate on\nLIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In\nreal-world experiments across five manipulation tasks, our method consistently\noutperforms the teacher model, achieving 82.0% success rate (17% improvement),\nwhich demonstrate that action distillation effectively enables VLMs to generate\nprecise actions while substantially reducing training costs.", "authors": ["Shaoqi Dong", "Chaoyou Fu", "Haihan Gao", "Yi-Fan Zhang", "Chi Yan", "Chu Wu", "Xiaoyu Liu", "Yunhang Shen", "Jing Huo", "Deqiang Jiang", "Haoyu Cao", "Yang Gao", "Xing Sun", "Ran He", "Caifeng Shan"], "categories": ["cs.CV"], "published": "2025-10-10T17:59:56Z", "pdf": "https://arxiv.org/pdf/2510.09607v1", "abs": "https://arxiv.org/abs/2510.09607v1", "comment": "Homepage: https://ltbai.github.io/VITA-VLA/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u5c0f\u52a8\u4f5c\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4e3aVLM\u8d4b\u4e88\u52a8\u4f5c\u6267\u884c\u80fd\u529b\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u867d\u7136\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4ece\u5934\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\u8fdb\u884c\u8f7b\u91cf\u7ea7\u5bf9\u9f50\uff0c\u5c06VLM\u9690\u85cf\u72b6\u6001\u6620\u5c04\u5230\u5c0f\u52a8\u4f5c\u6a21\u578b\u7684\u52a8\u4f5c\u7a7a\u95f4\uff1b\u7136\u540e\u9009\u62e9\u6027\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u3001\u72b6\u6001\u7f16\u7801\u5668\u548c\u52a8\u4f5c\u6a21\u5757\uff0c\u6574\u5408\u591a\u6a21\u6001\u8f93\u5165\u4e0e\u7cbe\u786e\u52a8\u4f5c\u751f\u6210\u3002", "result": "\u5728LIBERO\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.3%\u5e73\u5747\u6210\u529f\u7387\uff08\u63d0\u534711.8%\uff09\uff0cLIBERO-LONG\u4e0a\u8fbe\u523093.5%\uff08\u63d0\u534724.5%\uff09\uff1b\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u57285\u4e2a\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8fbe\u523082.0%\u6210\u529f\u7387\uff08\u63d0\u534717%\uff09\u3002", "conclusion": "\u52a8\u4f5c\u84b8\u998f\u65b9\u6cd5\u80fd\u6709\u6548\u4f7fVLM\u751f\u6210\u7cbe\u786e\u52a8\u4f5c\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.09606v1", "title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km", "summary": "With the current surge in spatial reasoning explorations, researchers have\nmade significant progress in understanding indoor scenes, but still struggle\nwith diverse applications such as robotics and autonomous driving. This paper\naims to advance all-scale spatial reasoning across diverse scenarios by\ntackling two key challenges: 1) the heavy reliance on indoor 3D scans and\nlabor-intensive manual annotations for dataset curation; 2) the absence of\neffective all-scale scene modeling, which often leads to overfitting to\nindividual scenes. In this paper, we introduce a holistic solution that\nintegrates a structured spatial reasoning knowledge system, scale-aware\nmodeling, and a progressive training paradigm, as the first attempt to broaden\nthe all-scale spatial intelligence of MLLMs to the best of our knowledge. Using\na task-specific, specialist-driven automated pipeline, we curate over 38K video\nscenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising\napproximately 1M spatial QA pairs spanning 19 diverse task types. While\nspecialist models can inject useful domain knowledge, they are not reliable for\nevaluation. We then build an all-scale benchmark with precise annotations by\nmanually recording, retrieving, and assembling video-based data. However, naive\ntraining with SpaceVista-1M often yields suboptimal results due to the\npotential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a\nspatial reasoning model that accepts dense inputs beyond semantics and uses\nscale as an anchor for scale-aware experts and progressive rewards. Finally,\nextensive evaluations across 5 benchmarks, including our SpaceVista-Bench,\ndemonstrate competitive performance, showcasing strong generalization across\nall scales and scenarios. Our dataset, model, and benchmark will be released on\nhttps://peiwensun2000.github.io/mm2km .", "authors": ["Peiwen Sun", "Shiqiang Lang", "Dongming Wu", "Yi Ding", "Kaituo Feng", "Huadai Liu", "Zhen Ye", "Rui Liu", "Yun-Hui Liu", "Jianan Wang", "Xiangyu Yue"], "categories": ["cs.CV"], "published": "2025-10-10T17:59:46Z", "pdf": "https://arxiv.org/pdf/2510.09606v1", "abs": "https://arxiv.org/abs/2510.09606v1", "comment": "Project Page: https://peiwensun2000.github.io/mm2km/", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u5c3a\u5ea6\u7a7a\u95f4\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b38K\u89c6\u9891\u573a\u666f\u548c\u7ea6100\u4e07\u7a7a\u95f4\u95ee\u7b54\u5bf9\u7684SpaceVista-1M\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86SpaceVista-7B\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f9d\u8d56\u5ba4\u51853D\u626b\u63cf\u548c\u624b\u52a8\u6807\u6ce8\u3001\u7f3a\u4e4f\u6709\u6548\u5168\u5c3a\u5ea6\u5efa\u6a21\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7a7a\u95f4\u63a8\u7406\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u5ba4\u51853D\u626b\u63cf\u548c\u4eba\u5de5\u6807\u6ce8\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u5168\u5c3a\u5ea6\u573a\u666f\u5efa\u6a21\uff0c\u5bfc\u81f4\u6a21\u578b\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u5355\u4e2a\u573a\u666f\u3002\u8bba\u6587\u65e8\u5728\u63a8\u8fdb\u8de8\u591a\u6837\u5316\u573a\u666f\u7684\u5168\u5c3a\u5ea6\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6574\u5408\u7ed3\u6784\u5316\u7a7a\u95f4\u63a8\u7406\u77e5\u8bc6\u7cfb\u7edf\u3001\u5c3a\u5ea6\u611f\u77e5\u5efa\u6a21\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u8303\u5f0f\uff1b\u4f7f\u7528\u4efb\u52a1\u7279\u5b9a\u7684\u4e13\u5bb6\u9a71\u52a8\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u6784\u5efa\u6570\u636e\u96c6\uff1b\u5f00\u53d1\u63a5\u53d7\u5bc6\u96c6\u8f93\u5165\u5e76\u4f7f\u7528\u5c3a\u5ea6\u4f5c\u4e3a\u951a\u70b9\u7684SpaceVista-7B\u6a21\u578b\u3002", "result": "\u5728\u5305\u62ecSpaceVista-Bench\u5728\u5185\u76845\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5728\u6240\u6709\u5c3a\u5ea6\u548c\u573a\u666f\u4e0a\u7684\u7ade\u4e89\u6027\u6027\u80fd\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u6210\u529f\u6269\u5c55\u4e86MLLMs\u7684\u5168\u5c3a\u5ea6\u7a7a\u95f4\u667a\u80fd\uff0c\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2510.09598v1", "title": "Defensive Model Expansion for Robust Bayesian Inference", "summary": "Some applied researchers hesitate to use nonparametric methods, worrying that\nthey will lose power in small samples or overfit the data when simpler models\nare sufficient. We argue that at least some of these concerns are unfounded\nwhen nonparametric models are strongly shrunk towards parametric submodels. We\nconsider expanding a parametric model with a nonparametric component that is\nheavily shrunk toward zero. This construction allows the model to adapt\nautomatically: if the parametric model is correct, the nonparametric component\ndisappears, recovering parametric efficiency, while if it is misspecified, the\nflexible component activates to capture the missing signal. We show that this\nadaptive behavior follows from simple and general conditions. Specifically, we\nprove that Bayesian nonparametric models anchored to linear regression,\nincluding variants of Gaussian processes regression and Bayesian additive\nregression trees, consistently identify the correct parametric submodel when it\nholds and give asymptotically efficient inference for regression coefficients.\nIn simulations, we find that the \"general BART\" model performs identically to\ncorrectly specified linear regression when the parametric model holds, and\nsubstantially outperform it when nonlinear effects are present. This suggests a\npractical paradigm: \"defensive model expansion\" as a safeguard against model\nmisspecification.", "authors": ["Antonio R. Linero"], "categories": ["stat.ME"], "published": "2025-10-10T17:55:28Z", "pdf": "https://arxiv.org/pdf/2510.09598v1", "abs": "https://arxiv.org/abs/2510.09598v1", "comment": null, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u6027\u6a21\u578b\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u53c2\u6570\u6a21\u578b\u57fa\u7840\u4e0a\u6dfb\u52a0\u5f3a\u6536\u7f29\u7684\u975e\u53c2\u6570\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u81ea\u52a8\u9002\u5e94\uff1a\u5f53\u53c2\u6570\u6a21\u578b\u6b63\u786e\u65f6\u6062\u590d\u53c2\u6570\u6548\u7387\uff0c\u5f53\u6a21\u578b\u8bef\u8bbe\u65f6\u6fc0\u6d3b\u7075\u6d3b\u7ec4\u4ef6\u6355\u83b7\u7f3a\u5931\u4fe1\u53f7\u3002", "motivation": "\u5e94\u7528\u7814\u7a76\u4eba\u5458\u5bf9\u975e\u53c2\u6570\u65b9\u6cd5\u5b58\u5728\u987e\u8651\uff0c\u62c5\u5fc3\u5728\u5c0f\u6837\u672c\u4e2d\u5931\u53bb\u529f\u6548\u6216\u5728\u7b80\u5355\u6a21\u578b\u8db3\u591f\u65f6\u8fc7\u62df\u5408\u3002\u8bba\u6587\u65e8\u5728\u8bc1\u660e\u5f53\u975e\u53c2\u6570\u6a21\u578b\u5f3a\u70c8\u6536\u7f29\u5411\u53c2\u6570\u5b50\u6a21\u578b\u65f6\uff0c\u8fd9\u4e9b\u62c5\u5fe7\u662f\u4e0d\u5fc5\u8981\u7684\u3002", "method": "\u8003\u8651\u5728\u53c2\u6570\u6a21\u578b\u57fa\u7840\u4e0a\u6269\u5c55\u4e00\u4e2a\u5411\u96f6\u5f3a\u6536\u7f29\u7684\u975e\u53c2\u6570\u7ec4\u4ef6\u3002\u91c7\u7528\u8d1d\u53f6\u65af\u975e\u53c2\u6570\u6a21\u578b\u951a\u5b9a\u5230\u7ebf\u6027\u56de\u5f52\uff0c\u5305\u62ec\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u548c\u8d1d\u53f6\u65af\u52a0\u6027\u56de\u5f52\u6811\u7684\u53d8\u4f53\u3002", "result": "\u8bc1\u660e\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u53c2\u6570\u6a21\u578b\u6210\u7acb\u65f6\u80fd\u4e00\u81f4\u8bc6\u522b\u6b63\u786e\u7684\u53c2\u6570\u5b50\u6a21\u578b\uff0c\u5e76\u4e3a\u56de\u5f52\u7cfb\u6570\u63d0\u4f9b\u6e10\u8fd1\u6709\u6548\u63a8\u65ad\u3002\u6a21\u62df\u663e\u793a\"\u901a\u7528BART\"\u6a21\u578b\u5728\u53c2\u6570\u6a21\u578b\u6210\u7acb\u65f6\u4e0e\u6b63\u786e\u6307\u5b9a\u7684\u7ebf\u6027\u56de\u5f52\u8868\u73b0\u76f8\u540c\uff0c\u5728\u5b58\u5728\u975e\u7ebf\u6027\u6548\u5e94\u65f6\u663e\u8457\u4f18\u4e8e\u7ebf\u6027\u56de\u5f52\u3002", "conclusion": "\u63d0\u51fa\"\u9632\u5fa1\u6027\u6a21\u578b\u6269\u5c55\"\u4f5c\u4e3a\u9632\u6b62\u6a21\u578b\u8bef\u8bbe\u7684\u5b9e\u7528\u8303\u5f0f\uff0c\u4f7f\u6a21\u578b\u65e2\u80fd\u4fdd\u6301\u53c2\u6570\u6548\u7387\uff0c\u53c8\u80fd\u7075\u6d3b\u9002\u5e94\u590d\u6742\u6a21\u5f0f\u3002"}}
{"id": "2510.09586v1", "title": "Vision Language Models: A Survey of 26K Papers", "summary": "We present a transparent, reproducible measurement of research trends across\n26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles\nand abstracts are normalized, phrase-protected, and matched against a\nhand-crafted lexicon to assign up to 35 topical labels and mine fine-grained\ncues about tasks, architectures, training regimes, objectives, datasets, and\nco-mentioned modalities. The analysis quantifies three macro shifts: (1) a\nsharp rise of multimodal vision-language-LLM work, which increasingly reframes\nclassic perception as instruction following and multi-step reasoning; (2)\nsteady expansion of generative methods, with diffusion research consolidating\naround controllability, distillation, and speed; and (3) resilient 3D and video\nactivity, with composition moving from NeRFs to Gaussian splatting and a\ngrowing emphasis on human- and agent-centric understanding. Within VLMs,\nparameter-efficient adaptation like prompting/adapters/LoRA and lightweight\nvision-language bridges dominate; training practice shifts from building\nencoders from scratch to instruction tuning and finetuning strong backbones;\ncontrastive objectives recede relative to cross-entropy/ranking and\ndistillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and\nICLR the highest VLM share, while reliability themes such as efficiency or\nrobustness diffuse across areas. We release the lexicon and methodology to\nenable auditing and extension. Limitations include lexicon recall and\nabstract-only scope, but the longitudinal signals are consistent across venues\nand years.", "authors": ["Fengming Lin"], "categories": ["cs.CV"], "published": "2025-10-10T17:43:17Z", "pdf": "https://arxiv.org/pdf/2510.09586v1", "abs": "https://arxiv.org/abs/2510.09586v1", "comment": "VLM/LLM Learning Notes", "AI": {"tldr": "\u5bf92023-2025\u5e74CVPR\u3001ICLR\u548cNeurIPS\u768426,104\u7bc7\u8bba\u6587\u8fdb\u884c\u900f\u660e\u3001\u53ef\u91cd\u590d\u7684\u7814\u7a76\u8d8b\u52bf\u6d4b\u91cf\uff0c\u5206\u6790\u4e86\u4e09\u5927\u5b8f\u89c2\u8f6c\u53d8\uff1a\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00-LLM\u5de5\u4f5c\u7684\u6025\u5267\u589e\u957f\u3001\u751f\u6210\u65b9\u6cd5\u7684\u7a33\u6b65\u6269\u5c55\u4ee5\u53ca3D\u548c\u89c6\u9891\u6d3b\u52a8\u7684\u97e7\u6027\u53d1\u5c55\u3002", "motivation": "\u91cf\u5316\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u7814\u7a76\u8d8b\u52bf\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u5206\u6790\u5927\u91cf\u9876\u7ea7\u4f1a\u8bae\u8bba\u6587\uff0c\u63ed\u793a\u9886\u57df\u5185\u7684\u5b8f\u89c2\u53d8\u5316\u548c\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u5bf9\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u8fdb\u884c\u6807\u51c6\u5316\u3001\u77ed\u8bed\u4fdd\u62a4\u548c\u5339\u914d\u624b\u5de5\u5236\u4f5c\u7684\u8bcd\u5178\uff0c\u5206\u914d\u6700\u591a35\u4e2a\u4e3b\u9898\u6807\u7b7e\uff0c\u6316\u6398\u4efb\u52a1\u3001\u67b6\u6784\u3001\u8bad\u7ec3\u673a\u5236\u3001\u76ee\u6807\u3001\u6570\u636e\u96c6\u548c\u5171\u63d0\u53ca\u6a21\u6001\u7684\u7ec6\u7c92\u5ea6\u7ebf\u7d22\u3002", "result": "\u53d1\u73b0\u4e09\u5927\u5b8f\u89c2\u8f6c\u53d8\uff1a1) \u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00-LLM\u5de5\u4f5c\u6025\u5267\u589e\u957f\uff0c\u5c06\u7ecf\u5178\u611f\u77e5\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6307\u4ee4\u8ddf\u968f\u548c\u591a\u6b65\u63a8\u7406\uff1b2) \u751f\u6210\u65b9\u6cd5\u7a33\u6b65\u6269\u5c55\uff0c\u6269\u6563\u7814\u7a76\u56f4\u7ed5\u53ef\u63a7\u6027\u3001\u84b8\u998f\u548c\u901f\u5ea6\u8fdb\u884c\u6574\u5408\uff1b3) 3D\u548c\u89c6\u9891\u6d3b\u52a8\u4fdd\u6301\u97e7\u6027\uff0c\u7ec4\u5408\u4eceNeRFs\u8f6c\u5411\u9ad8\u65af\u6e85\u5c04\uff0c\u4eba\u7c7b\u548c\u667a\u80fd\u4f53\u4e2d\u5fc3\u7406\u89e3\u65e5\u76ca\u91cd\u8981\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u7cfb\u7edf\u6027\u8d8b\u52bf\u5206\u6790\uff0c\u63ed\u793a\u4e86\u591a\u6a21\u6001\u3001\u751f\u6210\u65b9\u6cd5\u548c3D/\u89c6\u9891\u7406\u89e3\u7684\u4e3b\u8981\u53d1\u5c55\u65b9\u5411\uff0c\u5e76\u53d1\u5e03\u4e86\u8bcd\u5178\u548c\u65b9\u6cd5\u8bba\u4ee5\u4f9b\u5ba1\u8ba1\u548c\u6269\u5c55\u3002"}}
{"id": "2510.09574v1", "title": "Zero-shot Structure Learning and Planning for Autonomous Robot Navigation using Active Inference", "summary": "Autonomous navigation in unfamiliar environments requires robots to\nsimultaneously explore, localise, and plan under uncertainty, without relying\non predefined maps or extensive training. We present a biologically inspired,\nActive Inference-based framework, Active Inference MAPping and Planning\n(AIMAPP). This model unifies mapping, localisation, and decision-making within\na single generative model. Inspired by hippocampal navigation, it uses\ntopological reasoning, place-cell encoding, and episodic memory to guide\nbehaviour. The agent builds and updates a sparse topological map online, learns\nstate transitions dynamically, and plans actions by minimising Expected Free\nEnergy. This allows it to balance goal-directed and exploratory behaviours. We\nimplemented a ROS-compatible navigation system that is sensor and\nrobot-agnostic, capable of integrating with diverse hardware configurations. It\noperates in a fully self-supervised manner, is resilient to drift, and supports\nboth exploration and goal-directed navigation without any pre-training. We\ndemonstrate robust performance in large-scale real and simulated environments\nagainst state-of-the-art planning models, highlighting the system's\nadaptability to ambiguous observations, environmental changes, and sensor\nnoise. The model offers a biologically inspired, modular solution to scalable,\nself-supervised navigation in unstructured settings. AIMAPP is available at\nhttps://github.com/decide-ugent/AIMAPP.", "authors": ["Daria de tinguy", "Tim Verbelen", "Emilio Gamba", "Bart Dhoedt"], "categories": ["cs.RO"], "published": "2025-10-10T17:28:12Z", "pdf": "https://arxiv.org/pdf/2510.09574v1", "abs": "https://arxiv.org/abs/2510.09574v1", "comment": "yet to be submitted", "AI": {"tldr": "AIMAPP\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u751f\u7269\u542f\u53d1\u5f0f\u81ea\u4e3b\u5bfc\u822a\u6846\u67b6\uff0c\u5c06\u5efa\u56fe\u3001\u5b9a\u4f4d\u548c\u51b3\u7b56\u7edf\u4e00\u5728\u5355\u4e00\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u5730\u56fe\u6216\u8bad\u7ec3\u5373\u53ef\u5728\u964c\u751f\u73af\u5883\u4e2d\u8fdb\u884c\u63a2\u7d22\u548c\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u964c\u751f\u73af\u5883\u4e2d\u540c\u65f6\u8fdb\u884c\u63a2\u7d22\u3001\u5b9a\u4f4d\u548c\u89c4\u5212\u7684\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u5b9a\u4e49\u5730\u56fe\u6216\u5927\u91cf\u8bad\u7ec3\uff0c\u5b9e\u73b0\u5b8c\u5168\u81ea\u76d1\u7763\u7684\u5bfc\u822a\u3002", "method": "\u91c7\u7528\u4e3b\u52a8\u63a8\u7406\u65b9\u6cd5\uff0c\u7ed3\u5408\u6d77\u9a6c\u4f53\u5bfc\u822a\u673a\u5236\uff0c\u4f7f\u7528\u62d3\u6251\u63a8\u7406\u3001\u4f4d\u7f6e\u7ec6\u80de\u7f16\u7801\u548c\u60c5\u666f\u8bb0\u5fc6\u6765\u6307\u5bfc\u884c\u4e3a\u3002\u5728\u7ebf\u6784\u5efa\u7a00\u758f\u62d3\u6251\u5730\u56fe\uff0c\u52a8\u6001\u5b66\u4e60\u72b6\u6001\u8f6c\u79fb\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u671f\u671b\u81ea\u7531\u80fd\u6765\u89c4\u5212\u52a8\u4f5c\u3002", "result": "\u5f00\u53d1\u4e86ROS\u517c\u5bb9\u7684\u5bfc\u822a\u7cfb\u7edf\uff0c\u5728\u5404\u79cd\u5927\u89c4\u6a21\u771f\u5b9e\u548c\u6a21\u62df\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\uff0c\u80fd\u591f\u9002\u5e94\u6a21\u7cca\u89c2\u6d4b\u3001\u73af\u5883\u53d8\u5316\u548c\u4f20\u611f\u5668\u566a\u58f0\u3002", "conclusion": "AIMAPP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u751f\u7269\u542f\u53d1\u7684\u6a21\u5757\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u81ea\u76d1\u7763\u5bfc\u822a\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u7684\u5f3a\u5927\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2510.09570v1", "title": "Differential Analysis of Pseudo Haptic Feedback: Novel Comparative Study of Visual and Auditory Cue Integration for Psychophysical Evaluation", "summary": "Pseudo-haptics exploit carefully crafted visual or auditory cues to trick the\nbrain into \"feeling\" forces that are never physically applied, offering a\nlow-cost alternative to traditional haptic hardware. Here, we present a\ncomparative psychophysical study that quantifies how visual and auditory\nstimuli combine to evoke pseudo-haptic pressure sensations on a commodity\ntablet. Using a Unity-based Rollball game, participants (n = 4) guided a\nvirtual ball across three textured terrains while their finger forces were\ncaptured in real time with a Robotous RFT40 force-torque sensor. Each terrain\nwas paired with a distinct rolling-sound profile spanning 440 Hz - 4.7 kHz, 440\nHz - 13.1 kHz, or 440 Hz - 8.9 kHz; crevice collisions triggered additional\n\"knocking\" bursts to heighten realism. Average tactile forces increased\nsystematically with cue intensity: 0.40 N, 0.79 N and 0.88 N for visual-only\ntrials and 0.41 N, 0.81 N and 0.90 N for audio-only trials on Terrains 1-3,\nrespectively. Higher audio frequencies and denser visual textures both elicited\nstronger muscle activation, and their combination further reduced the force\nneeded to perceive surface changes, confirming multisensory integration. These\nresults demonstrate that consumer-grade isometric devices can reliably induce\nand measure graded pseudo-haptic feedback without specialized actuators,\nopening a path toward affordable rehabilitation tools, training simulators and\nassistive interfaces.", "authors": ["Nishant Gautam", "Somya Sharma", "Peter Corcoran", "Kaspar Althoefer"], "categories": ["cs.HC", "cs.GR", "cs.NE", "cs.RO", "physics.med-ph"], "published": "2025-10-10T17:22:41Z", "pdf": "https://arxiv.org/pdf/2510.09570v1", "abs": "https://arxiv.org/abs/2510.09570v1", "comment": "17 Pages, 9 Figures", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5fc3\u7406\u7269\u7406\u5b66\u5b9e\u9a8c\u91cf\u5316\u4e86\u89c6\u89c9\u548c\u542c\u89c9\u523a\u6fc0\u5982\u4f55\u7ed3\u5408\u5728\u666e\u901a\u5e73\u677f\u8bbe\u5907\u4e0a\u8bf1\u53d1\u4f2a\u89e6\u89c9\u538b\u529b\u611f\u53d7\uff0c\u53d1\u73b0\u591a\u611f\u5b98\u6574\u5408\u80fd\u6709\u6548\u589e\u5f3a\u4f2a\u89e6\u89c9\u53cd\u9988\u6548\u679c\u3002", "motivation": "\u4f2a\u89e6\u89c9\u6280\u672f\u5229\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u89c6\u89c9\u6216\u542c\u89c9\u7ebf\u7d22\u6765\u6b3a\u9a97\u5927\u8111\"\u611f\u53d7\"\u4ece\u672a\u7269\u7406\u65bd\u52a0\u7684\u529b\uff0c\u4e3a\u4f20\u7edf\u89e6\u89c9\u786c\u4ef6\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u66ff\u4ee3\u65b9\u6848\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u89c6\u89c9\u548c\u542c\u89c9\u523a\u6fc0\u5982\u4f55\u534f\u540c\u5de5\u4f5c\u6765\u589e\u5f3a\u4f2a\u89e6\u89c9\u4f53\u9a8c\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eUnity\u7684Rollball\u6e38\u620f\uff0c\u53c2\u4e0e\u8005\uff08n=4\uff09\u5728\u4e09\u79cd\u4e0d\u540c\u7eb9\u7406\u7684\u5730\u5f62\u4e0a\u5f15\u5bfc\u865a\u62df\u7403\uff0c\u540c\u65f6\u901a\u8fc7Robotous RFT40\u529b-\u626d\u77e9\u4f20\u611f\u5668\u5b9e\u65f6\u6355\u83b7\u624b\u6307\u529b\u3002\u6bcf\u4e2a\u5730\u5f62\u914d\u6709\u4e0d\u540c\u7684\u6eda\u52a8\u58f0\u97f3\u914d\u7f6e\u6587\u4ef6\uff0c\u78b0\u649e\u65f6\u89e6\u53d1\u989d\u5916\u7684\"\u6572\u51fb\"\u97f3\u6548\u4ee5\u589e\u5f3a\u771f\u5b9e\u611f\u3002", "result": "\u89e6\u89c9\u529b\u968f\u7ebf\u7d22\u5f3a\u5ea6\u7cfb\u7edf\u589e\u52a0\uff1a\u4ec5\u89c6\u89c9\u8bd5\u9a8c\u4e3a0.40N\u30010.79N\u548c0.88N\uff0c\u4ec5\u97f3\u9891\u8bd5\u9a8c\u4e3a0.41N\u30010.81N\u548c0.90N\u3002\u66f4\u9ad8\u7684\u97f3\u9891\u9891\u7387\u548c\u66f4\u5bc6\u96c6\u7684\u89c6\u89c9\u7eb9\u7406\u90fd\u5f15\u53d1\u4e86\u66f4\u5f3a\u7684\u808c\u8089\u6fc0\u6d3b\uff0c\u5b83\u4eec\u7684\u7ec4\u5408\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e86\u611f\u77e5\u8868\u9762\u53d8\u5316\u6240\u9700\u7684\u529b\u91cf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6d88\u8d39\u7ea7\u7b49\u957f\u8bbe\u5907\u53ef\u4ee5\u5728\u6ca1\u6709\u4e13\u95e8\u6267\u884c\u5668\u7684\u60c5\u51b5\u4e0b\u53ef\u9760\u5730\u8bf1\u5bfc\u548c\u6d4b\u91cf\u5206\u7ea7\u4f2a\u89e6\u89c9\u53cd\u9988\uff0c\u4e3a\u7ecf\u6d4e\u5b9e\u60e0\u7684\u5eb7\u590d\u5de5\u5177\u3001\u8bad\u7ec3\u6a21\u62df\u5668\u548c\u8f85\u52a9\u754c\u9762\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2510.09558v1", "title": "AutoPR: Let's Automate Your Academic Promotion!", "summary": "As the volume of peer-reviewed research surges, scholars increasingly rely on\nsocial platforms for discovery, while authors invest considerable effort in\npromoting their work to ensure visibility and citations. To streamline this\nprocess and reduce the reliance on human effort, we introduce Automatic\nPromotion (AutoPR), a novel task that transforms research papers into accurate,\nengaging, and timely public content. To enable rigorous evaluation, we release\nPRBench, a multimodal benchmark that links 512 peer-reviewed articles to\nhigh-quality promotional posts, assessing systems along three axes: Fidelity\n(accuracy and tone), Engagement (audience targeting and appeal), and Alignment\n(timing and channel optimization). We also introduce PRAgent, a multi-agent\nframework that automates AutoPR in three stages: content extraction with\nmultimodal preparation, collaborative synthesis for polished outputs, and\nplatform-specific adaptation to optimize norms, tone, and tagging for maximum\nreach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates\nsubstantial improvements, including a 604% increase in total watch time, a 438%\nrise in likes, and at least a 2.9x boost in overall engagement. Ablation\nstudies show that platform modeling and targeted promotion contribute the most\nto these gains. Our results position AutoPR as a tractable, measurable research\nproblem and provide a roadmap for scalable, impactful automated scholarly\ncommunication.", "authors": ["Qiguang Chen", "Zheng Yan", "Mingda Yang", "Libo Qin", "Yixin Yuan", "Hanjing Li", "Jinhao Liu", "Yiyan Ji", "Dengyun Peng", "Jiannan Guan", "Mengkang Hu", "Yantao Du", "Wanxiang Che"], "categories": ["cs.CL"], "published": "2025-10-10T17:08:36Z", "pdf": "https://arxiv.org/pdf/2510.09558v1", "abs": "https://arxiv.org/abs/2510.09558v1", "comment": "Preprint. Code: https://github.com/LightChen2333/AutoPR . Benchmark:\n  https://huggingface.co/datasets/yzweak/PRBench", "AI": {"tldr": "AutoPR\u662f\u4e00\u79cd\u81ea\u52a8\u5c06\u7814\u7a76\u8bba\u6587\u8f6c\u5316\u4e3a\u51c6\u786e\u3001\u5438\u5f15\u4eba\u4e14\u53ca\u65f6\u516c\u5f00\u5185\u5bb9\u7684\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7PRAgent\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5b9e\u73b0\u4e09\u9636\u6bb5\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u5b66\u672f\u63a8\u5e7f\u6548\u679c\u3002", "motivation": "\u968f\u7740\u540c\u884c\u8bc4\u5ba1\u7814\u7a76\u6570\u91cf\u6fc0\u589e\uff0c\u5b66\u8005\u4f9d\u8d56\u793e\u4ea4\u5e73\u53f0\u53d1\u73b0\u7814\u7a76\uff0c\u4f5c\u8005\u9700\u8981\u6295\u5165\u5927\u91cf\u7cbe\u529b\u63a8\u5e7f\u5de5\u4f5c\u4ee5\u786e\u4fdd\u53ef\u89c1\u6027\u548c\u5f15\u7528\u3002\u4e3a\u7b80\u5316\u6d41\u7a0b\u5e76\u51cf\u5c11\u4eba\u529b\u4f9d\u8d56\uff0c\u9700\u8981\u81ea\u52a8\u5316\u63a8\u5e7f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPRAgent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a\u591a\u6a21\u6001\u5185\u5bb9\u63d0\u53d6\u3001\u534f\u4f5c\u5408\u6210\u751f\u6210\u7cbe\u70bc\u8f93\u51fa\u3001\u5e73\u53f0\u7279\u5b9a\u9002\u914d\u4ee5\u4f18\u5316\u89c4\u8303\u3001\u8bed\u6c14\u548c\u6807\u7b7e\u5b9e\u73b0\u6700\u5927\u8986\u76d6\u3002", "result": "\u5728PRBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u76f4\u63a5LLM\u6d41\u6c34\u7ebf\uff0cPRAgent\u5b9e\u73b0\u663e\u8457\u6539\u8fdb\uff1a\u603b\u89c2\u770b\u65f6\u95f4\u589e\u52a0604%\uff0c\u70b9\u8d5e\u6570\u589e\u957f438%\uff0c\u6574\u4f53\u53c2\u4e0e\u5ea6\u81f3\u5c11\u63d0\u53472.9\u500d\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u5e73\u53f0\u5efa\u6a21\u548c\u76ee\u6807\u63a8\u5e7f\u8d21\u732e\u6700\u5927\u3002", "conclusion": "AutoPR\u662f\u4e00\u4e2a\u53ef\u5904\u7406\u3001\u53ef\u8861\u91cf\u7684\u7814\u7a76\u95ee\u9898\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u6709\u5f71\u54cd\u529b\u7684\u81ea\u52a8\u5316\u5b66\u672f\u4ea4\u6d41\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002"}}
{"id": "2510.09543v1", "title": "Guiding Energy-Efficient Locomotion through Impact Mitigation Rewards", "summary": "Animals achieve energy-efficient locomotion by their implicit passive\ndynamics, a marvel that has captivated roboticists for decades.Recently,\nmethods incorporated Adversarial Motion Prior (AMP) and Reinforcement learning\n(RL) shows promising progress to replicate Animals' naturalistic motion.\nHowever, such imitation learning approaches predominantly capture explicit\nkinematic patterns, so-called gaits, while overlooking the implicit passive\ndynamics. This work bridges this gap by incorporating a reward term guided by\nImpact Mitigation Factor (IMF), a physics-informed metric that quantifies a\nrobot's ability to passively mitigate impacts. By integrating IMF with AMP, our\napproach enables RL policies to learn both explicit motion trajectories from\nanimal reference motion and the implicit passive dynamic. We demonstrate energy\nefficiency improvements of up to 32%, as measured by the Cost of Transport\n(CoT), across both AMP and handcrafted reward structure.", "authors": ["Chenghao Wang", "Arjun Viswanathan", "Eric Sihite", "Alireza Ramezani"], "categories": ["cs.RO"], "published": "2025-10-10T16:56:35Z", "pdf": "https://arxiv.org/pdf/2510.09543v1", "abs": "https://arxiv.org/abs/2510.09543v1", "comment": null, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u51b2\u51fb\u7f13\u89e3\u56e0\u5b50(IMF)\u548c\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c(AMP)\u7684\u65b9\u6cd5\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u52a8\u7269\u7684\u663e\u6027\u8fd0\u52a8\u8f68\u8ff9\u548c\u9690\u6027\u88ab\u52a8\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe32%\u7684\u80fd\u6e90\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u52a8\u7269\u901a\u8fc7\u5176\u9690\u542b\u7684\u88ab\u52a8\u52a8\u529b\u5b66\u5b9e\u73b0\u8282\u80fd\u8fd0\u52a8\uff0c\u4f46\u73b0\u6709\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u6355\u6349\u663e\u6027\u6b65\u6001\u6a21\u5f0f\uff0c\u800c\u5ffd\u7565\u4e86\u9690\u6027\u88ab\u52a8\u52a8\u529b\u5b66\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u51b2\u51fb\u7f13\u89e3\u56e0\u5b50(IMF)\u4f5c\u4e3a\u5956\u52b1\u9879\uff0c\u5e76\u5c06\u5176\u4e0e\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c(AMP)\u7ed3\u5408\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u80fd\u591f\u5b66\u4e60\u52a8\u7269\u7684\u663e\u6027\u8fd0\u52a8\u8f68\u8ff9\u548c\u9690\u6027\u88ab\u52a8\u52a8\u529b\u5b66\u3002", "result": "\u5728AMP\u548c\u624b\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u7ed3\u6784\u4e2d\u90fd\u5b9e\u73b0\u4e86\u9ad8\u8fbe32%\u7684\u80fd\u6e90\u6548\u7387\u63d0\u5347\uff0c\u901a\u8fc7\u8fd0\u8f93\u6210\u672c(CoT)\u6765\u8861\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u88ab\u52a8\u52a8\u529b\u5b66\u6574\u5408\u5230\u8fd0\u52a8\u6a21\u4eff\u5b66\u4e60\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7684\u80fd\u6e90\u6548\u7387\u3002"}}
{"id": "2510.09542v1", "title": "Lie symmetry analysis of the two-Higgs-doublet model field equations", "summary": "We apply Lie symmetry analysis of partial differential equations (PDEs) to\nthe Euler-Lagrange equations of the two-Higgs-doublet model (2HDM), to\ndetermine its scalar Lie point symmetries. A Lie point symmetry is a\nstructure-preserving transformation of the spacetime variables and the fields\nof the model, which is also continuous and connected to the identity.\nSymmetries of PDEs may in general be divided into strict variational\nsymmetries, divergence symmetries and non-variational symmetries, where the\nfirst two are collectively referred to as variational symmetries. Variational\nsymmetries are usually preserved under quantization, and variational Lie\nsymmetries yield conservation laws. We demonstrate that there are no scalar Lie\npoint divergence symmetries or non-variational Lie point symmetries in the\n2HDM, and re-derive its well-known strict variational Lie point symmetries,\nthus confirming the consistency of our implementation of Lie's method.\nMoreover, we prove three general results which may simplify Lie symmetry\ncalculations for a wide class of particle physics models. Lie symmetry analysis\nof PDEs is a broadly applicable method for determining Lie symmetries. As\ndemonstrated here by example, it can be applied to models with many variables,\nparameters and reparametrization freedom, while any missing discrete symmetries\nmay be identified through the automorphism groups of the resulting Lie symmetry\nalgebras.", "authors": ["M. Aa. Solberg"], "categories": ["hep-ph", "math-ph", "math.MP"], "published": "2025-10-10T16:55:08Z", "pdf": "https://arxiv.org/pdf/2510.09542v1", "abs": "https://arxiv.org/abs/2510.09542v1", "comment": "38 pages", "AI": {"tldr": "\u672c\u6587\u5e94\u7528\u674e\u5bf9\u79f0\u6027\u5206\u6790\u7814\u7a76\u53cc\u5e0c\u683c\u65af\u4e8c\u91cd\u6001\u6a21\u578b(2HDM)\u7684\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u65b9\u7a0b\uff0c\u786e\u5b9a\u4e86\u5176\u6807\u91cf\u674e\u70b9\u5bf9\u79f0\u6027\u3002\u7814\u7a76\u53d1\u73b02HDM\u4e2d\u4e0d\u5b58\u5728\u6807\u91cf\u674e\u70b9\u6563\u5ea6\u5bf9\u79f0\u6027\u6216\u975e\u53d8\u5206\u674e\u70b9\u5bf9\u79f0\u6027\uff0c\u5e76\u91cd\u65b0\u63a8\u5bfc\u4e86\u5df2\u77e5\u7684\u4e25\u683c\u53d8\u5206\u674e\u70b9\u5bf9\u79f0\u6027\u3002", "motivation": "\u7814\u7a762HDM\u7684\u5bf9\u79f0\u6027\u7ed3\u6784\uff0c\u56e0\u4e3a\u53d8\u5206\u5bf9\u79f0\u6027\u901a\u5e38\u5728\u91cf\u5b50\u5316\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e0d\u53d8\uff0c\u4e14\u53d8\u5206\u674e\u5bf9\u79f0\u6027\u4f1a\u4ea7\u751f\u5b88\u6052\u5b9a\u5f8b\u3002", "method": "\u4f7f\u7528\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u674e\u5bf9\u79f0\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u5206\u67902HDM\u7684\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u65b9\u7a0b\uff0c\u786e\u5b9a\u674e\u70b9\u5bf9\u79f0\u6027\u3002", "result": "\u786e\u8ba42HDM\u4e2d\u53ea\u6709\u4e25\u683c\u53d8\u5206\u674e\u70b9\u5bf9\u79f0\u6027\uff0c\u4e0d\u5b58\u5728\u6563\u5ea6\u5bf9\u79f0\u6027\u6216\u975e\u53d8\u5206\u5bf9\u79f0\u6027\u3002\u8bc1\u660e\u4e86\u4e09\u4e2a\u53ef\u7b80\u5316\u7c92\u5b50\u7269\u7406\u6a21\u578b\u674e\u5bf9\u79f0\u6027\u8ba1\u7b97\u7684\u901a\u7528\u7ed3\u679c\u3002", "conclusion": "\u674e\u5bf9\u79f0\u6027\u5206\u6790\u662f\u786e\u5b9a\u674e\u5bf9\u79f0\u6027\u7684\u5e7f\u6cdb\u5e94\u7528\u65b9\u6cd5\uff0c\u53ef\u5904\u7406\u591a\u53d8\u91cf\u3001\u591a\u53c2\u6570\u548c\u91cd\u53c2\u6570\u5316\u81ea\u7531\u5ea6\u7684\u6a21\u578b\uff0c\u7f3a\u5931\u7684\u79bb\u6563\u5bf9\u79f0\u6027\u53ef\u901a\u8fc7\u6240\u5f97\u674e\u5bf9\u79f0\u6027\u4ee3\u6570\u7684\u81ea\u540c\u6784\u7fa4\u8bc6\u522b\u3002"}}
{"id": "2510.09529v1", "title": "Self-Resetting Soft Ring Enables Autonomous and Continuous Leaping under Uniform Light", "summary": "Jumping is an efficient locomotion strategy to traverse cluttered, uneven, or\nunstable environments in nature, yet replicating continuous, autonomous leaping\nin soft robots remains challenging due to limited energy storage and reliance\non human intervention or latches. Here, we report a millimeter-scale,\nself-resetting soft ring that achieves repeated vertical and stable horizontal\nleaps under uniform infrared illumination without external control. The\nring-shaped liquid crystal elastomer body twists to store elastic energy, which\nis suddenly released when a rigid tail strikes the ground, propelling the\nrobot. During the airborne phase, the twisted body autonomously untwists,\nresetting for the next cycle. By tuning geometric asymmetry and the center of\nmass, the robot transitions between crawling, directional leaping, and vertical\njumping. Optimized configurations yield vertical jumps exceeding 80 body\nheights and directional horizontal leaps over 3 body lengths. Beyond controlled\nmotion on flat ground, the robot demonstrates resilient and robust locomotion\nacross slopes, parallel hurdles, and diverse cluttered natural terrains\nincluding grass, wet sand, and mulch. This work establishes a new paradigm of\ntwisting-enabled, photothermally powered soft robots capable of autonomous,\ncontinuous leaping, with potential applications in environmental navigation,\nswarm robotics, and unstructured terrain navigation.", "authors": ["Fangjie Qi", "Caizhi Zhou", "Haitao Qing", "Haoze Sun", "Jie Yin"], "categories": ["physics.app-ph"], "published": "2025-10-10T16:43:22Z", "pdf": "https://arxiv.org/pdf/2510.09529v1", "abs": "https://arxiv.org/abs/2510.09529v1", "comment": null, "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6beb\u7c73\u7ea7\u81ea\u590d\u4f4d\u8f6f\u73af\u673a\u5668\u4eba\uff0c\u80fd\u591f\u5728\u5747\u5300\u7ea2\u5916\u5149\u7167\u4e0b\u5b9e\u73b0\u91cd\u590d\u5782\u76f4\u548c\u7a33\u5b9a\u6c34\u5e73\u8df3\u8dc3\uff0c\u65e0\u9700\u5916\u90e8\u63a7\u5236\u3002\u901a\u8fc7\u51e0\u4f55\u4e0d\u5bf9\u79f0\u548c\u8d28\u5fc3\u8c03\u8282\uff0c\u673a\u5668\u4eba\u53ef\u5728\u722c\u884c\u3001\u5b9a\u5411\u8df3\u8dc3\u548c\u5782\u76f4\u8df3\u8dc3\u95f4\u5207\u6362\uff0c\u5728\u591a\u79cd\u590d\u6742\u5730\u5f62\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u8fd0\u52a8\u80fd\u529b\u3002", "motivation": "\u81ea\u7136\u754c\u4e2d\u8df3\u8dc3\u662f\u4e00\u79cd\u5728\u6742\u4e71\u3001\u4e0d\u5e73\u6216\u4e0d\u7a33\u5b9a\u73af\u5883\u4e2d\u9ad8\u6548\u79fb\u52a8\u7684\u7b56\u7565\uff0c\u4f46\u5728\u8f6f\u673a\u5668\u4eba\u4e2d\u5b9e\u73b0\u8fde\u7eed\u81ea\u4e3b\u8df3\u8dc3\u4ecd\u5177\u6311\u6218\u6027\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u80fd\u91cf\u5b58\u50a8\u6709\u9650\u548c\u5bf9\u4eba\u5de5\u5e72\u9884\u6216\u9501\u5b58\u673a\u5236\u7684\u4f9d\u8d56\u3002", "method": "\u91c7\u7528\u73af\u5f62\u6db2\u6676\u5f39\u6027\u4f53\u7ed3\u6784\uff0c\u901a\u8fc7\u626d\u66f2\u5b58\u50a8\u5f39\u6027\u80fd\u91cf\uff0c\u5f53\u521a\u6027\u5c3e\u90e8\u649e\u51fb\u5730\u9762\u65f6\u7a81\u7136\u91ca\u653e\u80fd\u91cf\u63a8\u52a8\u673a\u5668\u4eba\u3002\u5728\u7a7a\u4e2d\u9636\u6bb5\uff0c\u626d\u66f2\u4f53\u81ea\u4e3b\u89e3\u626d\u4e3a\u4e0b\u4e00\u5468\u671f\u590d\u4f4d\u3002\u901a\u8fc7\u8c03\u8282\u51e0\u4f55\u4e0d\u5bf9\u79f0\u6027\u548c\u8d28\u5fc3\u4f4d\u7f6e\u63a7\u5236\u8fd0\u52a8\u6a21\u5f0f\u3002", "result": "\u4f18\u5316\u914d\u7f6e\u53ef\u5b9e\u73b0\u8d85\u8fc780\u500d\u4f53\u9ad8\u7684\u5782\u76f4\u8df3\u8dc3\u548c\u8d85\u8fc73\u500d\u4f53\u957f\u7684\u5b9a\u5411\u6c34\u5e73\u8df3\u8dc3\u3002\u673a\u5668\u4eba\u80fd\u591f\u5728\u659c\u5761\u3001\u5e73\u884c\u969c\u788d\u4ee5\u53ca\u8349\u5730\u3001\u6e7f\u6c99\u3001\u8986\u76d6\u7269\u7b49\u591a\u79cd\u81ea\u7136\u6742\u4e71\u5730\u5f62\u4e2d\u8868\u73b0\u51fa\u5f39\u6027\u548c\u9c81\u68d2\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5efa\u7acb\u4e86\u4e00\u79cd\u57fa\u4e8e\u626d\u66f2\u673a\u5236\u3001\u5149\u70ed\u9a71\u52a8\u7684\u8f6f\u673a\u5668\u4eba\u65b0\u8303\u5f0f\uff0c\u80fd\u591f\u5b9e\u73b0\u81ea\u4e3b\u8fde\u7eed\u8df3\u8dc3\uff0c\u5728\u73af\u5883\u5bfc\u822a\u3001\u7fa4\u4f53\u673a\u5668\u4eba\u548c\u975e\u7ed3\u6784\u5316\u5730\u5f62\u5bfc\u822a\u65b9\u9762\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.09526v1", "title": "Dynamic Quadrupedal Legged and Aerial Locomotion via Structure Repurposing", "summary": "Multi-modal ground-aerial robots have been extensively studied, with a\nsignificant challenge lying in the integration of conflicting requirements\nacross different modes of operation. The Husky robot family, developed at\nNortheastern University, and specifically the Husky v.2 discussed in this\nstudy, addresses this challenge by incorporating posture manipulation and\nthrust vectoring into multi-modal locomotion through structure repurposing.\nThis quadrupedal robot features leg structures that can be repurposed for\ndynamic legged locomotion and flight. In this paper, we present the hardware\ndesign of the robot and report primary results on dynamic quadrupedal legged\nlocomotion and hovering.", "authors": ["Chenghao Wang", "Kaushik Venkatesh Krishnamurthy", "Shreyansh Pitroda", "Adarsh Salagame", "Ioannis Mandralis", "Eric Sihite", "Alireza Ramezani", "Morteza Gharib"], "categories": ["cs.RO"], "published": "2025-10-10T16:38:48Z", "pdf": "https://arxiv.org/pdf/2510.09526v1", "abs": "https://arxiv.org/abs/2510.09526v1", "comment": null, "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Husky v.2\u591a\u6a21\u6001\u5730\u9762-\u7a7a\u4e2d\u673a\u5668\u4eba\u7684\u786c\u4ef6\u8bbe\u8ba1\uff0c\u8be5\u673a\u5668\u4eba\u901a\u8fc7\u7ed3\u6784\u91cd\u6784\u5b9e\u73b0\u4e86\u52a8\u6001\u56db\u8db3\u884c\u8d70\u548c\u60ac\u505c\u98de\u884c\u529f\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u673a\u5668\u4eba\u5728\u4e0d\u540c\u64cd\u4f5c\u6a21\u5f0f\u4e0b\u9700\u6c42\u51b2\u7a81\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u5730\u9762\u79fb\u52a8\u548c\u7a7a\u4e2d\u98de\u884c\u7684\u96c6\u6210\u3002", "method": "\u91c7\u7528\u59ff\u6001\u64cd\u7eb5\u548c\u63a8\u529b\u77e2\u91cf\u6280\u672f\uff0c\u901a\u8fc7\u7ed3\u6784\u91cd\u6784\u5c06\u817f\u90e8\u7ed3\u6784\u91cd\u65b0\u7528\u4e8e\u52a8\u6001\u56db\u8db3\u884c\u8d70\u548c\u98de\u884c\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u52a8\u6001\u56db\u8db3\u884c\u8d70\u548c\u60ac\u505c\u529f\u80fd\uff0c\u9a8c\u8bc1\u4e86\u7ed3\u6784\u91cd\u6784\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "Husky v.2\u673a\u5668\u4eba\u901a\u8fc7\u521b\u65b0\u7684\u7ed3\u6784\u91cd\u6784\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u673a\u5668\u4eba\u9762\u4e34\u7684\u6a21\u5f0f\u51b2\u7a81\u95ee\u9898\uff0c\u4e3a\u5730\u9762-\u7a7a\u4e2d\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09513v1", "title": "Interpretable Generative and Discriminative Learning for Multimodal and Incomplete Clinical Data", "summary": "Real-world clinical problems are often characterized by multimodal data,\nusually associated with incomplete views and limited sample sizes in their\ncohorts, posing significant limitations for machine learning algorithms. In\nthis work, we propose a Bayesian approach designed to efficiently handle these\nchallenges while providing interpretable solutions. Our approach integrates (1)\na generative formulation to capture cross-view relationships with a\nsemi-supervised strategy, and (2) a discriminative task-oriented formulation to\nidentify relevant information for specific downstream objectives. This dual\ngenerative-discriminative formulation offers both general understanding and\ntask-specific insights; thus, it provides an automatic imputation of the\nmissing views while enabling robust inference across different data sources.\nThe potential of this approach becomes evident when applied to the multimodal\nclinical data, where our algorithm is able to capture and disentangle the\ncomplex interactions among biological, psychological, and sociodemographic\nmodalities.", "authors": ["Albert Belenguer-Llorens", "Carlos Sevilla-Salcedo", "Janaina Mourao-Miranda", "Vanessa G\u00f3mez-Verdejo"], "categories": ["stat.ML", "cs.LG"], "published": "2025-10-10T16:20:24Z", "pdf": "https://arxiv.org/pdf/2510.09513v1", "abs": "https://arxiv.org/abs/2510.09513v1", "comment": null, "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8d1d\u53f6\u65af\u65b9\u6cd5\u5904\u7406\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\u4e2d\u7684\u4e0d\u5b8c\u6574\u89c6\u56fe\u548c\u6709\u9650\u6837\u672c\u95ee\u9898\uff0c\u7ed3\u5408\u751f\u6210\u5f0f\u548c\u5224\u522b\u5f0f\u5b66\u4e60\u5b9e\u73b0\u81ea\u52a8\u63d2\u8865\u7f3a\u5931\u89c6\u56fe\u548c\u9c81\u68d2\u63a8\u7406\u3002", "motivation": "\u73b0\u5b9e\u4e34\u5e8a\u95ee\u9898\u901a\u5e38\u5177\u6709\u591a\u6a21\u6001\u6570\u636e\u7279\u5f81\uff0c\u4f46\u5b58\u5728\u89c6\u56fe\u4e0d\u5b8c\u6574\u548c\u6837\u672c\u91cf\u6709\u9650\u7684\u95ee\u9898\uff0c\u8fd9\u5bf9\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u6784\u6210\u91cd\u5927\u6311\u6218\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u96c6\u6210(1)\u751f\u6210\u5f0f\u516c\u5f0f\u6355\u6349\u8de8\u89c6\u56fe\u5173\u7cfb\u7684\u534a\u76d1\u7763\u7b56\u7565\uff0c\u548c(2)\u5224\u522b\u5f0f\u4efb\u52a1\u5bfc\u5411\u516c\u5f0f\u8bc6\u522b\u7279\u5b9a\u4e0b\u6e38\u76ee\u6807\u7684\u76f8\u5173\u4fe1\u606f\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6355\u83b7\u548c\u89e3\u5f00\u751f\u7269\u3001\u5fc3\u7406\u548c\u793e\u4f1a\u4eba\u53e3\u7b49\u591a\u6a21\u6001\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u4f5c\u7528\uff0c\u5728\u4e34\u5e8a\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u660e\u663e\u6f5c\u529b\u3002", "conclusion": "\u8fd9\u79cd\u53cc\u91cd\u751f\u6210-\u5224\u522b\u5f0f\u516c\u5f0f\u65e2\u63d0\u4f9b\u4e00\u822c\u7406\u89e3\u53c8\u63d0\u4f9b\u4efb\u52a1\u7279\u5b9a\u89c1\u89e3\uff0c\u5b9e\u73b0\u7f3a\u5931\u89c6\u56fe\u7684\u81ea\u52a8\u63d2\u8865\u548c\u8de8\u6570\u636e\u6e90\u7684\u9c81\u68d2\u63a8\u7406\u3002"}}
{"id": "2510.09511v1", "title": "Toggling stiffness via multistability", "summary": "Mechanical metamaterials enable unconventional and programmable mechanical\nresponses through structural design rather than material composition. In this\nwork, we introduce a multistable mechanical metamaterial that exhibits a\ntoggleable stiffness effect, where the effective shear stiffness switches\ndiscretely between stable configurations. The mechanical analysis of surrogate\nbeam models of the unit cell reveal that this behavior originates from the\nrotation transmitted by the support beams to the curved beam, which governs the\nbalance between bending and axial deformation. The stiffness ratio between the\ntwo states of the unit cell can be tuned by varying the slenderness of the\nsupport beams or by incorporating localized hinges that modulate rotational\ntransfer. Experiments on 3D-printed prototypes validate the numerical\npredictions, confirming consistent stiffness toggling across different\ngeometries. Finally, we demonstrate a monolithic soft clutch that leverages\nthis effect to achieve programmable, stepwise stiffness modulation. This work\nestablishes a design strategy for toggleable stiffness using multistable\nmetamaterials, paving the way for adaptive, lightweight, and autonomous systems\nin soft robotics and smart structures.", "authors": ["Hugo de Souza Oliveira", "Michele Curatolo", "Renate Sachse", "Edoardo Milana"], "categories": ["cond-mat.soft", "cs.RO", "physics.app-ph"], "published": "2025-10-10T16:16:50Z", "pdf": "https://arxiv.org/pdf/2510.09511v1", "abs": "https://arxiv.org/abs/2510.09511v1", "comment": null, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7a33\u6001\u673a\u68b0\u8d85\u6750\u6599\uff0c\u901a\u8fc7\u7ed3\u6784\u8bbe\u8ba1\u5b9e\u73b0\u53ef\u5207\u6362\u7684\u521a\u5ea6\u6548\u5e94\uff0c\u5176\u4e2d\u6709\u6548\u526a\u5207\u521a\u5ea6\u5728\u4e0d\u540c\u7a33\u5b9a\u6784\u578b\u4e4b\u95f4\u79bb\u6563\u5207\u6362\u3002\u8be5\u6750\u6599\u53ef\u7528\u4e8e\u8f6f\u673a\u5668\u4eba\u548c\u667a\u80fd\u7ed3\u6784\u4e2d\u7684\u81ea\u9002\u5e94\u7cfb\u7edf\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u901a\u8fc7\u7ed3\u6784\u8bbe\u8ba1\u800c\u975e\u6750\u6599\u6210\u5206\u5b9e\u73b0\u53ef\u7f16\u7a0b\u673a\u68b0\u54cd\u5e94\u7684\u673a\u68b0\u8d85\u6750\u6599\uff0c\u7279\u522b\u662f\u5b9e\u73b0\u53ef\u5207\u6362\u521a\u5ea6\u6548\u5e94\u7684\u591a\u7a33\u6001\u7cfb\u7edf\uff0c\u4e3a\u8f6f\u673a\u5668\u4eba\u548c\u667a\u80fd\u7ed3\u6784\u63d0\u4f9b\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u66ff\u4ee3\u6881\u6a21\u578b\u8fdb\u884c\u529b\u5b66\u5206\u6790\uff0c\u901a\u8fc7\u6539\u53d8\u652f\u6491\u6881\u7684\u7ec6\u957f\u6bd4\u6216\u5f15\u5165\u5c40\u90e8\u94f0\u94fe\u6765\u8c03\u8282\u65cb\u8f6c\u4f20\u9012\uff0c\u4ece\u800c\u63a7\u5236\u521a\u5ea6\u6bd4\u3002\u901a\u8fc73D\u6253\u5370\u539f\u578b\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6570\u503c\u9884\u6d4b\uff0c\u786e\u8ba4\u4e86\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u4e0b\u4e00\u81f4\u7684\u521a\u5ea6\u5207\u6362\u884c\u4e3a\u3002\u6210\u529f\u6f14\u793a\u4e86\u5229\u7528\u6b64\u6548\u5e94\u7684\u5355\u7247\u8f6f\u79bb\u5408\u5668\uff0c\u5b9e\u73b0\u4e86\u53ef\u7f16\u7a0b\u7684\u9010\u6b65\u521a\u5ea6\u8c03\u5236\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5efa\u7acb\u4e86\u4f7f\u7528\u591a\u7a33\u6001\u8d85\u6750\u6599\u5b9e\u73b0\u53ef\u5207\u6362\u521a\u5ea6\u7684\u8bbe\u8ba1\u7b56\u7565\uff0c\u4e3a\u8f6f\u673a\u5668\u4eba\u548c\u667a\u80fd\u7ed3\u6784\u4e2d\u7684\u81ea\u9002\u5e94\u3001\u8f7b\u91cf\u5316\u548c\u81ea\u4e3b\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.09510v1", "title": "MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval", "summary": "We introduce MRMR, the first expert-level multidisciplinary multimodal\nretrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries\nspanning 23 domains, with positive documents carefully verified by human\nexperts. Compared to prior benchmarks, MRMR introduces three key advancements.\nFirst, it challenges retrieval systems across diverse areas of expertise,\nenabling fine-grained model comparison across domains. Second, queries are\nreasoning-intensive, with images requiring deeper interpretation such as\ndiagnosing microscopic slides. We further introduce Contradiction Retrieval, a\nnovel task requiring models to identify conflicting concepts. Finally, queries\nand documents are constructed as image-text interleaved sequences. Unlike\nearlier benchmarks restricted to single images or unimodal documents, MRMR\noffers a realistic setting with multi-image queries and mixed-modality corpus\ndocuments. We conduct an extensive evaluation of 4 categories of multimodal\nretrieval systems and 14 frontier models on MRMR. The text embedding model\nQwen3-Embedding with LLM-generated image captions achieves the highest\nperformance, highlighting substantial room for improving multimodal retrieval\nmodels. Although latest multimodal models such as Ops-MM-Embedding perform\ncompetitively on expert-domain queries, they fall short on reasoning-intensive\ntasks. We believe that MRMR paves the way for advancing multimodal retrieval in\nmore realistic and challenging scenarios.", "authors": ["Siyue Zhang", "Yuan Gao", "Xiao Zhou", "Yilun Zhao", "Tingyu Song", "Arman Cohan", "Anh Tuan Luu", "Chen Zhao"], "categories": ["cs.IR"], "published": "2025-10-10T16:14:56Z", "pdf": "https://arxiv.org/pdf/2510.09510v1", "abs": "https://arxiv.org/abs/2510.09510v1", "comment": null, "AI": {"tldr": "MRMR\u662f\u9996\u4e2a\u9700\u8981\u5bc6\u96c6\u63a8\u7406\u7684\u4e13\u5bb6\u7ea7\u591a\u5b66\u79d1\u591a\u6a21\u6001\u68c0\u7d22\u57fa\u51c6\uff0c\u5305\u542b1502\u4e2a\u67e5\u8be2\uff0c\u6db5\u76d623\u4e2a\u9886\u57df\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u9886\u57df\u4e13\u5bb6\u7ea7\u67e5\u8be2\u3001\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u548c\u56fe\u50cf-\u6587\u672c\u4ea4\u9519\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u7684\u6311\u6218\u6027\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u9886\u57df\uff0c\u7f3a\u4e4f\u5bf9\u4e13\u5bb6\u7ea7\u591a\u5b66\u79d1\u77e5\u8bc6\u548c\u5bc6\u96c6\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u9700\u8981\u6784\u5efa\u66f4\u771f\u5b9e\u3001\u66f4\u5177\u6311\u6218\u6027\u7684\u591a\u6a21\u6001\u68c0\u7d22\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u5305\u542b1502\u4e2a\u67e5\u8be2\u7684\u57fa\u51c6\uff0c\u6db5\u76d623\u4e2a\u4e13\u4e1a\u9886\u57df\uff0c\u5f15\u5165\u77db\u76fe\u68c0\u7d22\u65b0\u4efb\u52a1\uff0c\u91c7\u7528\u56fe\u50cf-\u6587\u672c\u4ea4\u9519\u5e8f\u5217\u7684\u67e5\u8be2\u548c\u6587\u6863\u7ed3\u6784\uff0c\u8bc4\u4f304\u7c7b\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u548c14\u4e2a\u524d\u6cbf\u6a21\u578b\u3002", "result": "Qwen3-Embedding\u6a21\u578b\u7ed3\u5408LLM\u751f\u6210\u7684\u56fe\u50cf\u63cf\u8ff0\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u591a\u6a21\u6001\u6a21\u578b\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u4ecd\u6709\u4e0d\u8db3\uff0c\u663e\u793a\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u578b\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "MRMR\u57fa\u51c6\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u68c0\u7d22\u5728\u66f4\u771f\u5b9e\u548c\u5177\u6709\u6311\u6218\u6027\u573a\u666f\u4e2d\u7684\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u4e13\u5bb6\u7ea7\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.09507v1", "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs", "summary": "The ability to use, understand, and create tools is a hallmark of human\nintelligence, enabling sophisticated interaction with the physical world. For\nany general-purpose intelligent agent to achieve true versatility, it must also\nmaster these fundamental skills. While modern Multimodal Large Language Models\n(MLLMs) leverage their extensive common knowledge for high-level planning in\nembodied AI and in downstream Vision-Language-Action (VLA) models, the extent\nof their true understanding of physical tools remains unquantified. To bridge\nthis gap, we present PhysToolBench, the first benchmark dedicated to evaluating\nthe comprehension of physical tools by MLLMs. Our benchmark is structured as a\nVisual Question Answering (VQA) dataset comprising over 1,000 image-text pairs.\nIt assesses capabilities across three distinct difficulty levels: (1) Tool\nRecognition: Requiring the recognition of a tool's primary function. (2) Tool\nUnderstanding: Testing the ability to grasp the underlying principles of a\ntool's operation. (3) Tool Creation: Challenging the model to fashion a new\ntool from surrounding objects when conventional options are unavailable. Our\ncomprehensive evaluation of 32 MLLMs-spanning proprietary, open-source,\nspecialized embodied, and backbones in VLAs-reveals a significant deficiency in\ntool understanding. Furthermore, we provide an in-depth analysis and propose\npreliminary solutions. Code and dataset are publicly available.", "authors": ["Zixin Zhang", "Kanghao Chen", "Xingwang Lin", "Lutao Jiang", "Xu Zheng", "Yuanhuiyi Lyu", "Litao Guo", "Yinchuan Li", "Ying-Cong Chen"], "categories": ["cs.CV", "cs.RO"], "published": "2025-10-10T16:10:45Z", "pdf": "https://arxiv.org/pdf/2510.09507v1", "abs": "https://arxiv.org/abs/2510.09507v1", "comment": null, "AI": {"tldr": "PhysToolBench\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u7269\u7406\u5de5\u5177\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b1000\u591a\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u8bc4\u4f30\u5de5\u5177\u8bc6\u522b\u3001\u5de5\u5177\u7406\u89e3\u548c\u5de5\u5177\u521b\u9020\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\u3002\u5bf932\u4e2aMLLM\u7684\u8bc4\u4f30\u663e\u793a\u5b83\u4eec\u5728\u5de5\u5177\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002", "motivation": "\u5de5\u5177\u4f7f\u7528\u3001\u7406\u89e3\u548c\u521b\u9020\u662f\u4eba\u7c7b\u667a\u80fd\u7684\u6807\u5fd7\uff0c\u4f46\u73b0\u4ee3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u7269\u7406\u5de5\u5177\u7684\u771f\u6b63\u7406\u89e3\u7a0b\u5ea6\u5c1a\u672a\u91cf\u5316\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u4e13\u95e8\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efaPhysToolBench\u57fa\u51c6\uff0c\u4f5c\u4e3a\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5305\u542b1000\u591a\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u8bc4\u4f30\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff1a\u5de5\u5177\u8bc6\u522b\uff08\u8bc6\u522b\u5de5\u5177\u4e3b\u8981\u529f\u80fd\uff09\u3001\u5de5\u5177\u7406\u89e3\uff08\u7406\u89e3\u5de5\u5177\u64cd\u4f5c\u539f\u7406\uff09\u3001\u5de5\u5177\u521b\u9020\uff08\u5728\u5e38\u89c4\u5de5\u5177\u4e0d\u53ef\u7528\u65f6\u7528\u5468\u56f4\u7269\u4f53\u521b\u9020\u65b0\u5de5\u5177\uff09\u3002", "result": "\u5bf932\u4e2aMLLM\uff08\u5305\u62ec\u4e13\u6709\u3001\u5f00\u6e90\u3001\u4e13\u7528\u5177\u8eab\u6a21\u578b\u548cVLA\u9aa8\u5e72\u6a21\u578b\uff09\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u5de5\u5177\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u5de5\u5177\u7406\u89e3\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002\u4f5c\u8005\u63d0\u4f9b\u4e86\u6df1\u5165\u5206\u6790\u5e76\u63d0\u51fa\u4e86\u521d\u6b65\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09498v1", "title": "Unsupervised full-field Bayesian inference of orthotropic hyperelasticity from a single biaxial test: a myocardial case study", "summary": "Fully capturing this behavior in traditional homogenized tissue testing\nrequires the excitation of multiple deformation modes, i.e. combined triaxial\nshear tests and biaxial stretch tests. Inherently, such multimodal experimental\nprotocols necessitate multiple tissue samples and extensive sample\nmanipulations. Intrinsic inter-sample variability and manipulation-induced\ntissue damage might have an adverse effect on the inversely identified tissue\nbehavior. In this work, we aim to overcome this gap by focusing our attention\nto the use of heterogeneous deformation profiles in a parameter estimation\nproblem. More specifically, we adapt EUCLID, an unsupervised method for the\nautomated discovery of constitutive models, towards the purpose of parameter\nidentification for highly nonlinear, orthotropic constitutive models using a\nBayesian inference approach and three-dimensional continuum elements. We\nshowcase its strength to quantitatively infer, with varying noise levels, the\nmaterial model parameters of synthetic myocardial tissue slabs from a single\nheterogeneous biaxial stretch test. This method shows good agreement with the\nground-truth simulations and with corresponding credibility intervals. Our work\nhighlights the potential for characterizing highly nonlinear and orthotropic\nmaterial models from a single biaxial stretch test with uncertainty\nquantification.", "authors": ["Rogier P. Krijnen", "Akshay Joshi", "Siddhant Kumar", "Mathias Peirlinck"], "categories": ["q-bio.TO", "cs.CE", "cs.LG"], "published": "2025-10-10T15:59:49Z", "pdf": "https://arxiv.org/pdf/2510.09498v1", "abs": "https://arxiv.org/abs/2510.09498v1", "comment": null, "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u63a8\u7406\u7684\u65e0\u76d1\u7763\u65b9\u6cd5EUCLID\uff0c\u7528\u4e8e\u4ece\u5355\u4e2a\u5f02\u8d28\u53cc\u8f74\u62c9\u4f38\u8bd5\u9a8c\u4e2d\u8bc6\u522b\u9ad8\u5ea6\u975e\u7ebf\u6027\u6b63\u4ea4\u5404\u5411\u5f02\u6027\u6750\u6599\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u5747\u8d28\u7ec4\u7ec7\u6d4b\u8bd5\u9700\u8981\u591a\u79cd\u53d8\u5f62\u6a21\u5f0f\uff08\u5982\u4e09\u8f74\u526a\u5207\u548c\u53cc\u8f74\u62c9\u4f38\uff09\uff0c\u8fd9\u9700\u8981\u591a\u4e2a\u6837\u672c\u548c\u5927\u91cf\u64cd\u4f5c\uff0c\u5b58\u5728\u6837\u672c\u95f4\u53d8\u5f02\u6027\u548c\u64cd\u4f5c\u635f\u4f24\u95ee\u9898\u3002", "method": "\u91c7\u7528EUCLID\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u63a8\u7406\u65b9\u6cd5\u548c\u4e09\u7ef4\u8fde\u7eed\u4f53\u5355\u5143\uff0c\u4ece\u5355\u4e2a\u5f02\u8d28\u53cc\u8f74\u62c9\u4f38\u8bd5\u9a8c\u4e2d\u8bc6\u522b\u9ad8\u5ea6\u975e\u7ebf\u6027\u6b63\u4ea4\u5404\u5411\u5f02\u6027\u672c\u6784\u6a21\u578b\u53c2\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u80fd\u591f\u5b9a\u91cf\u63a8\u65ad\u5408\u6210\u5fc3\u808c\u7ec4\u7ec7\u677f\u7684\u6750\u6599\u6a21\u578b\u53c2\u6570\uff0c\u4e0e\u771f\u5b9e\u6a21\u62df\u7ed3\u679c\u543b\u5408\u826f\u597d\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5e94\u7684\u53ef\u4fe1\u533a\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u4ece\u5355\u4e2a\u53cc\u8f74\u62c9\u4f38\u8bd5\u9a8c\u4e2d\u8868\u5f81\u9ad8\u5ea6\u975e\u7ebf\u6027\u548c\u6b63\u4ea4\u5404\u5411\u5f02\u6027\u6750\u6599\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u5e76\u5177\u5907\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002"}}
{"id": "2510.09497v1", "title": "Autonomous Soft Robotic Guidewire Navigation via Imitation Learning", "summary": "In endovascular surgery, endovascular interventionists push a thin tube\ncalled a catheter, guided by a thin wire to a treatment site inside the\npatient's blood vessels to treat various conditions such as blood clots,\naneurysms, and malformations. Guidewires with robotic tips can enhance\nmaneuverability, but they present challenges in modeling and control.\nAutomation of soft robotic guidewire navigation has the potential to overcome\nthese challenges, increasing the precision and safety of endovascular\nnavigation. In other surgical domains, end-to-end imitation learning has shown\npromising results. Thus, we develop a transformer-based imitation learning\nframework with goal conditioning, relative action outputs, and automatic\ncontrast dye injections to enable generalizable soft robotic guidewire\nnavigation in an aneurysm targeting task. We train the model on 36 different\nmodular bifurcated geometries, generating 647 total demonstrations under\nsimulated fluoroscopy, and evaluate it on three previously unseen vascular\ngeometries. The model can autonomously drive the tip of the robot to the\naneurysm location with a success rate of 83% on the unseen geometries,\noutperforming several baselines. In addition, we present ablation and baseline\nstudies to evaluate the effectiveness of each design and data collection\nchoice. Project website: https://softrobotnavigation.github.io/", "authors": ["Noah Barnes", "Ji Woong Kim", "Lingyun Di", "Hannah Qu", "Anuruddha Bhattacharjee", "Miroslaw Janowski", "Dheeraj Gandhi", "Bailey Felix", "Shaopeng Jiang", "Olivia Young", "Mark Fuge", "Ryan D. Sochol", "Jeremy D. Brown", "Axel Krieger"], "categories": ["cs.RO", "cs.AI"], "published": "2025-10-10T15:57:09Z", "pdf": "https://arxiv.org/pdf/2510.09497v1", "abs": "https://arxiv.org/abs/2510.09497v1", "comment": null, "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8f6f\u4f53\u673a\u5668\u4eba\u5bfc\u4e1d\u5728\u8840\u7ba1\u5185\u5bfc\u822a\uff0c\u5728\u52a8\u8109\u7624\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8fbe\u523083%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u8840\u7ba1\u5185\u624b\u672f\u4e2d\u673a\u5668\u4eba\u5bfc\u4e1d\u5bfc\u822a\u7684\u5efa\u6a21\u548c\u63a7\u5236\u6311\u6218\uff0c\u63d0\u9ad8\u8840\u7ba1\u5185\u5bfc\u822a\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eTransformer\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u76ee\u6807\u6761\u4ef6\u3001\u76f8\u5bf9\u52a8\u4f5c\u8f93\u51fa\u548c\u81ea\u52a8\u5bf9\u6bd4\u67d3\u6599\u6ce8\u5c04\uff0c\u572836\u79cd\u4e0d\u540c\u5206\u53c9\u51e0\u4f55\u7ed3\u6784\u4e0a\u8bad\u7ec3647\u4e2a\u6f14\u793a\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u8840\u7ba1\u51e0\u4f55\u7ed3\u6784\u4e0a\uff0c\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u5c06\u673a\u5668\u4eba\u5c16\u7aef\u5bfc\u822a\u81f3\u52a8\u8109\u7624\u4f4d\u7f6e\uff0c\u6210\u529f\u7387\u8fbe\u523083%\uff0c\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u53ef\u6cdb\u5316\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u5bfc\u4e1d\u5bfc\u822a\uff0c\u4e3a\u8840\u7ba1\u5185\u624b\u672f\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09483v1", "title": "FOGMACHINE -- Leveraging Discrete-Event Simulation and Scene Graphs for Modeling Hierarchical, Interconnected Environments under Partial Observations from Mobile Agents", "summary": "Dynamic Scene Graphs (DSGs) provide a structured representation of\nhierarchical, interconnected environments, but current approaches struggle to\ncapture stochastic dynamics, partial observability, and multi-agent activity.\nThese aspects are critical for embodied AI, where agents must act under\nuncertainty and delayed perception. We introduce FOGMACHINE , an open-source\nframework that fuses DSGs with discrete-event simulation to model object\ndynamics, agent observations, and interactions at scale. This setup enables the\nstudy of uncertainty propagation, planning under limited perception, and\nemergent multi-agent behavior. Experiments in urban scenarios illustrate\nrealistic temporal and spatial patterns while revealing the challenges of\nbelief estimation under sparse observations. By combining structured\nrepresentations with efficient simulation, FOGMACHINE establishes an effective\ntool for benchmarking, model training, and advancing embodied AI in complex,\nuncertain environments.", "authors": ["Lars Ohnemus", "Nils Hantke", "Max Wei\u00dfer", "Kai Furmans"], "categories": ["cs.RO"], "published": "2025-10-10T15:45:31Z", "pdf": "https://arxiv.org/pdf/2510.09483v1", "abs": "https://arxiv.org/abs/2510.09483v1", "comment": "submitted to the IEEE for possible publication; 8 pages, 3 figures, 1\n  table", "AI": {"tldr": "FOGMACHINE\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u5c06\u52a8\u6001\u573a\u666f\u56fe\u4e0e\u79bb\u6563\u4e8b\u4ef6\u6a21\u62df\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u5efa\u6a21\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bf9\u8c61\u52a8\u6001\u3001\u667a\u80fd\u4f53\u89c2\u5bdf\u548c\u4ea4\u4e92\uff0c\u7279\u522b\u5173\u6ce8\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u548c\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u3002", "motivation": "\u5f53\u524d\u52a8\u6001\u573a\u666f\u56fe\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u968f\u673a\u52a8\u6001\u3001\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u548c\u591a\u667a\u80fd\u4f53\u6d3b\u52a8\uff0c\u800c\u8fd9\u4e9b\u5bf9\u4e8e\u5177\u8eabAI\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u5ef6\u8fdf\u611f\u77e5\u4e0b\u7684\u884c\u52a8\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u878d\u5408\u52a8\u6001\u573a\u666f\u56fe\u548c\u79bb\u6563\u4e8b\u4ef6\u6a21\u62df\u6765\u5efa\u6a21\u5bf9\u8c61\u52a8\u6001\u3001\u667a\u80fd\u4f53\u89c2\u5bdf\u548c\u4ea4\u4e92\uff0c\u652f\u6301\u5927\u89c4\u6a21\u4eff\u771f\u3002", "result": "\u5728\u57ce\u5e02\u573a\u666f\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u771f\u5b9e\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u6a21\u5f0f\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5728\u7a00\u758f\u89c2\u5bdf\u4e0b\u4fe1\u5ff5\u4f30\u8ba1\u7684\u6311\u6218\u3002", "conclusion": "FOGMACHINE\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u8868\u793a\u548c\u9ad8\u6548\u6a21\u62df\uff0c\u4e3a\u590d\u6742\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u57fa\u51c6\u6d4b\u8bd5\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u5177\u8eabAI\u53d1\u5c55\u5efa\u7acb\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.09474v1", "title": "Multimodal Policy Internalization for Conversational Agents", "summary": "Modern conversational agents like ChatGPT and Alexa+ rely on predefined\npolicies specifying metadata, response styles, and tool-usage rules. As these\nLLM-based systems expand to support diverse business and user queries, such\npolicies, often implemented as in-context prompts, are becoming increasingly\ncomplex and lengthy, making faithful adherence difficult and imposing large\nfixed computational costs. With the rise of multimodal agents, policies that\ngovern visual and multimodal behaviors are critical but remain understudied.\nPrior prompt-compression work mainly shortens task templates and\ndemonstrations, while existing policy-alignment studies focus only on\ntext-based safety rules. We introduce Multimodal Policy Internalization (MPI),\na new task that internalizes reasoning-intensive multimodal policies into model\nparameters, enabling stronger policy-following without including the policy\nduring inference. MPI poses unique data and algorithmic challenges. We build\ntwo datasets spanning synthetic and real-world decision-making and tool-using\ntasks and propose TriMPI, a three-stage training framework. TriMPI first\ninjects policy knowledge via continual pretraining, then performs supervised\nfinetuning, and finally applies PolicyRollout, a GRPO-style reinforcement\nlearning extension that augments rollouts with policy-aware responses for\ngrounded exploration. TriMPI achieves notable gains in end-to-end accuracy,\ngeneralization, and robustness to forgetting. As the first work on multimodal\npolicy internalization, we provide datasets, training recipes, and\ncomprehensive evaluations to foster future research. Project page:\nhttps://mikewangwzhl.github.io/TriMPI.", "authors": ["Zhenhailong Wang", "Jiateng Liu", "Amin Fazel", "Ritesh Sarkhel", "Xing Fan", "Xiang Li", "Chenlei Guo", "Heng Ji", "Ruhi Sarikaya"], "categories": ["cs.CL", "cs.AI"], "published": "2025-10-10T15:28:30Z", "pdf": "https://arxiv.org/pdf/2510.09474v1", "abs": "https://arxiv.org/abs/2510.09474v1", "comment": null, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u7b56\u7565\u5185\u5316(MPI)\u4efb\u52a1\uff0c\u901a\u8fc7TriMPI\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u5c06\u590d\u6742\u7684\u591a\u6a21\u6001\u7b56\u7565\u5185\u5316\u5230\u6a21\u578b\u53c2\u6570\u4e2d\uff0c\u5b9e\u73b0\u65e0\u9700\u63a8\u7406\u65f6\u5305\u542b\u7b56\u7565\u7684\u5f3a\u7b56\u7565\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3\u5bf9\u8bdd\u7cfb\u7edf\u4f9d\u8d56\u9884\u5b9a\u4e49\u7b56\u7565\uff0c\u4f46\u968f\u7740\u7cfb\u7edf\u6269\u5c55\uff0c\u8fd9\u4e9b\u7b56\u7565\u53d8\u5f97\u590d\u6742\u5197\u957f\uff0c\u5bfc\u81f4\u9075\u5faa\u56f0\u96be\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u591a\u6a21\u6001\u7b56\u7565\u7ba1\u7406\u5173\u952e\u4f46\u7814\u7a76\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u538b\u7f29\u548c\u5b89\u5168\u89c4\u5219\u5bf9\u9f50\u3002", "method": "\u63d0\u51faTriMPI\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1)\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u6ce8\u5165\u7b56\u7565\u77e5\u8bc6\uff1b2)\u76d1\u7763\u5fae\u8c03\uff1b3)PolicyRollout\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u7528\u7b56\u7565\u611f\u77e5\u54cd\u5e94\u8fdb\u884c\u57fa\u4e8e\u7b56\u7565\u7684\u63a2\u7d22\u3002", "result": "TriMPI\u5728\u7aef\u5230\u7aef\u51c6\u786e\u6027\u3001\u6cdb\u5316\u6027\u548c\u6297\u9057\u5fd8\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u6784\u5efa\u4e86\u4e24\u4e2a\u6db5\u76d6\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u51b3\u7b56\u4e0e\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u4f5c\u4e3a\u591a\u6a21\u6001\u7b56\u7565\u5185\u5316\u7684\u9996\u4e2a\u5de5\u4f5c\uff0c\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u65b9\u6cd5\u548c\u5168\u9762\u8bc4\u4f30\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.09469v1", "title": "Scalable Multi-Agent Path Finding using Collision-Aware Dynamic Alert Mask and a Hybrid Execution Strategy", "summary": "Multi-agent pathfinding (MAPF) remains a critical problem in robotics and\nautonomous systems, where agents must navigate shared spaces efficiently while\navoiding conflicts. Traditional centralized algorithms that have global\ninformation, such as Conflict-Based Search (CBS), provide high-quality\nsolutions but become computationally expensive in large-scale scenarios due to\nthe combinatorial explosion of conflicts that need resolution. Conversely,\ndistributed approaches that have local information, particularly learning-based\nmethods, offer better scalability by operating with relaxed information\navailability, yet often at the cost of solution quality. To address these\nlimitations, we propose a hybrid framework that combines decentralized path\nplanning with a lightweight centralized coordinator. Our framework leverages\nreinforcement learning (RL) for decentralized planning, enabling agents to\nadapt their planning based on minimal, targeted alerts--such as static\nconflict-cell flags or brief conflict tracks--that are dynamically shared\ninformation from the central coordinator for effective conflict resolution. We\nempirically study the effect of the information available to an agent on its\nplanning performance. Our approach reduces the inter-agent information sharing\ncompared to fully centralized and distributed methods, while still consistently\nfinding feasible, collision-free solutions--even in large-scale scenarios\nhaving higher agent counts.", "authors": ["Bharath Muppasani", "Ritirupa Dey", "Biplav Srivastava", "Vignesh Narayanan"], "categories": ["cs.MA", "cs.AI", "cs.RO"], "published": "2025-10-10T15:25:40Z", "pdf": "https://arxiv.org/pdf/2510.09469v1", "abs": "https://arxiv.org/abs/2510.09469v1", "comment": null, "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53bb\u4e2d\u5fc3\u5316\u8def\u5f84\u89c4\u5212\u548c\u8f7b\u91cf\u7ea7\u4e2d\u5fc3\u5316\u534f\u8c03\u5668\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u53bb\u4e2d\u5fc3\u5316\u89c4\u5212\uff0c\u5229\u7528\u6700\u5c0f\u5316\u4fe1\u606f\u5171\u4eab\u5b9e\u73b0\u6709\u6548\u51b2\u7a81\u89e3\u51b3\u3002", "motivation": "\u4f20\u7edf\u4e2d\u5fc3\u5316\u7b97\u6cd5\u5728\u5927\u578b\u573a\u666f\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u5206\u5e03\u5f0f\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u53ef\u6269\u5c55\u6027\u597d\u4f46\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u8f83\u4f4e\uff0c\u9700\u8981\u5e73\u8861\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u53bb\u4e2d\u5fc3\u5316\u8def\u5f84\u89c4\u5212\uff0c\u914d\u5408\u8f7b\u91cf\u7ea7\u4e2d\u5fc3\u5316\u534f\u8c03\u5668\u52a8\u6001\u5171\u4eab\u9759\u6001\u51b2\u7a81\u5355\u5143\u6807\u5fd7\u6216\u7b80\u77ed\u51b2\u7a81\u8f68\u8ff9\u7b49\u6700\u5c0f\u5316\u4fe1\u606f\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u667a\u80fd\u4f53\u95f4\u4fe1\u606f\u5171\u4eab\u7684\u540c\u65f6\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u6301\u7eed\u627e\u5230\u53ef\u884c\u7684\u65e0\u78b0\u649e\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6df7\u5408\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2510.09458v1", "title": "SilvaScenes: Tree Segmentation and Species Classification from Under-Canopy Images in Natural Forests", "summary": "Interest in robotics for forest management is growing, but perception in\ncomplex, natural environments remains a significant hurdle. Conditions such as\nheavy occlusion, variable lighting, and dense vegetation pose challenges to\nautomated systems, which are essential for precision forestry, biodiversity\nmonitoring, and the automation of forestry equipment. These tasks rely on\nadvanced perceptual capabilities, such as detection and fine-grained species\nclassification of individual trees. Yet, existing datasets are inadequate to\ndevelop such perception systems, as they often focus on urban settings or a\nlimited number of species. To address this, we present SilvaScenes, a new\ndataset for instance segmentation of tree species from under-canopy images.\nCollected across five bioclimatic domains in Quebec, Canada, SilvaScenes\nfeatures 1476 trees from 24 species with annotations from forestry experts. We\ndemonstrate the relevance and challenging nature of our dataset by benchmarking\nmodern deep learning approaches for instance segmentation. Our results show\nthat, while tree segmentation is easy, with a top mean average precision (mAP)\nof 67.65%, species classification remains a significant challenge with an mAP\nof only 35.69%. Our dataset and source code will be available at\nhttps://github.com/norlab-ulaval/SilvaScenes.", "authors": ["David-Alexandre Duclos", "William Guimont-Martin", "Gabriel Jeanson", "Arthur Larochelle-Tremblay", "Th\u00e9o Defosse", "Fr\u00e9d\u00e9ric Moore", "Philippe Nolet", "Fran\u00e7ois Pomerleau", "Philippe Gigu\u00e8re"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "published": "2025-10-10T15:08:35Z", "pdf": "https://arxiv.org/pdf/2510.09458v1", "abs": "https://arxiv.org/abs/2510.09458v1", "comment": "8 pages, 5 figures", "AI": {"tldr": "\u63d0\u51fa\u4e86SilvaScenes\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4ece\u6797\u4e0b\u56fe\u50cf\u8fdb\u884c\u6811\u79cd\u5b9e\u4f8b\u5206\u5272\uff0c\u5305\u542b24\u4e2a\u7269\u79cd\u76841476\u68f5\u6811\uff0c\u5728\u4e94\u4e2a\u751f\u7269\u6c14\u5019\u533a\u57df\u6536\u96c6\u3002\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u6811\u5206\u5272\u76f8\u5bf9\u5bb9\u6613\uff08mAP 67.65%\uff09\uff0c\u4f46\u6811\u79cd\u5206\u7c7b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff08mAP 35.69%\uff09\u3002", "motivation": "\u68ee\u6797\u7ba1\u7406\u4e2d\u7684\u673a\u5668\u4eba\u6280\u672f\u9700\u6c42\u589e\u957f\uff0c\u4f46\u5728\u590d\u6742\u81ea\u7136\u73af\u5883\u4e2d\u611f\u77e5\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u969c\u788d\u3002\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u5173\u6ce8\u57ce\u5e02\u73af\u5883\u6216\u7269\u79cd\u6709\u9650\uff0c\u65e0\u6cd5\u5f00\u53d1\u5148\u8fdb\u7684\u611f\u77e5\u7cfb\u7edf\u6765\u652f\u6301\u7cbe\u51c6\u6797\u4e1a\u3001\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u548c\u6797\u4e1a\u8bbe\u5907\u81ea\u52a8\u5316\u3002", "method": "\u5728\u52a0\u62ff\u5927\u9b41\u5317\u514b\u7684\u4e94\u4e2a\u751f\u7269\u6c14\u5019\u533a\u57df\u6536\u96c6\u6797\u4e0b\u56fe\u50cf\uff0c\u7531\u6797\u4e1a\u4e13\u5bb6\u6807\u6ce8\u4e8624\u4e2a\u7269\u79cd\u76841476\u68f5\u6811\uff0c\u521b\u5efa\u4e86SilvaScenes\u6570\u636e\u96c6\u3002\u4f7f\u7528\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6811\u5206\u5272\u8868\u73b0\u826f\u597d\uff0c\u6700\u9ad8\u5e73\u5747\u7cbe\u5ea6\u4e3a67.65%\uff0c\u4f46\u6811\u79cd\u5206\u7c7b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u5e73\u5747\u7cbe\u5ea6\u4ec5\u4e3a35.69%\u3002", "conclusion": "SilvaScenes\u6570\u636e\u96c6\u586b\u8865\u4e86\u68ee\u6797\u73af\u5883\u4e2d\u6811\u79cd\u5b9e\u4f8b\u5206\u5272\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u6797\u4e1a\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u540c\u65f6\u7a81\u663e\u4e86\u5728\u590d\u6742\u81ea\u7136\u73af\u5883\u4e2d\u6811\u79cd\u5206\u7c7b\u7684\u6301\u7eed\u6311\u6218\u3002"}}
{"id": "2510.09447v1", "title": "Quantization of charged fields in the presence of intense electromagnetic fields", "summary": "This thesis applies techniques from quantum field theory in curved spacetimes\nto study particle creation in external fields, focusing on the Schwinger effect\n(i.e., the production of particle-antiparticle pairs by intense electric\nfields). Although experimental verification remains out of reach, theoretical\nanalysis advances our understanding of this phenomenon and its broader\nimplications.\n  The work develops the theoretical framework for quantizing charged fields in\nnontrivial backgrounds, addressing the ambiguities in defining the quantum\nvacuum and extending the concept of states of low energy from cosmology to the\nSchwinger setting. It examines how different quantizations allow for unitary\ntime evolution, and generalizes the quantum Vlasov equation to encompass a\nwider range of schemes. An operational perspective reveals that quantum\nambiguities have genuine physical meaning, being linked to different modes of\ninteraction and measurement. The study also analyzes dynamical transitions\nbetween static regimes and their impact on observables, with applications to\nanalog cosmological expansion in Bose-Einstein condensates and the Schwinger\neffect itself. In the context of black holes, the thesis shows that the\nSchwinger effect prevents the formation of black holes from light under current\nconditions and investigates fermionic charge superradiance, demonstrating how\nquantum effects lead to black-hole discharge (a process without classical\nanalogue).\n  Overall, the thesis underscores the fundamental role of external\nelectromagnetic and gravitational fields in defining particles and vacua,\nrevealing the limits of flat-spacetime intuition and identifying purely quantum\nphenomena with implications for black-hole physics. It contributes to bridging\nthe conceptual gap between general relativity and quantum field theory and\noffers new tools toward a consistent quantum description of spacetime.", "authors": ["\u00c1lvaro \u00c1lvarez-Dom\u00ednguez"], "categories": ["hep-th"], "published": "2025-10-10T14:58:28Z", "pdf": "https://arxiv.org/pdf/2510.09447v1", "abs": "https://arxiv.org/abs/2510.09447v1", "comment": "201 pages, 23 figures, PhD thesis", "AI": {"tldr": "\u8be5\u8bba\u6587\u5e94\u7528\u5f2f\u66f2\u65f6\u7a7a\u4e2d\u7684\u91cf\u5b50\u573a\u8bba\u6280\u672f\u7814\u7a76\u5916\u90e8\u573a\u4e2d\u7684\u7c92\u5b50\u4ea7\u751f\uff0c\u91cd\u70b9\u5173\u6ce8\u65bd\u6e29\u683c\u6548\u5e94\uff08\u5f3a\u7535\u573a\u4ea7\u751f\u7c92\u5b50-\u53cd\u7c92\u5b50\u5bf9\uff09\u3002\u901a\u8fc7\u53d1\u5c55\u975e\u5e73\u51e1\u80cc\u666f\u4e2d\u5e26\u7535\u573a\u91cf\u5b50\u5316\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5206\u6790\u91cf\u5b50\u771f\u7a7a\u5b9a\u4e49\u7684\u6a21\u7cca\u6027\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u91cf\u5b50\u5316\u65b9\u6848\u5982\u4f55\u5b9e\u73b0\u5e7a\u6b63\u65f6\u95f4\u6f14\u5316\u3002", "motivation": "\u7814\u7a76\u65bd\u6e29\u683c\u6548\u5e94\u53ca\u5176\u5728\u9ed1\u6d1e\u7269\u7406\u4e2d\u7684\u610f\u4e49\uff0c\u63a2\u7d22\u5916\u90e8\u7535\u78c1\u573a\u548c\u5f15\u529b\u573a\u5728\u5b9a\u4e49\u7c92\u5b50\u548c\u771f\u7a7a\u6001\u4e2d\u7684\u57fa\u672c\u4f5c\u7528\uff0c\u63ed\u793a\u5e73\u76f4\u65f6\u7a7a\u76f4\u89c9\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5e7f\u4e49\u76f8\u5bf9\u8bba\u4e0e\u91cf\u5b50\u573a\u8bba\u7684\u6982\u5ff5\u9e3f\u6c9f\u642d\u5efa\u6865\u6881\u3002", "method": "\u5e94\u7528\u5f2f\u66f2\u65f6\u7a7a\u91cf\u5b50\u573a\u8bba\u6280\u672f\uff0c\u53d1\u5c55\u5e26\u7535\u573a\u5728\u975e\u5e73\u51e1\u80cc\u666f\u4e2d\u7684\u91cf\u5b50\u5316\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5b87\u5b99\u5b66\u4e2d\u7684\u4f4e\u80fd\u6001\u6982\u5ff5\u6269\u5c55\u5230\u65bd\u6e29\u683c\u8bbe\u7f6e\uff0c\u63a8\u5e7f\u91cf\u5b50\u5f17\u62c9\u7d22\u592b\u65b9\u7a0b\uff0c\u5206\u6790\u4e0d\u540c\u91cf\u5b50\u5316\u65b9\u6848\u7684\u5e7a\u6b63\u6027\uff0c\u5e76\u91c7\u7528\u64cd\u4f5c\u89c6\u89d2\u7814\u7a76\u91cf\u5b50\u6a21\u7cca\u6027\u7684\u7269\u7406\u610f\u4e49\u3002", "result": "\u8bc1\u660e\u4e86\u65bd\u6e29\u683c\u6548\u5e94\u5728\u5f53\u524d\u6761\u4ef6\u4e0b\u963b\u6b62\u4e86\u5149\u5f62\u6210\u9ed1\u6d1e\uff0c\u7814\u7a76\u4e86\u8d39\u7c73\u5b50\u7535\u8377\u8d85\u8f90\u5c04\u73b0\u8c61\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u6548\u5e94\u5982\u4f55\u5bfc\u81f4\u9ed1\u6d1e\u653e\u7535\uff08\u65e0\u7ecf\u5178\u7c7b\u6bd4\u7684\u8fc7\u7a0b\uff09\u3002\u63ed\u793a\u4e86\u91cf\u5b50\u6a21\u7cca\u6027\u4e0e\u4e0d\u540c\u76f8\u4e92\u4f5c\u7528\u548c\u6d4b\u91cf\u6a21\u5f0f\u76f8\u5173\uff0c\u5177\u6709\u771f\u5b9e\u7684\u7269\u7406\u610f\u4e49\u3002", "conclusion": "\u8be5\u8bba\u6587\u5f3a\u8c03\u4e86\u5916\u90e8\u7535\u78c1\u573a\u548c\u5f15\u529b\u573a\u5728\u5b9a\u4e49\u7c92\u5b50\u548c\u771f\u7a7a\u6001\u4e2d\u7684\u57fa\u672c\u4f5c\u7528\uff0c\u8bc6\u522b\u4e86\u5177\u6709\u9ed1\u6d1e\u7269\u7406\u610f\u4e49\u7684\u7eaf\u91cf\u5b50\u73b0\u8c61\uff0c\u4e3a\u6784\u5efa\u4e00\u81f4\u7684\u65f6\u7a7a\u91cf\u5b50\u63cf\u8ff0\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u5f25\u5408\u5e7f\u4e49\u76f8\u5bf9\u8bba\u4e0e\u91cf\u5b50\u573a\u8bba\u4e4b\u95f4\u7684\u6982\u5ff5\u5dee\u8ddd\u3002"}}
{"id": "2510.09438v1", "title": "Mono4DEditor: Text-Driven 4D Scene Editing from Monocular Video via Point-Level Localization of Language-Embedded Gaussians", "summary": "Editing 4D scenes reconstructed from monocular videos based on text prompts\nis a valuable yet challenging task with broad applications in content creation\nand virtual environments. The key difficulty lies in achieving semantically\nprecise edits in localized regions of complex, dynamic scenes, while preserving\nthe integrity of unedited content. To address this, we introduce Mono4DEditor,\na novel framework for flexible and accurate text-driven 4D scene editing. Our\nmethod augments 3D Gaussians with quantized CLIP features to form a\nlanguage-embedded dynamic representation, enabling efficient semantic querying\nof arbitrary spatial regions. We further propose a two-stage point-level\nlocalization strategy that first selects candidate Gaussians via CLIP\nsimilarity and then refines their spatial extent to improve accuracy. Finally,\ntargeted edits are performed on localized regions using a diffusion-based video\nediting model, with flow and scribble guidance ensuring spatial fidelity and\ntemporal coherence. Extensive experiments demonstrate that Mono4DEditor enables\nhigh-quality, text-driven edits across diverse scenes and object types, while\npreserving the appearance and geometry of unedited areas and surpassing prior\napproaches in both flexibility and visual fidelity.", "authors": ["Jin-Chuan Shi", "Chengye Su", "Jiajun Wang", "Ariel Shamir", "Miao Wang"], "categories": ["cs.CV"], "published": "2025-10-10T14:49:49Z", "pdf": "https://arxiv.org/pdf/2510.09438v1", "abs": "https://arxiv.org/abs/2510.09438v1", "comment": "19 pages, 9 figures", "AI": {"tldr": "Mono4DEditor\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7f16\u8f91\u5355\u76ee\u89c6\u9891\u91cd\u5efa4D\u573a\u666f\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316CLIP\u7279\u5f81\u589e\u5f3a3D\u9ad8\u65af\u8868\u793a\uff0c\u5b9e\u73b0\u8bed\u4e49\u7cbe\u786e\u7684\u5c40\u90e8\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u672a\u7f16\u8f91\u5185\u5bb9\u7684\u5b8c\u6574\u6027\u3002", "motivation": "\u4ece\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u76844D\u573a\u666f\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u8fdb\u884c\u7f16\u8f91\u5728\u5185\u5bb9\u521b\u4f5c\u548c\u865a\u62df\u73af\u5883\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u9762\u4e34\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u4e2d\u5b9e\u73b0\u8bed\u4e49\u7cbe\u786e\u5c40\u90e8\u7f16\u8f91\u5e76\u4fdd\u6301\u672a\u7f16\u8f91\u5185\u5bb9\u5b8c\u6574\u6027\u7684\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u7528\u91cf\u5316CLIP\u7279\u5f81\u589e\u5f3a3D\u9ad8\u65af\u5f62\u6210\u8bed\u8a00\u5d4c\u5165\u52a8\u6001\u8868\u793a\uff1b2\uff09\u4e24\u9636\u6bb5\u70b9\u7ea7\u5b9a\u4f4d\u7b56\u7565\uff08CLIP\u76f8\u4f3c\u5ea6\u5019\u9009\u9009\u62e9+\u7a7a\u95f4\u8303\u56f4\u7ec6\u5316\uff09\uff1b3\uff09\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u7f16\u8f91\u6a21\u578b\u8fdb\u884c\u76ee\u6807\u7f16\u8f91\uff0c\u4f7f\u7528\u6d41\u548c\u6d82\u9e26\u5f15\u5bfc\u786e\u4fdd\u7a7a\u95f4\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMono4DEditor\u80fd\u591f\u5728\u591a\u6837\u5316\u573a\u666f\u548c\u5bf9\u8c61\u7c7b\u578b\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u9a71\u52a8\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u672a\u7f16\u8f91\u533a\u57df\u7684\u5916\u89c2\u548c\u51e0\u4f55\u7279\u6027\uff0c\u5728\u7075\u6d3b\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "Mono4DEditor\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e864D\u573a\u666f\u7f16\u8f91\u4e2d\u7684\u8bed\u4e49\u7cbe\u786e\u6027\u548c\u5c40\u90e8\u5316\u6311\u6218\uff0c\u4e3a\u6587\u672c\u9a71\u52a8\u7684\u52a8\u6001\u573a\u666f\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09424v1", "title": "The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach", "summary": "This paper presents a comparative study of context management strategies for\nend-to-end Spoken Dialog State Tracking using Speech-LLMs. We systematically\nevaluate traditional multimodal context (combining text history and spoken\ncurrent turn), full spoken history, and compressed spoken history approaches.\nOur experiments on the SpokenWOZ corpus demonstrate that providing the full\nspoken conversation as input yields the highest performance among models of\nsimilar size, significantly surpassing prior methods. Furthermore, we show that\nattention-pooling-based compression of the spoken history offers a strong\ntrade-off, maintaining competitive accuracy with reduced context size. Detailed\nanalysis confirms that improvements stem from more effective context\nutilization.", "authors": ["Nizar El Ghazal", "Antoine Caubri\u00e8re", "Valentin Vielzeuf"], "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "published": "2025-10-10T14:27:01Z", "pdf": "https://arxiv.org/pdf/2510.09424v1", "abs": "https://arxiv.org/abs/2510.09424v1", "comment": null, "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u57fa\u4e8eSpeech-LLM\u7684\u7aef\u5230\u7aef\u53e3\u8bed\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u7b56\u7565\uff0c\u53d1\u73b0\u5b8c\u6574\u53e3\u8bed\u5bf9\u8bdd\u5386\u53f2\u8f93\u5165\u6027\u80fd\u6700\u4f73\uff0c\u800c\u6ce8\u610f\u529b\u6c60\u5316\u538b\u7f29\u65b9\u6cd5\u80fd\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c11\u4e0a\u4e0b\u6587\u5927\u5c0f\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u4e0a\u4e0b\u6587\u7ba1\u7406\u7b56\u7565\u5bf9\u53e3\u8bed\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u5982\u4f55\u66f4\u6709\u6548\u5730\u5229\u7528\u53e3\u8bed\u5bf9\u8bdd\u5386\u53f2\u4fe1\u606f\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e09\u79cd\u7b56\u7565\uff1a\u4f20\u7edf\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\uff08\u6587\u672c\u5386\u53f2+\u5f53\u524d\u53e3\u8bed\u8f6e\u6b21\uff09\u3001\u5b8c\u6574\u53e3\u8bed\u5386\u53f2\u3001\u538b\u7f29\u53e3\u8bed\u5386\u53f2\u65b9\u6cd5\uff0c\u5e76\u5728SpokenWOZ\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b8c\u6574\u53e3\u8bed\u5bf9\u8bdd\u5386\u53f2\u8f93\u5165\u5728\u76f8\u4f3c\u89c4\u6a21\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u663e\u8457\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\uff1b\u6ce8\u610f\u529b\u6c60\u5316\u538b\u7f29\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u4e0a\u4e0b\u6587\u5927\u5c0f\u3002", "conclusion": "\u6539\u8fdb\u4e3b\u8981\u6e90\u4e8e\u66f4\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u5229\u7528\uff0c\u5b8c\u6574\u53e3\u8bed\u5386\u53f2\u7b56\u7565\u6027\u80fd\u6700\u4f18\uff0c\u538b\u7f29\u65b9\u6cd5\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u6743\u8861\u9009\u62e9\u3002"}}
{"id": "2510.09404v1", "title": "Agentic Systems in Radiology: Design, Applications, Evaluation, and Challenges", "summary": "Building agents, systems that perceive and act upon their environment with a\ndegree of autonomy, has long been a focus of AI research. This pursuit has\nrecently become vastly more practical with the emergence of large language\nmodels (LLMs) capable of using natural language to integrate information,\nfollow instructions, and perform forms of \"reasoning\" and planning across a\nwide range of tasks. With its multimodal data streams and orchestrated\nworkflows spanning multiple systems, radiology is uniquely suited to benefit\nfrom agents that can adapt to context and automate repetitive yet complex\ntasks. In radiology, LLMs and their multimodal variants have already\ndemonstrated promising performance for individual tasks such as information\nextraction and report summarization. However, using LLMs in isolation\nunderutilizes their potential to support complex, multi-step workflows where\ndecisions depend on evolving context from multiple information sources.\nEquipping LLMs with external tools and feedback mechanisms enables them to\ndrive systems that exhibit a spectrum of autonomy, ranging from semi-automated\nworkflows to more adaptive agents capable of managing complex processes. This\nreview examines the design of such LLM-driven agentic systems, highlights key\napplications, discusses evaluation methods for planning and tool use, and\noutlines challenges such as error cascades, tool-use efficiency, and health IT\nintegration.", "authors": ["Christian Bluethgen", "Dave Van Veen", "Daniel Truhn", "Jakob Nikolas Kather", "Michael Moor", "Malgorzata Polacin", "Akshay Chaudhari", "Thomas Frauenfelder", "Curtis P. Langlotz", "Michael Krauthammer", "Farhad Nooralahzadeh"], "categories": ["cs.AI"], "published": "2025-10-10T13:56:27Z", "pdf": "https://arxiv.org/pdf/2510.09404v1", "abs": "https://arxiv.org/abs/2510.09404v1", "comment": null, "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u653e\u5c04\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5916\u90e8\u5de5\u5177\u548c\u53cd\u9988\u673a\u5236\u589e\u5f3aLLM\u80fd\u529b\uff0c\u5b9e\u73b0\u4ece\u534a\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u5230\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u7684\u4e0d\u540c\u81ea\u4e3b\u7a0b\u5ea6\u3002", "motivation": "\u653e\u5c04\u5b66\u5177\u6709\u591a\u6a21\u6001\u6570\u636e\u6d41\u548c\u534f\u8c03\u5de5\u4f5c\u6d41\u7684\u7279\u70b9\uff0c\u975e\u5e38\u9002\u5408\u5e94\u7528\u80fd\u591f\u9002\u5e94\u4e0a\u4e0b\u6587\u5e76\u81ea\u52a8\u5316\u590d\u6742\u91cd\u590d\u4efb\u52a1\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u867d\u7136LLM\u5728\u653e\u5c04\u5b66\u5355\u4e2a\u4efb\u52a1\u4e2d\u5df2\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5355\u72ec\u4f7f\u7528LLM\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u5728\u590d\u6742\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u4e3aLLM\u914d\u5907\u5916\u90e8\u5de5\u5177\u548c\u53cd\u9988\u673a\u5236\uff0c\u4f7f\u5176\u80fd\u591f\u9a71\u52a8\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u81ea\u4e3b\u6027\u7684\u7cfb\u7edf\uff0c\u4ece\u534a\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u5230\u80fd\u591f\u7ba1\u7406\u590d\u6742\u8fc7\u7a0b\u7684\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u3002", "result": "LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u653e\u5c04\u5b66\u4e2d\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\uff0c\u80fd\u591f\u5904\u7406\u4f9d\u8d56\u591a\u4e2a\u4fe1\u606f\u6e90\u52a8\u6001\u4e0a\u4e0b\u6587\u7684\u590d\u6742\u51b3\u7b56\u8fc7\u7a0b\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u4e3a\u653e\u5c04\u5b66\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u673a\u9047\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u9519\u8bef\u7ea7\u8054\u3001\u5de5\u5177\u4f7f\u7528\u6548\u7387\u548c\u533b\u7597IT\u96c6\u6210\u7b49\u6311\u6218\u3002"}}
{"id": "2510.09396v1", "title": "Bridging Research and Practice in Simulation-based Testing of Industrial Robot Navigation Systems", "summary": "Ensuring robust robotic navigation in dynamic environments is a key\nchallenge, as traditional testing methods often struggle to cover the full\nspectrum of operational requirements. This paper presents the industrial\nadoption of Surrealist, a simulation-based test generation framework originally\nfor UAVs, now applied to the ANYmal quadrupedal robot for industrial\ninspection. Our method uses a search-based algorithm to automatically generate\nchallenging obstacle avoidance scenarios, uncovering failures often missed by\nmanual testing. In a pilot phase, generated test suites revealed critical\nweaknesses in one experimental algorithm (40.3% success rate) and served as an\neffective benchmark to prove the superior robustness of another (71.2% success\nrate). The framework was then integrated into the ANYbotics workflow for a\nsix-month industrial evaluation, where it was used to test five proprietary\nalgorithms. A formal survey confirmed its value, showing it enhances the\ndevelopment process, uncovers critical failures, provides objective benchmarks,\nand strengthens the overall verification pipeline.", "authors": ["Sajad Khatiri", "Francisco Eli Vina Barrientos", "Maximilian Wulf", "Paolo Tonella", "Sebastiano Panichella"], "categories": ["cs.RO", "cs.SE"], "published": "2025-10-10T13:50:32Z", "pdf": "https://arxiv.org/pdf/2510.09396v1", "abs": "https://arxiv.org/abs/2510.09396v1", "comment": "12 pages, accepted for publication at IEEE/ACM International\n  Conference on Automated Software Engineering (ASE) 2025 - Industry Showcase\n  Track", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5c06\u65e0\u4eba\u673a\u6d4b\u8bd5\u6846\u67b6Surrealist\u5e94\u7528\u4e8eANYmal\u56db\u8db3\u673a\u5668\u4eba\u5de5\u4e1a\u68c0\u6d4b\u7684\u5de5\u4e1a\u5e94\u7528\uff0c\u901a\u8fc7\u57fa\u4e8e\u641c\u7d22\u7684\u7b97\u6cd5\u81ea\u52a8\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u907f\u969c\u573a\u666f\uff0c\u5728\u5de5\u4e1a\u8bc4\u4f30\u4e2d\u6210\u529f\u6d4b\u8bd5\u4e86\u4e94\u79cd\u4e13\u6709\u7b97\u6cd5\u5e76\u9a8c\u8bc1\u4e86\u5176\u4ef7\u503c\u3002", "motivation": "\u4f20\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u96be\u4ee5\u8986\u76d6\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5bfc\u822a\u7684\u5168\u90e8\u64cd\u4f5c\u9700\u6c42\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6d4b\u8bd5\u6846\u67b6\u6765\u786e\u4fdd\u673a\u5668\u4eba\u5bfc\u822a\u7684\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u641c\u7d22\u7684\u7b97\u6cd5\u81ea\u52a8\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u907f\u969c\u573a\u666f\uff0c\u5c06Surrealist\u6846\u67b6\u4ece\u65e0\u4eba\u673a\u5e94\u7528\u6269\u5c55\u5230ANYmal\u56db\u8db3\u673a\u5668\u4eba\u5de5\u4e1a\u68c0\u6d4b\u9886\u57df\u3002", "result": "\u5728\u8bd5\u70b9\u9636\u6bb5\uff0c\u751f\u6210\u7684\u6d4b\u8bd5\u5957\u4ef6\u63ed\u793a\u4e86\u4e00\u4e2a\u5b9e\u9a8c\u7b97\u6cd5\u7684\u5173\u952e\u5f31\u70b9\uff08\u6210\u529f\u738740.3%\uff09\uff0c\u5e76\u8bc1\u660e\u4e86\u53e6\u4e00\u4e2a\u7b97\u6cd5\u7684\u4f18\u8d8a\u9c81\u68d2\u6027\uff08\u6210\u529f\u738771.2%\uff09\u3002\u5728\u516d\u4e2a\u6708\u7684\u5de5\u4e1a\u8bc4\u4f30\u4e2d\uff0c\u6210\u529f\u6d4b\u8bd5\u4e86\u4e94\u79cd\u4e13\u6709\u7b97\u6cd5\uff0c\u6b63\u5f0f\u8c03\u67e5\u786e\u8ba4\u8be5\u6846\u67b6\u589e\u5f3a\u4e86\u5f00\u53d1\u8fc7\u7a0b\u3001\u53d1\u73b0\u5173\u952e\u6545\u969c\u3001\u63d0\u4f9b\u5ba2\u89c2\u57fa\u51c6\u5e76\u52a0\u5f3a\u4e86\u6574\u4f53\u9a8c\u8bc1\u6d41\u7a0b\u3002", "conclusion": "Surrealist\u6846\u67b6\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u6210\u529f\u5e94\u7528\u8bc1\u660e\u5176\u80fd\u591f\u6709\u6548\u53d1\u73b0\u4f20\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u9057\u6f0f\u7684\u6545\u969c\uff0c\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2510.09380v1", "title": "Utilizing dynamic sparsity on pretrained DETR", "summary": "Efficient inference with transformer-based models remains a challenge,\nespecially in vision tasks like object detection. We analyze the inherent\nsparsity in the MLP layers of DETR and introduce two methods to exploit it\nwithout retraining. First, we propose Static Indicator-Based Sparsification\n(SIBS), a heuristic method that predicts neuron inactivity based on fixed\nactivation patterns. While simple, SIBS offers limited gains due to the\ninput-dependent nature of sparsity. To address this, we introduce Micro-Gated\nSparsification (MGS), a lightweight gating mechanism trained on top of a\npretrained DETR. MGS predicts dynamic sparsity using a small linear layer and\nachieves up to 85 to 95% activation sparsity. Experiments on the COCO dataset\nshow that MGS maintains or even improves performance while significantly\nreducing computation. Our method offers a practical, input-adaptive approach to\nsparsification, enabling efficient deployment of pretrained vision transformers\nwithout full model retraining.", "authors": ["Reza Sedghi", "Anand Subramoney", "David Kappel"], "categories": ["cs.CV"], "published": "2025-10-10T13:36:00Z", "pdf": "https://arxiv.org/pdf/2510.09380v1", "abs": "https://arxiv.org/abs/2510.09380v1", "comment": "6 pages 4 figures and 4 tables , accepted for 2025 IEEE INTERNATIONAL\n  WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING, AUG. 31 to SEP. 3, 2025,\n  ISTANBUL, TURKEY", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684MLP\u5c42\u7a00\u758f\u5316\u65b9\u6cd5\uff1aSIBS\uff08\u9759\u6001\u6307\u793a\u5668\u7a00\u758f\u5316\uff09\u548cMGS\uff08\u5fae\u95e8\u63a7\u7a00\u758f\u5316\uff09\uff0c\u7528\u4e8e\u63d0\u5347DETR\u6a21\u578b\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\uff09\u4e2d\u7684\u63a8\u7406\u6548\u7387\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u7279\u522b\u662f\u5728DETR\u7684MLP\u5c42\u4e2d\u5b58\u5728\u56fa\u6709\u7684\u7a00\u758f\u6027\uff0c\u9700\u8981\u5f00\u53d1\u65b9\u6cd5\u6765\u5229\u7528\u8fd9\u79cd\u7a00\u758f\u6027\u800c\u4e0d\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09SIBS\uff1a\u57fa\u4e8e\u56fa\u5b9a\u6fc0\u6d3b\u6a21\u5f0f\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u9884\u6d4b\u795e\u7ecf\u5143\u4e0d\u6d3b\u8dc3\u6027\uff1b2\uff09MGS\uff1a\u5728\u9884\u8bad\u7ec3DETR\u4e0a\u8bad\u7ec3\u7684\u8f7b\u91cf\u7ea7\u95e8\u63a7\u673a\u5236\uff0c\u4f7f\u7528\u5c0f\u578b\u7ebf\u6027\u5c42\u9884\u6d4b\u52a8\u6001\u7a00\u758f\u6027\u3002", "result": "\u5728COCO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMGS\u5b9e\u73b0\u4e8685-95%\u7684\u6fc0\u6d3b\u7a00\u758f\u5ea6\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\u3002", "conclusion": "MGS\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u8f93\u5165\u81ea\u9002\u5e94\u7684\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u65e0\u9700\u5b8c\u6574\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9884\u8bad\u7ec3\u89c6\u89c9Transformer\u7684\u9ad8\u6548\u90e8\u7f72\u3002"}}
{"id": "2510.09361v1", "title": "BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception", "summary": "Recently, Multimodal Large Language Models (MLLMs) have made rapid progress,\nparticularly in enhancing their reasoning capabilities. However, existing\nreasoning benchmarks still primarily assess language-based reasoning, often\ntreating visual input as replaceable context. To address this gap, we introduce\nBLINK-Twice, a vision-centric reasoning benchmark grounded in challenging\nperceptual tasks. Instead of relying on external knowledge, our tasks require\nmodels to reason from visual content alone, shifting the focus from\nlanguage-based to image-grounded reasoning. Compared to prior perception\nbenchmarks, it moves beyond shallow perception (\"see\") and requires\nfine-grained observation and analytical reasoning (\"observe\"). BLINK-Twice\nintegrates three core components: seven types of visual challenges for testing\nvisual reasoning, natural adversarial image pairs that enforce reliance on\nvisual content, and annotated reasoning chains for fine-grained evaluation of\nthe reasoning process rather than final answers alone. We evaluate 20 leading\nMLLMs, including 12 foundation models and 8 reasoning-enhanced models.\nBLINK-Twice poses a significant challenge to current models. While existing\nreasoning strategies in the language space-such as chain-of-thought or\nself-criticism can improve performance, they often result in unstable and\nredundant reasoning. We observe that repeated image observation improves\nperformance across models, and active visual interaction, as demonstrated by\nmodels like o3, highlights the need for a new paradigm for vision reasoning.\nThe dataset is publicly available at https://github.com/PicoTrex/BLINK-Twice", "authors": ["Junyan Ye", "Dongzhi Jiang", "Jun He", "Baichuan Zhou", "Zilong Huang", "Zhiyuan Yan", "Hongsheng Li", "Conghui He", "Weijia Li"], "categories": ["cs.CV"], "published": "2025-10-10T13:14:13Z", "pdf": "https://arxiv.org/pdf/2510.09361v1", "abs": "https://arxiv.org/abs/2510.09361v1", "comment": "Accepted to 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Track on Datasets and Benchmarks", "AI": {"tldr": "BLINK-Twice\u662f\u4e00\u4e2a\u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u63a8\u7406\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u4ece\u7eaf\u89c6\u89c9\u5185\u5bb9\u8fdb\u884c\u63a8\u7406\uff0c\u8d85\u8d8a\u4e86\u6d45\u5c42\u611f\u77e5\uff0c\u9700\u8981\u7ec6\u7c92\u5ea6\u89c2\u5bdf\u548c\u5206\u6790\u63a8\u7406\u3002\u5b83\u5305\u542b\u4e03\u79cd\u89c6\u89c9\u6311\u6218\u7c7b\u578b\u3001\u81ea\u7136\u5bf9\u6297\u56fe\u50cf\u5bf9\u548c\u5e26\u6ce8\u91ca\u7684\u63a8\u7406\u94fe\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u57fa\u4e8e\u8bed\u8a00\u7684\u63a8\u7406\uff0c\u5c06\u89c6\u89c9\u8f93\u5165\u89c6\u4e3a\u53ef\u66ff\u6362\u7684\u4e0a\u4e0b\u6587\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u57fa\u4e8e\u6311\u6218\u6027\u611f\u77e5\u4efb\u52a1\u7684\u89c6\u89c9\u4e2d\u5fc3\u63a8\u7406\u57fa\u51c6\u3002", "method": "BLINK-Twice\u96c6\u6210\u4e86\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e03\u79cd\u89c6\u89c9\u6311\u6218\u7c7b\u578b\u7528\u4e8e\u6d4b\u8bd5\u89c6\u89c9\u63a8\u7406\uff0c\u81ea\u7136\u5bf9\u6297\u56fe\u50cf\u5bf9\u5f3a\u5236\u4f9d\u8d56\u89c6\u89c9\u5185\u5bb9\uff0c\u4ee5\u53ca\u5e26\u6ce8\u91ca\u7684\u63a8\u7406\u94fe\u7528\u4e8e\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u3002\u8bc4\u4f30\u4e8620\u4e2a\u9886\u5148\u7684MLLM\u6a21\u578b\u3002", "result": "BLINK-Twice\u5bf9\u5f53\u524d\u6a21\u578b\u6784\u6210\u4e86\u663e\u8457\u6311\u6218\u3002\u73b0\u6709\u7684\u8bed\u8a00\u7a7a\u95f4\u63a8\u7406\u7b56\u7565\uff08\u5982\u601d\u7ef4\u94fe\u6216\u81ea\u6211\u6279\u8bc4\uff09\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u5f80\u5f80\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u548c\u5197\u4f59\u7684\u63a8\u7406\u3002\u91cd\u590d\u56fe\u50cf\u89c2\u5bdf\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u4e3b\u52a8\u89c6\u89c9\u4ea4\u4e92\u7a81\u51fa\u4e86\u89c6\u89c9\u63a8\u7406\u65b0\u8303\u5f0f\u7684\u9700\u6c42\u3002", "conclusion": "\u8be5\u57fa\u51c6\u5f3a\u8c03\u4e86\u4ece\u8bed\u8a00\u57fa\u7840\u63a8\u7406\u5411\u56fe\u50cf\u57fa\u7840\u63a8\u7406\u7684\u8f6c\u53d8\u9700\u6c42\uff0c\u5c55\u793a\u4e86\u5f53\u524dMLLM\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u9700\u8981\u65b0\u7684\u89c6\u89c9\u63a8\u7406\u8303\u5f0f\u3002"}}
{"id": "2510.09323v1", "title": "Parametrized Topological Complexity for a Multi-Robot System with Variable Tasks", "summary": "We study a generalized motion planning problem involving multiple autonomous\nrobots navigating in a $d$-dimensional Euclidean space in the presence of a set\nof obstacles whose positions are unknown a priori. Each robot is required to\nvisit sequentially a prescribed set of target states, with the number of\ntargets varying between robots. This heterogeneous setting generalizes the\nframework considered in the prior works on sequential parametrized topological\ncomplexity by Farber and the second author of this article. To determine the\ntopological complexity of our problem, we formulate it mathematically by\nconstructing an appropriate fibration. Our main contribution is the\ndetermination of this invariant in the generalized setting, which captures the\nminimal algorithmic instability required for designing collision-free motion\nplanning algorithms under parameter-dependent constraints. We provide a\ndetailed analysis for both odd and even-dimensional ambient spaces, including\nthe essential cohomological computations and explicit constructions of\ncorresponding motion planning algorithms.", "authors": ["Gopal Chandra Dutta", "Amit Kumar Paul", "Subhankar Sau"], "categories": ["math.AT", "cs.RO", "55M30, 55R80"], "published": "2025-10-10T12:26:26Z", "pdf": "https://arxiv.org/pdf/2510.09323v1", "abs": "https://arxiv.org/abs/2510.09323v1", "comment": "25 pages. All comments are welcome", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u591a\u673a\u5668\u4eba\u5728\u672a\u77e5\u969c\u788d\u73af\u5883\u4e2d\u7684\u5e7f\u4e49\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u62d3\u6251\u590d\u6742\u6027\u7406\u8bba\u786e\u5b9a\u78b0\u649e\u907f\u514d\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u7684\u6700\u5c0f\u4e0d\u7a33\u5b9a\u6027\u8981\u6c42\u3002", "motivation": "\u6269\u5c55Farber\u7b49\u4eba\u7684\u5e8f\u5217\u53c2\u6570\u5316\u62d3\u6251\u590d\u6742\u6027\u6846\u67b6\uff0c\u89e3\u51b3\u5f02\u6784\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u672a\u77e5\u969c\u788d\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u5176\u4e2d\u6bcf\u4e2a\u673a\u5668\u4eba\u9700\u8981\u6309\u987a\u5e8f\u8bbf\u95ee\u4e0d\u540c\u6570\u91cf\u7684\u76ee\u6807\u72b6\u6001\u3002", "method": "\u6784\u5efa\u9002\u5f53\u7684\u7ea4\u7ef4\u5316\u6570\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u62d3\u6251\u590d\u6742\u6027\u7406\u8bba\u5206\u6790\u95ee\u9898\uff0c\u5305\u62ec\u5947\u5076\u7ef4\u6570\u73af\u5883\u7a7a\u95f4\u7684\u8be6\u7ec6\u5206\u6790\u3001\u4e0a\u540c\u8c03\u8ba1\u7b97\u548c\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u7684\u663e\u5f0f\u6784\u9020\u3002", "result": "\u786e\u5b9a\u4e86\u5e7f\u4e49\u8bbe\u7f6e\u4e0b\u7684\u62d3\u6251\u590d\u6742\u6027\u4e0d\u53d8\u91cf\uff0c\u8be5\u4e0d\u53d8\u91cf\u6355\u83b7\u4e86\u5728\u53c2\u6570\u4f9d\u8d56\u7ea6\u675f\u4e0b\u8bbe\u8ba1\u65e0\u78b0\u649e\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u6240\u9700\u7684\u6700\u5c0f\u7b97\u6cd5\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f02\u6784\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u62d3\u6251\u590d\u6742\u6027\u91cf\u5316\u4e86\u7b97\u6cd5\u8bbe\u8ba1\u7684\u6839\u672c\u9650\u5236\u3002"}}
{"id": "2510.09302v1", "title": "CapGeo: A Caption-Assisted Approach to Geometric Reasoning", "summary": "Geometric reasoning remains a core challenge for Multimodal Large Language\nModels (MLLMs). Even the most advanced closed-source systems, such as GPT-O3\nand Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despite\nexhibiting strong textual reasoning abilities on tasks like the International\nMathematical Olympiad (IMO). This gap suggests that the bottleneck lies in\nunderstanding geometric diagrams rather than reasoning itself. Since geometric\nfigures can often be faithfully described in concise textual form, converting\nvisual content into captions offers a promising direction. Motivated by this\ninsight, we introduce CapGeo, a caption-assisted reasoning framework that\nbridges visual and textual modalities. Experiments show substantial\nimprovements when models are equipped with captions: Qwen2.5-VL-72B improves\nfrom 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to\n73.0%. To systematically evaluate and identify high-quality geometric\ncaptioning models, we further propose CapGeo-Bench, a dataset of 4,641 curated\nfigure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-based\nevaluation metric that correlates strongly with downstream CapGeo performance,\nenabling reliable assessment of geometric captioning ability. Together, our\nframework and benchmark highlight a new pathway toward advancing geometric\nreasoning in MLLMs.", "authors": ["Yuying Li", "Siyi Qian", "Hao Liang", "Leqi Zheng", "Ruichuan An", "Yongzhen Guo", "Wentao Zhang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-10-10T11:47:54Z", "pdf": "https://arxiv.org/pdf/2510.09302v1", "abs": "https://arxiv.org/abs/2510.09302v1", "comment": "preprint, under review", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCapGeo\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u51e0\u4f55\u56fe\u5f62\u8f6c\u6362\u4e3a\u6587\u672c\u63cf\u8ff0\u6765\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51e0\u4f55\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u4e86CapGeo-Bench\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u51e0\u4f55\u63cf\u8ff0\u6a21\u578b\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u51e0\u4f55\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u74f6\u9888\u5728\u4e8e\u7406\u89e3\u51e0\u4f55\u56fe\u5f62\u800c\u975e\u63a8\u7406\u80fd\u529b\u672c\u8eab\u3002\u7531\u4e8e\u51e0\u4f55\u56fe\u5f62\u53ef\u4ee5\u7528\u7b80\u6d01\u7684\u6587\u672c\u5f62\u5f0f\u51c6\u786e\u63cf\u8ff0\uff0c\u5c06\u89c6\u89c9\u5185\u5bb9\u8f6c\u6362\u4e3a\u6587\u672c\u63cf\u8ff0\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "method": "\u5f15\u5165CapGeo\u6846\u67b6\uff0c\u901a\u8fc7\u8f85\u52a9\u63cf\u8ff0\u6765\u6865\u63a5\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u3002\u540c\u65f6\u6784\u5efa\u4e86CapGeo-Bench\u6570\u636e\u96c6\uff0c\u5305\u542b4,641\u4e2a\u7cbe\u5fc3\u7b5b\u9009\u7684\u56fe\u5f62-\u63cf\u8ff0\u5bf9\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u8bc4\u4f30\u6307\u6807\u6765\u53ef\u9760\u8bc4\u4f30\u51e0\u4f55\u63cf\u8ff0\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u914d\u5907\u63cf\u8ff0\u540e\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1aQwen2.5-VL-72B\u4ece8.6%\u63d0\u5347\u81f359.0%\uff0cClaude-Opus-4\u4ece44.8%\u63d0\u5347\u81f373.0%\u3002\u5173\u952e\u70b9\u8bc4\u4f30\u6307\u6807\u4e0e\u4e0b\u6e38CapGeo\u6027\u80fd\u5f3a\u76f8\u5173\u3002", "conclusion": "CapGeo\u6846\u67b6\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u8868\u660e\u901a\u8fc7\u51e0\u4f55\u63cf\u8ff0\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002"}}
{"id": "2510.09285v1", "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning", "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the\nreasoning capabilities of Large Vision-Language Models (LVLMs), most existing\nmethods in multimodal reasoning neglect the critical role of visual perception\nwithin the RLVR optimization process. In this paper, we undertake a pioneering\nexploration of multimodal RLVR through the novel perspective of token\nperception, which measures the visual dependency of each generated token. With\na granular analysis of Chain-of-Thought (CoT) processes, we uncover two key\ninsights: first, token perception in a rollout trajectory is sparsely\ndistributed, where only a small fraction of tokens have high visual dependency\nfor visually-grounded reasoning; second, different trajectories exhibit\nsignificant divergence in their overall visual dependency. Based on these\nobservations, we propose Visually-Perceptive Policy Optimization (VPPO), a\nnovel policy gradient algorithm that explicitly leverages token perception to\nrefine the learning signal. Specifically, VPPO achieves this through a dual\nmechanism: it reweights a trajectory's advantage by its overall visual\ndependency, and focuses policy updates exclusively on perceptually pivotal\ntokens. On a comprehensive suite of eight perception and reasoning benchmarks,\nVPPO demonstrates substantial gains over leading open-source RL-tuned models,\nwith its effectiveness consistently validated across 7B and 32B model scales.\nOur findings not only establish a new token-level perceptual perspective for\nanalyzing multimodal RLVR but also present a novel and effective optimization\nstrategy to significantly enhance the multimodal reasoning capabilities of\nLVLMs.", "authors": ["Siyuan Huang", "Xiaoye Qu", "Yafu Li", "Yun Luo", "Zefeng He", "Daizong Liu", "Yu Cheng"], "categories": ["cs.CV"], "published": "2025-10-10T11:25:33Z", "pdf": "https://arxiv.org/pdf/2510.09285v1", "abs": "https://arxiv.org/abs/2510.09285v1", "comment": "31 pages, 10 figures, project page:\n  https://github.com/huaixuheqing/VPPO-RL", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5VPPO\uff0c\u901a\u8fc7token\u611f\u77e5\u89c6\u89d2\u5206\u6790\u89c6\u89c9\u4f9d\u8d56\u6027\uff0c\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u5ffd\u89c6\u4e86\u89c6\u89c9\u611f\u77e5\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u9700\u8981\u4ecetoken\u611f\u77e5\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u591a\u6a21\u6001RLVR\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u611f\u77e5\u7b56\u7565\u4f18\u5316(VPPO)\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u91cd\u673a\u5236\uff1a\u57fa\u4e8e\u6574\u4f53\u89c6\u89c9\u4f9d\u8d56\u6027\u91cd\u65b0\u52a0\u6743\u8f68\u8ff9\u4f18\u52bf\uff0c\u5e76\u4ec5\u5bf9\u611f\u77e5\u5173\u952etoken\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u57288\u4e2a\u611f\u77e5\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVPPO\u76f8\u6bd4\u9886\u5148\u7684\u5f00\u6e90RL\u8c03\u4f18\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u57287B\u548c32B\u6a21\u578b\u89c4\u6a21\u4e0a\u5747\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u4e3a\u5206\u6790\u591a\u6a21\u6001RLVR\u5efa\u7acb\u4e86\u65b0\u7684token\u7ea7\u611f\u77e5\u89c6\u89d2\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u6709\u6548\u7684\u4f18\u5316\u7b56\u7565\u6765\u663e\u8457\u589e\u5f3aLVLMs\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.09276v1", "title": "The bixplot: A variation on the boxplot suited for bimodal data", "summary": "Boxplots and related visualization methods are widely used exploratory tools\nfor taking a first look at collections of univariate variables. In this note an\nextension is provided that is specifically designed to detect and display\nbimodality and multimodality when the data warrant it. For this purpose a\nunivariate clustering method is constructed that ensures contiguous clusters,\nmeaning that no cluster has members inside another cluster, and such that each\ncluster contains at least a given number of unique members. The resulting\nbixplot display facilitates the identification and interpretation of\npotentially meaningful subgroups underlying the data. The bixplot also displays\nthe individual data values, which can draw attention to isolated points.\nImplementations of the bixplot are available in both Python and R, and their\nmany options are illustrated on several real datasets. For instance, an\nexternal variable can be visualized by color gradations inside the display.", "authors": ["Camille M. Montalcini", "Peter J. Rousseeuw"], "categories": ["stat.ME", "stat.AP"], "published": "2025-10-10T11:19:47Z", "pdf": "https://arxiv.org/pdf/2510.09276v1", "abs": "https://arxiv.org/abs/2510.09276v1", "comment": null, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3abixplot\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\u6269\u5c55\uff0c\u4e13\u95e8\u7528\u4e8e\u68c0\u6d4b\u548c\u663e\u793a\u53cc\u5cf0\u548c\u591a\u5cf0\u5206\u5e03\uff0c\u901a\u8fc7\u6784\u5efa\u786e\u4fdd\u8fde\u7eed\u7c07\u7684\u5355\u53d8\u91cf\u805a\u7c7b\u65b9\u6cd5\u6765\u8bc6\u522b\u6570\u636e\u4e2d\u6f5c\u5728\u7684\u6709\u610f\u4e49\u5b50\u7ec4\u3002", "motivation": "\u7bb1\u7ebf\u56fe\u53ca\u76f8\u5173\u53ef\u89c6\u5316\u65b9\u6cd5\u88ab\u5e7f\u6cdb\u7528\u4e8e\u5355\u53d8\u91cf\u6570\u636e\u7684\u521d\u6b65\u63a2\u7d22\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u548c\u663e\u793a\u53cc\u5cf0\u548c\u591a\u5cf0\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e13\u95e8\u5de5\u5177\u6765\u66f4\u597d\u5730\u8bc6\u522b\u6570\u636e\u4e2d\u7684\u5b50\u7ec4\u7ed3\u6784\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u79cd\u5355\u53d8\u91cf\u805a\u7c7b\u65b9\u6cd5\uff0c\u786e\u4fdd\u7c07\u7684\u8fde\u7eed\u6027\uff08\u5373\u6ca1\u6709\u7c07\u7684\u6210\u5458\u4f4d\u4e8e\u53e6\u4e00\u4e2a\u7c07\u5185\u90e8\uff09\uff0c\u4e14\u6bcf\u4e2a\u7c07\u5305\u542b\u81f3\u5c11\u7ed9\u5b9a\u6570\u91cf\u7684\u552f\u4e00\u6210\u5458\uff0c\u4ece\u800c\u521b\u5efabixplot\u53ef\u89c6\u5316\u663e\u793a\u3002", "result": "bixplot\u663e\u793a\u6709\u52a9\u4e8e\u8bc6\u522b\u548c\u89e3\u91ca\u6570\u636e\u4e2d\u6f5c\u5728\u7684\u6709\u610f\u4e49\u5b50\u7ec4\uff0c\u540c\u65f6\u663e\u793a\u5355\u4e2a\u6570\u636e\u503c\u4ee5\u5f15\u8d77\u5bf9\u5b64\u7acb\u70b9\u7684\u6ce8\u610f\u3002\u8be5\u65b9\u6cd5\u5df2\u5728Python\u548cR\u4e2d\u5b9e\u73b0\uff0c\u5e76\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u591a\u79cd\u9009\u9879\u3002", "conclusion": "bixplot\u4f5c\u4e3a\u7bb1\u7ebf\u56fe\u7684\u6269\u5c55\uff0c\u6709\u6548\u4fc3\u8fdb\u4e86\u53cc\u5cf0\u548c\u591a\u5cf0\u5206\u5e03\u7684\u68c0\u6d4b\u4e0e\u53ef\u89c6\u5316\uff0c\u4e3a\u63a2\u7d22\u6570\u636e\u4e2d\u7684\u5b50\u7ec4\u7ed3\u6784\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2510.09269v1", "title": "Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects", "summary": "Recent advances in vision-language-action (VLA) models have greatly improved\nembodied AI, enabling robots to follow natural language instructions and\nperform diverse tasks. However, their reliance on uncurated training datasets\nraises serious security concerns. Existing backdoor attacks on VLAs mostly\nassume white-box access and result in task failures instead of enforcing\nspecific actions. In this work, we reveal a more practical threat: attackers\ncan manipulate VLAs by simply injecting physical objects as triggers into the\ntraining dataset. We propose goal-oriented backdoor attacks (GoBA), where the\nVLA behaves normally in the absence of physical triggers but executes\npredefined and goal-oriented actions in the presence of physical triggers.\nSpecifically, based on a popular VLA benchmark LIBERO, we introduce BadLIBERO\nthat incorporates diverse physical triggers and goal-oriented backdoor actions.\nIn addition, we propose a three-level evaluation that categorizes the victim\nVLA's actions under GoBA into three states: nothing to do, try to do, and\nsuccess to do. Experiments show that GoBA enables the victim VLA to\nsuccessfully achieve the backdoor goal in 97 percentage of inputs when the\nphysical trigger is present, while causing zero performance degradation on\nclean inputs. Finally, by investigating factors related to GoBA, we find that\nthe action trajectory and trigger color significantly influence attack\nperformance, while trigger size has surprisingly little effect. The code and\nBadLIBERO dataset are accessible via the project page at\nhttps://goba-attack.github.io/.", "authors": ["Zirun Zhou", "Zhengyang Xiao", "Haochuan Xu", "Jing Sun", "Di Wang", "Jingfeng Zhang"], "categories": ["cs.CR", "cs.CV", "cs.LG"], "published": "2025-10-10T11:09:36Z", "pdf": "https://arxiv.org/pdf/2510.09269v1", "abs": "https://arxiv.org/abs/2510.09269v1", "comment": null, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u76ee\u6807\u5bfc\u5411\u540e\u95e8\u653b\u51fb\u65b9\u6cd5GoBA\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u6ce8\u5165\u7269\u7406\u5bf9\u8c61\u4f5c\u4e3a\u89e6\u53d1\u5668\uff0c\u4f7f\u6a21\u578b\u5728\u9047\u5230\u7269\u7406\u89e6\u53d1\u5668\u65f6\u6267\u884c\u9884\u5b9a\u4e49\u7684\u76ee\u6807\u52a8\u4f5c\uff0c\u800c\u6b63\u5e38\u8f93\u5165\u4e0b\u8868\u73b0\u6b63\u5e38\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u4f9d\u8d56\u672a\u7b5b\u9009\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u5f53\u524d\u540e\u95e8\u653b\u51fb\u5927\u591a\u5047\u8bbe\u767d\u76d2\u8bbf\u95ee\u4e14\u4ec5\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u800c\u975e\u6267\u884c\u7279\u5b9a\u52a8\u4f5c\u3002\u672c\u6587\u63ed\u793a\u66f4\u5b9e\u9645\u7684\u5a01\u80c1\uff1a\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u6ce8\u5165\u7269\u7406\u5bf9\u8c61\u4f5c\u4e3a\u89e6\u53d1\u5668\u6765\u64cd\u63a7VLA\u6a21\u578b\u3002", "method": "\u57fa\u4e8eLIBERO\u57fa\u51c6\u63d0\u51faBadLIBERO\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u7269\u7406\u89e6\u53d1\u5668\u548c\u76ee\u6807\u5bfc\u5411\u7684\u540e\u95e8\u52a8\u4f5c\u3002\u91c7\u7528\u4e09\u7ea7\u8bc4\u4f30\u65b9\u6cd5\u5c06\u53d7\u5bb3VLA\u5728GoBA\u4e0b\u7684\u52a8\u4f5c\u5206\u4e3a\u4e09\u79cd\u72b6\u6001\uff1a\u65e0\u52a8\u4f5c\u3001\u5c1d\u8bd5\u6267\u884c\u3001\u6210\u529f\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u7269\u7406\u89e6\u53d1\u5668\u5b58\u5728\u65f6\uff0cGoBA\u4f7f\u53d7\u5bb3VLA\u572897%\u7684\u8f93\u5165\u4e2d\u6210\u529f\u5b9e\u73b0\u540e\u95e8\u76ee\u6807\uff0c\u540c\u65f6\u5728\u5e72\u51c0\u8f93\u5165\u4e0a\u9020\u6210\u96f6\u6027\u80fd\u4e0b\u964d\u3002\u52a8\u4f5c\u8f68\u8ff9\u548c\u89e6\u53d1\u5668\u989c\u8272\u663e\u8457\u5f71\u54cd\u653b\u51fb\u6027\u80fd\uff0c\u800c\u89e6\u53d1\u5668\u5927\u5c0f\u5f71\u54cd\u5f88\u5c0f\u3002", "conclusion": "GoBA\u5c55\u793a\u4e86\u7269\u7406\u540e\u95e8\u653b\u51fb\u5bf9VLA\u6a21\u578b\u7684\u5b9e\u9645\u5a01\u80c1\uff0c\u5f3a\u8c03\u4e86\u8bad\u7ec3\u6570\u636e\u5b89\u5168\u7684\u91cd\u8981\u6027\u3002\u8be5\u65b9\u6cd5\u6210\u529f\u7387\u9ad8\u4e14\u4e0d\u5f71\u54cd\u6b63\u5e38\u6027\u80fd\uff0c\u4e3aVLA\u6a21\u578b\u7684\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.09267v1", "title": "Placeit! A Framework for Learning Robot Object Placement Skills", "summary": "Robotics research has made significant strides in learning, yet mastering\nbasic skills like object placement remains a fundamental challenge. A key\nbottleneck is the acquisition of large-scale, high-quality data, which is often\na manual and laborious process. Inspired by Graspit!, a foundational work that\nused simulation to automatically generate dexterous grasp poses, we introduce\nPlaceit!, an evolutionary-computation framework for generating valid placement\npositions for rigid objects. Placeit! is highly versatile, supporting tasks\nfrom placing objects on tables to stacking and inserting them. Our experiments\nshow that by leveraging quality-diversity optimization, Placeit! significantly\noutperforms state-of-the-art methods across all scenarios for generating\ndiverse valid poses. A pick&place pipeline built on our framework achieved a\n90% success rate over 120 real-world deployments. This work positions Placeit!\nas a powerful tool for open-environment pick-and-place tasks and as a valuable\nengine for generating the data needed to train simulation-based foundation\nmodels in robotics.", "authors": ["Amina Ferrad", "Johann Huber", "Fran\u00e7ois H\u00e9l\u00e9non", "Julien Gleyze", "Mahdi Khoramshahi", "St\u00e9phane Doncieux"], "categories": ["cs.RO", "cs.LG"], "published": "2025-10-10T11:08:06Z", "pdf": "https://arxiv.org/pdf/2510.09267v1", "abs": "https://arxiv.org/abs/2510.09267v1", "comment": "8 pages, 8 figures. Draft version", "AI": {"tldr": "Placeit!\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fdb\u5316\u8ba1\u7b97\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u521a\u6027\u7269\u4f53\u81ea\u52a8\u751f\u6210\u6709\u6548\u7684\u653e\u7f6e\u4f4d\u7f6e\uff0c\u652f\u6301\u4ece\u684c\u9762\u653e\u7f6e\u5230\u5806\u53e0\u548c\u63d2\u5165\u7b49\u591a\u79cd\u4efb\u52a1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u8fbe\u523090%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u9762\u4e34\u83b7\u53d6\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u74f6\u9888\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u52b3\u52a8\u3002\u53d7Graspit!\u4f7f\u7528\u4eff\u771f\u81ea\u52a8\u751f\u6210\u6293\u53d6\u59ff\u6001\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u7c7b\u4f3c\u65b9\u6cd5\u89e3\u51b3\u7269\u4f53\u653e\u7f6e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8fdb\u5316\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u8d28\u91cf\u591a\u6837\u6027\u4f18\u5316\u751f\u6210\u591a\u6837\u5316\u7684\u6709\u6548\u653e\u7f6e\u59ff\u6001\uff0c\u652f\u6301\u591a\u79cd\u653e\u7f6e\u573a\u666f\u5305\u62ec\u684c\u9762\u653e\u7f6e\u3001\u5806\u53e0\u548c\u63d2\u5165\u3002", "result": "\u5728\u6240\u6709\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8be5\u6846\u67b6\u6784\u5efa\u7684\u62fe\u653e\u7ba1\u9053\u5728120\u6b21\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u8fbe\u523090%\u7684\u6210\u529f\u7387\u3002", "conclusion": "Placeit!\u662f\u5f00\u653e\u73af\u5883\u62fe\u653e\u4efb\u52a1\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u4e5f\u662f\u4e3a\u8bad\u7ec3\u57fa\u4e8e\u4eff\u771f\u7684\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u751f\u6210\u6240\u9700\u6570\u636e\u7684\u5b9d\u8d35\u5f15\u64ce\u3002"}}
{"id": "2510.09266v1", "title": "CFVBench: A Comprehensive Video Benchmark for Fine-grained Multimodal Retrieval-Augmented Generation", "summary": "Multimodal Retrieval-Augmented Generation (MRAG) enables Multimodal Large\nLanguage Models (MLLMs) to generate responses with external multimodal\nevidence, and numerous video-based MRAG benchmarks have been proposed to\nevaluate model capabilities across retrieval and generation stages. However,\nexisting benchmarks remain limited in modality coverage and format diversity,\noften focusing on single- or limited-modality tasks, or coarse-grained scene\nunderstanding. To address these gaps, we introduce CFVBench, a large-scale,\nmanually verified benchmark constructed from 599 publicly available videos,\nyielding 5,360 open-ended QA pairs. CFVBench spans high-density formats and\ndomains such as chart-heavy reports, news broadcasts, and software tutorials,\nrequiring models to retrieve and reason over long temporal video spans while\nmaintaining fine-grained multimodal information. Using CFVBench, we\nsystematically evaluate 7 retrieval methods and 14 widely-used MLLMs, revealing\na critical bottleneck: current models (even GPT5 or Gemini) struggle to capture\ntransient yet essential fine-grained multimodal details. To mitigate this, we\npropose Adaptive Visual Refinement (AVR), a simple yet effective framework that\nadaptively increases frame sampling density and selectively invokes external\ntools when necessary. Experiments show that AVR consistently enhances\nfine-grained multimodal comprehension and improves performance across all\nevaluated MLLMs", "authors": ["Kaiwen Wei", "Xiao Liu", "Jie Zhang", "Zijian Wang", "Ruida Liu", "Yuming Yang", "Xin Xiao", "Xiao Sun", "Haoyang Zeng", "Changzai Pan", "Yidan Zhang", "Jiang Zhong", "Peijin Wang", "Yingchao Feng"], "categories": ["cs.CL"], "published": "2025-10-10T11:05:37Z", "pdf": "https://arxiv.org/pdf/2510.09266v1", "abs": "https://arxiv.org/abs/2510.09266v1", "comment": null, "AI": {"tldr": "\u63d0\u51fa\u4e86CFVBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6a21\u578b\u5728\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u74f6\u9888\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u89c6\u89c9\u4f18\u5316\u6846\u67b6AVR\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891MRAG\u57fa\u51c6\u6d4b\u8bd5\u5728\u6a21\u6001\u8986\u76d6\u548c\u683c\u5f0f\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6216\u6709\u9650\u6a21\u6001\u4efb\u52a1\uff0c\u6216\u7c97\u7c92\u5ea6\u573a\u666f\u7406\u89e3\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u4e86CFVBench\u5927\u89c4\u6a21\u4eba\u5de5\u9a8c\u8bc1\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b599\u4e2a\u516c\u5f00\u89c6\u9891\u548c5,360\u4e2a\u5f00\u653e\u5f0f\u95ee\u7b54\u5bf9\uff1b\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u89c6\u89c9\u4f18\u5316\u6846\u67b6AVR\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u589e\u52a0\u5e27\u91c7\u6837\u5bc6\u5ea6\u548c\u9009\u62e9\u6027\u8c03\u7528\u5916\u90e8\u5de5\u5177\u6765\u63d0\u5347\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u7406\u89e3\u3002", "result": "\u7cfb\u7edf\u8bc4\u4f30\u4e867\u79cd\u68c0\u7d22\u65b9\u6cd5\u548c14\u4e2a\u5e38\u7528MLLMs\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\uff08\u5305\u62ecGPT5\u548cGemini\uff09\u5728\u6355\u6349\u77ac\u6001\u4f46\u5173\u952e\u7684\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff1bAVR\u6846\u67b6\u5728\u6240\u6709\u8bc4\u4f30\u7684MLLMs\u4e2d\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u7406\u89e3\u548c\u6027\u80fd\u3002", "conclusion": "CFVBench\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7ec6\u8282\u6355\u6349\u65b9\u9762\u7684\u5173\u952e\u74f6\u9888\uff0c\u800cAVR\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2510.09230v1", "title": "Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras", "summary": "Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis),\nare common conditions affecting the health of people worldwide, and have a high\nincidence rate among the elderly and workers engaged in repetitive shoulder\ntasks. In regions with scarce medical resources, achieving early and accurate\ndiagnosis poses significant challenges, and there is an urgent need for\nlow-cost and easily scalable auxiliary diagnostic solutions. This research\nintroduces videos captured by consumer-grade devices as the basis for\ndiagnosis, reducing the cost for users. We focus on the innovative application\nof Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of\nshoulder disorders and propose a Hybrid Motion Video Diagnosis framework\n(HMVDx). This framework divides the two tasks of action understanding and\ndisease diagnosis, which are respectively completed by two MLLMs. In addition\nto traditional evaluation indicators, this work proposes a novel metric called\nUsability Index by the logical process of medical decision-making (action\nrecognition, movement diagnosis, and final diagnosis). This index evaluates the\neffectiveness of MLLMs in the medical field from the perspective of the entire\nmedical diagnostic pathway, revealing the potential value of low-cost MLLMs in\nmedical applications for medical practitioners. In experimental comparisons,\nthe accuracy of HMVDx in diagnosing shoulder joint injuries has increased by\n79.6\\% compared with direct video diagnosis, a significant technical\ncontribution to future research on the application of MLLMs for video\nunderstanding in the medical field.", "authors": ["Jindong Hong", "Wencheng Zhang", "Shiqin Qiao", "Jianhai Chen", "Jianing Qiu", "Chuanyang Zheng", "Qian Xu", "Yun Ji", "Qianyue Wen", "Weiwei Sun", "Hao Li", "Huizhen Li", "Huichao Wang", "Kai Wu", "Meng Li", "Yijun He", "Lingjie Luo", "Jiankai Sun"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "published": "2025-10-10T10:17:23Z", "pdf": "https://arxiv.org/pdf/2510.09230v1", "abs": "https://arxiv.org/abs/2510.09230v1", "comment": null, "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d88\u8d39\u7ea7\u8bbe\u5907\u89c6\u9891\u7684\u80a9\u90e8\u75be\u75c5\u8bca\u65ad\u6846\u67b6HMVDx\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5206\u522b\u5904\u7406\u52a8\u4f5c\u7406\u89e3\u548c\u75be\u75c5\u8bca\u65ad\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u80a9\u5173\u8282\u635f\u4f24\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u533b\u7597\u8d44\u6e90\u7a00\u7f3a\u5730\u533a\uff0c\u80a9\u90e8\u75be\u75c5\u5982\u51bb\u7ed3\u80a9\u7b49\u5e38\u89c1\u75be\u75c5\u7684\u65e9\u671f\u51c6\u786e\u8bca\u65ad\u9762\u4e34\u6311\u6218\uff0c\u6025\u9700\u4f4e\u6210\u672c\u3001\u6613\u6269\u5c55\u7684\u8f85\u52a9\u8bca\u65ad\u65b9\u6848\u3002", "method": "\u63d0\u51faHMVDx\u6df7\u5408\u8fd0\u52a8\u89c6\u9891\u8bca\u65ad\u6846\u67b6\uff0c\u5c06\u52a8\u4f5c\u7406\u89e3\u548c\u75be\u75c5\u8bca\u65ad\u4e24\u4e2a\u4efb\u52a1\u5206\u522b\u7531\u4e24\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b8c\u6210\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u533b\u7597\u51b3\u7b56\u903b\u8f91\u8fc7\u7a0b\u7684\u65b0\u8bc4\u4f30\u6307\u6807Usability Index\u3002", "result": "\u5b9e\u9a8c\u6bd4\u8f83\u663e\u793a\uff0cHMVDx\u5728\u80a9\u5173\u8282\u635f\u4f24\u8bca\u65ad\u4e2d\u7684\u51c6\u786e\u7387\u6bd4\u76f4\u63a5\u89c6\u9891\u8bca\u65ad\u63d0\u9ad8\u4e8679.6%\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4f4e\u6210\u672c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u5e94\u7528\u4e2d\u7684\u6f5c\u5728\u4ef7\u503c\uff0c\u4e3a\u672a\u6765\u533b\u5b66\u9886\u57df\u89c6\u9891\u7406\u89e3\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u8d21\u732e\u3002"}}
{"id": "2510.09221v1", "title": "HANDO: Hierarchical Autonomous Navigation and Dexterous Omni-loco-manipulation", "summary": "Seamless loco-manipulation in unstructured environments requires robots to\nleverage autonomous exploration alongside whole-body control for physical\ninteraction. In this work, we introduce HANDO (Hierarchical Autonomous\nNavigation and Dexterous Omni-loco-manipulation), a two-layer framework\ndesigned for legged robots equipped with manipulators to perform human-centered\nmobile manipulation tasks. The first layer utilizes a goal-conditioned\nautonomous exploration policy to guide the robot to semantically specified\ntargets, such as a black office chair in a dynamic environment. The second\nlayer employs a unified whole-body loco-manipulation policy to coordinate the\narm and legs for precise interaction tasks-for example, handing a drink to a\nperson seated on the chair. We have conducted an initial deployment of the\nnavigation module, and will continue to pursue finer-grained deployment of\nwhole-body loco-manipulation.", "authors": ["Jingyuan Sun", "Chaoran Wang", "Mingyu Zhang", "Cui Miao", "Hongyu Ji", "Zihan Qu", "Han Sun", "Bing Wang", "Qingyi Si"], "categories": ["cs.RO"], "published": "2025-10-10T10:04:30Z", "pdf": "https://arxiv.org/pdf/2510.09221v1", "abs": "https://arxiv.org/abs/2510.09221v1", "comment": "4 pages, 2 figures, this paper has been accepted for the workshop\n  Perception and Planning for Mobile Manipulation in Changing Environments\n  (PM2CE) at IROS 2025", "AI": {"tldr": "HANDO\u662f\u4e00\u4e2a\u7528\u4e8e\u914d\u5907\u673a\u68b0\u81c2\u7684\u817f\u5f0f\u673a\u5668\u4eba\u7684\u5206\u5c42\u6846\u67b6\uff0c\u5305\u542b\u81ea\u4e3b\u63a2\u7d22\u5c42\u548c\u5168\u8eab\u8fd0\u52a8\u64cd\u4f5c\u5c42\uff0c\u5b9e\u73b0\u4eba\u7c7b\u4e2d\u5fc3\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u65e0\u7f1d\u7684\u8fd0\u52a8\u64cd\u4f5c\u9700\u8981\u673a\u5668\u4eba\u7ed3\u5408\u81ea\u4e3b\u63a2\u7d22\u548c\u5168\u8eab\u63a7\u5236\u6765\u8fdb\u884c\u7269\u7406\u4ea4\u4e92", "method": "\u91c7\u7528\u4e24\u5c42\u6846\u67b6\uff1a\u7b2c\u4e00\u5c42\u4f7f\u7528\u76ee\u6807\u5bfc\u5411\u7684\u81ea\u4e3b\u63a2\u7d22\u7b56\u7565\u5bfc\u822a\u5230\u8bed\u4e49\u6307\u5b9a\u76ee\u6807\uff1b\u7b2c\u4e8c\u5c42\u4f7f\u7528\u7edf\u4e00\u7684\u5168\u8eab\u8fd0\u52a8\u64cd\u4f5c\u7b56\u7565\u534f\u8c03\u624b\u81c2\u548c\u817f\u90e8\u8fdb\u884c\u7cbe\u786e\u4ea4\u4e92", "result": "\u5df2\u5b8c\u6210\u5bfc\u822a\u6a21\u5757\u7684\u521d\u6b65\u90e8\u7f72\uff0c\u5c06\u7ee7\u7eed\u63a8\u8fdb\u5168\u8eab\u8fd0\u52a8\u64cd\u4f5c\u7684\u66f4\u7cbe\u7ec6\u90e8\u7f72", "conclusion": "HANDO\u6846\u67b6\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.09220v1", "title": "Serial Polar Automorphism Ensemble Decoders for Physical Unclonable Functions", "summary": "Physical unclonable functions (PUFs) involve challenging practical\napplications of error-correcting codes (ECCs), requiring extremely low failure\nrates on the order of $10^{-6}$ and below despite raw input bit error rates as\nhigh as 22%. These requirements call for an efficient ultra-low rate code\ndesign. In this work, we propose a novel coding scheme tailored for PUFs based\non Polar codes and a low-complexity version of automorphism ensemble decoding\n(AED). Notably, our serial AED scheme reuses a single successive cancellation\n(SC) decoder across multiple decoding attempts. By introducing cascaded and\nrecursive interleavers, we efficiently scale the number of AED candidates\nwithout requiring expensive large multiplexers. An aggressive quantization\nstrategy of only 3 bits per message further reduces the area requirements of\nthe underlying SC decoder. The resulting coding scheme achieves the same block\nerror rate of $10^{-6}$ as our baseline based on Bose-Ray-Chaudhuri-Hocquenghem\n(BCH) codes while requiring 1.75x fewer codeword bits to encode the same K =\n312 payload bits. This reduction translates directly into 1.75x less helper\ndata storage and, consequently, a smaller overall chip area.", "authors": ["Marvin R\u00fcbenacke", "Sebastian Cammerer", "Michael Sullivan", "Alexander Keller"], "categories": ["cs.IT", "eess.SP", "math.IT"], "published": "2025-10-10T10:02:42Z", "pdf": "https://arxiv.org/pdf/2510.09220v1", "abs": "https://arxiv.org/abs/2510.09220v1", "comment": "7 Pages, 7 Figures, submitted to IEEE for possible publication", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePolar\u7801\u548c\u4f4e\u590d\u6742\u5ea6\u81ea\u540c\u6784\u96c6\u6210\u89e3\u7801(AED)\u7684\u7269\u7406\u4e0d\u53ef\u514b\u9686\u51fd\u6570(PUF)\u7f16\u7801\u65b9\u6848\uff0c\u76f8\u6bd4BCH\u7801\u51cf\u5c11\u4e8643%\u7684\u7801\u5b57\u6bd4\u7279\u6570\uff0c\u540c\u65f6\u4fdd\u630110^-6\u7684\u5757\u9519\u8bef\u7387\u3002", "motivation": "PUF\u5e94\u7528\u9700\u8981\u6781\u4f4e\u7684\u5931\u8d25\u7387(10^-6\u4ee5\u4e0b)\u548c\u9ad8\u539f\u59cb\u8bef\u7801\u7387(22%)\uff0c\u8fd9\u8981\u6c42\u8bbe\u8ba1\u9ad8\u6548\u7684\u8d85\u4f4e\u7801\u7387\u7f16\u7801\u65b9\u6848\u3002", "method": "\u91c7\u7528Polar\u7801\u548c\u4e32\u884cAED\u65b9\u6848\uff0c\u91cd\u7528\u5355\u4e2a\u8fde\u7eed\u6d88\u9664(SC)\u89e3\u7801\u5668\u8fdb\u884c\u591a\u6b21\u89e3\u7801\u5c1d\u8bd5\uff1b\u901a\u8fc7\u7ea7\u8054\u548c\u9012\u5f52\u4ea4\u7ec7\u5668\u6269\u5c55AED\u5019\u9009\u6570\uff1b\u4f7f\u75283\u4f4d\u91cf\u5316\u7b56\u7565\u964d\u4f4eSC\u89e3\u7801\u5668\u9762\u79ef\u9700\u6c42\u3002", "result": "\u5728K=312\u6709\u6548\u8f7d\u8377\u6bd4\u7279\u4e0b\uff0c\u4e0eBCH\u7801\u57fa\u7ebf\u76f8\u6bd4\uff0c\u7801\u5b57\u6bd4\u7279\u6570\u51cf\u5c111.75\u500d\uff0c\u540c\u65f6\u4fdd\u630110^-6\u7684\u5757\u9519\u8bef\u7387\uff1b\u51cf\u5c11\u4e861.75\u500d\u7684\u8f85\u52a9\u6570\u636e\u5b58\u50a8\u9700\u6c42\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7f16\u7801\u65b9\u6848\u5728\u4fdd\u6301\u76f8\u540c\u9519\u8bef\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7801\u5b57\u6bd4\u7279\u6570\u548c\u82af\u7247\u9762\u79ef\uff0c\u4e3aPUF\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7f16\u7801\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09203v1", "title": "Cattle-CLIP: A Multimodal Framework for Cattle Behaviour Recognition", "summary": "Cattle behaviour is a crucial indicator of an individual animal health,\nproductivity and overall well-being. Video-based monitoring, combined with deep\nlearning techniques, has become a mainstream approach in animal biometrics, and\nit can offer high accuracy in some behaviour recognition tasks. We present\nCattle-CLIP, a multimodal deep learning framework for cattle behaviour\nrecognition, using semantic cues to improve the performance of video-based\nvisual feature recognition. It is adapted from the large-scale image-language\nmodel CLIP by adding a temporal integration module. To address the domain gap\nbetween web data used for the pre-trained model and real-world cattle\nsurveillance footage, we introduce tailored data augmentation strategies and\nspecialised text prompts. Cattle-CLIP is evaluated under both fully-supervised\nand few-shot learning scenarios, with a particular focus on data-scarce\nbehaviour recognition - an important yet under-explored goal in livestock\nmonitoring. To evaluate the proposed method, we release the CattleBehaviours6\ndataset, which comprises six types of indoor behaviours: feeding, drinking,\nstanding-self-grooming, standing-ruminating, lying-self-grooming and\nlying-ruminating. The dataset consists of 1905 clips collected from our John\nOldacre Centre dairy farm research platform housing 200 Holstein-Friesian cows.\nExperiments show that Cattle-CLIP achieves 96.1% overall accuracy across six\nbehaviours in a supervised setting, with nearly 100% recall for feeding,\ndrinking and standing-ruminating behaviours, and demonstrates robust\ngeneralisation with limited data in few-shot scenarios, highlighting the\npotential of multimodal learning in agricultural and animal behaviour analysis.", "authors": ["Huimin Liu", "Jing Gao", "Daria Baran", "AxelX Montout", "Neill W Campbell", "Andrew W Dowsey"], "categories": ["cs.CV"], "published": "2025-10-10T09:43:12Z", "pdf": "https://arxiv.org/pdf/2510.09203v1", "abs": "https://arxiv.org/abs/2510.09203v1", "comment": "16 pages, 10 figures, submitted to Computers and Electronics in\n  Agriculture", "AI": {"tldr": "Cattle-CLIP\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u725b\u53ea\u884c\u4e3a\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u7ebf\u7d22\u63d0\u5347\u89c6\u9891\u7279\u5f81\u8bc6\u522b\u6027\u80fd\uff0c\u5728\u76d1\u7763\u5b66\u4e60\u548c\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u725b\u53ea\u884c\u4e3a\u662f\u52a8\u7269\u5065\u5eb7\u3001\u751f\u4ea7\u529b\u548c\u6574\u4f53\u798f\u7949\u7684\u91cd\u8981\u6307\u6807\u3002\u89c6\u9891\u76d1\u63a7\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u5df2\u6210\u4e3a\u52a8\u7269\u751f\u7269\u8bc6\u522b\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u7684\u884c\u4e3a\u8bc6\u522b\u4efb\u52a1\u4e2d\u5b58\u5728\u6027\u80fd\u74f6\u9888\u3002", "method": "\u57fa\u4e8e\u5927\u89c4\u6a21\u56fe\u50cf-\u8bed\u8a00\u6a21\u578bCLIP\u8fdb\u884c\u9002\u914d\uff0c\u6dfb\u52a0\u65f6\u95f4\u6574\u5408\u6a21\u5757\uff0c\u91c7\u7528\u5b9a\u5236\u5316\u6570\u636e\u589e\u5f3a\u7b56\u7565\u548c\u4e13\u95e8\u6587\u672c\u63d0\u793a\u6765\u89e3\u51b3\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u771f\u5b9e\u725b\u53ea\u76d1\u63a7\u89c6\u9891\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002", "result": "\u5728\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0cCattle-CLIP\u5728\u516d\u79cd\u884c\u4e3a\u4e0a\u7684\u603b\u4f53\u51c6\u786e\u7387\u8fbe\u523096.1%\uff0c\u5176\u4e2d\u8fdb\u98df\u3001\u996e\u6c34\u548c\u7ad9\u7acb\u53cd\u520d\u884c\u4e3a\u7684\u53ec\u56de\u7387\u63a5\u8fd1100%\uff0c\u5e76\u5728\u5c11\u6837\u672c\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u5728\u519c\u4e1a\u548c\u52a8\u7269\u884c\u4e3a\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u884c\u4e3a\u8bc6\u522b\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.09200v1", "title": "Towards Safer and Understandable Driver Intention Prediction", "summary": "Autonomous driving (AD) systems are becoming increasingly capable of handling\ncomplex tasks, mainly due to recent advances in deep learning and AI. As\ninteractions between autonomous systems and humans increase, the\ninterpretability of decision-making processes in driving systems becomes\nincreasingly crucial for ensuring safe driving operations. Successful\nhuman-machine interaction requires understanding the underlying representations\nof the environment and the driving task, which remains a significant challenge\nin deep learning-based systems. To address this, we introduce the task of\ninterpretability in maneuver prediction before they occur for driver safety,\ni.e., driver intent prediction (DIP), which plays a critical role in AD\nsystems. To foster research in interpretable DIP, we curate the eXplainable\nDriving Action Anticipation Dataset (DAAD-X), a new multimodal, ego-centric\nvideo dataset to provide hierarchical, high-level textual explanations as\ncausal reasoning for the driver's decisions. These explanations are derived\nfrom both the driver's eye-gaze and the ego-vehicle's perspective. Next, we\npropose Video Concept Bottleneck Model (VCBM), a framework that generates\nspatio-temporally coherent explanations inherently, without relying on post-hoc\ntechniques. Finally, through extensive evaluations of the proposed VCBM on the\nDAAD-X dataset, we demonstrate that transformer-based models exhibit greater\ninterpretability than conventional CNN-based models. Additionally, we introduce\na multilabel t-SNE visualization technique to illustrate the disentanglement\nand causal correlation among multiple explanations. Our data, code and models\nare available at: https://mukil07.github.io/VCBM.github.io/", "authors": ["Mukilan Karuppasamy", "Shankar Gangisetty", "Shyam Nandan Rai", "Carlo Masone", "C V Jawahar"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "published": "2025-10-10T09:41:25Z", "pdf": "https://arxiv.org/pdf/2510.09200v1", "abs": "https://arxiv.org/abs/2510.09200v1", "comment": "10 pages", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u9a7e\u9a76\u5458\u610f\u56fe\u9884\u6d4b\u6846\u67b6VCBM\uff0c\u521b\u5efa\u4e86DAAD-X\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u751f\u6210\u65f6\u7a7a\u4e00\u81f4\u7684\u89e3\u91ca\uff0c\u8bc1\u660e\u57fa\u4e8etransformer\u7684\u6a21\u578b\u6bd4CNN\u6a21\u578b\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u589e\u52a0\uff0c\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u5bf9\u5b89\u5168\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\u3002\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u9700\u8981\u7406\u89e3\u73af\u5883\u548c\u9a7e\u9a76\u4efb\u52a1\u7684\u57fa\u7840\u8868\u5f81\uff0c\u8fd9\u5728\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7cfb\u7edf\u4e2d\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u89c6\u9891\u6982\u5ff5\u74f6\u9888\u6a21\u578b(VCBM)\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u56fa\u6709\u5730\u751f\u6210\u65f6\u7a7a\u4e00\u81f4\u7684\u89e3\u91ca\uff0c\u800c\u4e0d\u4f9d\u8d56\u540e\u5904\u7406\u6280\u672f\u3002\u521b\u5efa\u4e86DAAD-X\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u9a7e\u9a76\u5458\u51b3\u7b56\u7684\u5c42\u6b21\u5316\u6587\u672c\u89e3\u91ca\u3002", "result": "\u5728DAAD-X\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u57fa\u4e8etransformer\u7684\u6a21\u578b\u6bd4\u4f20\u7edf\u7684\u57fa\u4e8eCNN\u7684\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5927\u7684\u53ef\u89e3\u91ca\u6027\u3002\u8fd8\u5f15\u5165\u4e86\u591a\u6807\u7b7et-SNE\u53ef\u89c6\u5316\u6280\u672f\u6765\u8bf4\u660e\u591a\u4e2a\u89e3\u91ca\u4e4b\u95f4\u7684\u89e3\u7f20\u548c\u56e0\u679c\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53ef\u89e3\u91ca\u7684\u9a7e\u9a76\u5458\u610f\u56fe\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u6846\u67b6\uff0c\u8bc1\u660e\u4e86transformer\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u4ea4\u4e92\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2510.09201v1", "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs", "summary": "Large Language Models (LLMs) have shown remarkable success, and their\nmultimodal expansions (MLLMs) further unlock capabilities spanning images,\nvideos, and other modalities beyond text. However, despite this shift, prompt\noptimization approaches, designed to reduce the burden of manual prompt\ncrafting while maximizing performance, remain confined to text, ultimately\nlimiting the full potential of MLLMs. Motivated by this gap, we introduce the\nnew problem of multimodal prompt optimization, which expands the prior\ndefinition of prompt optimization to the multimodal space defined by the pairs\nof textual and non-textual prompts. To tackle this problem, we then propose the\nMultimodal Prompt Optimizer (MPO), a unified framework that not only performs\nthe joint optimization of multimodal prompts through alignment-preserving\nupdates but also guides the selection process of candidate prompts by\nleveraging earlier evaluations as priors in a Bayesian-based selection\nstrategy. Through extensive experiments across diverse modalities that go\nbeyond text, such as images, videos, and even molecules, we demonstrate that\nMPO outperforms leading text-only optimization methods, establishing multimodal\nprompt optimization as a crucial step to realizing the potential of MLLMs.", "authors": ["Yumin Choi", "Dongki Kim", "Jinheon Baek", "Sung Ju Hwang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2025-10-10T09:41:25Z", "pdf": "https://arxiv.org/pdf/2510.09201v1", "abs": "https://arxiv.org/abs/2510.09201v1", "comment": null, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u591a\u6a21\u6001\u63d0\u793a\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86MPO\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6587\u672c\u548c\u975e\u6587\u672c\u63d0\u793a\u6765\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5c40\u9650\u4e8e\u6587\u672c\u6a21\u6001\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u56e0\u6b64\u9700\u8981\u6269\u5c55\u5230\u591a\u6a21\u6001\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u63d0\u793a\u4f18\u5316\u5668(MPO)\uff0c\u901a\u8fc7\u4fdd\u6301\u5bf9\u9f50\u7684\u8054\u5408\u66f4\u65b0\u6765\u4f18\u5316\u591a\u6a21\u6001\u63d0\u793a\uff0c\u5e76\u5229\u7528\u8d1d\u53f6\u65af\u9009\u62e9\u7b56\u7565\u6307\u5bfc\u5019\u9009\u63d0\u793a\u9009\u62e9\u3002", "result": "\u5728\u56fe\u50cf\u3001\u89c6\u9891\u3001\u5206\u5b50\u7b49\u591a\u79cd\u6a21\u6001\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMPO\u4f18\u4e8e\u9886\u5148\u7684\u7eaf\u6587\u672c\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u63d0\u793a\u4f18\u5316\u662f\u5b9e\u73b0\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6f5c\u529b\u7684\u5173\u952e\u6b65\u9aa4\u3002"}}
{"id": "2510.09188v1", "title": "Decentralized Multi-Robot Relative Navigation in Unknown, Structurally Constrained Environments under Limited Communication", "summary": "Multi-robot navigation in unknown, structurally constrained, and GPS-denied\nenvironments presents a fundamental trade-off between global strategic\nforesight and local tactical agility, particularly under limited communication.\nCentralized methods achieve global optimality but suffer from high\ncommunication overhead, while distributed methods are efficient but lack the\nbroader awareness to avoid deadlocks and topological traps. To address this, we\npropose a fully decentralized, hierarchical relative navigation framework that\nachieves both strategic foresight and tactical agility without a unified\ncoordinate system. At the strategic layer, robots build and exchange\nlightweight topological maps upon opportunistic encounters. This process\nfosters an emergent global awareness, enabling the planning of efficient,\ntrap-avoiding routes at an abstract level. This high-level plan then inspires\nthe tactical layer, which operates on local metric information. Here, a\nsampling-based escape point strategy resolves dense spatio-temporal conflicts\nby generating dynamically feasible trajectories in real time, concurrently\nsatisfying tight environmental and kinodynamic constraints. Extensive\nsimulations and real-world experiments demonstrate that our system\nsignificantly outperforms in success rate and efficiency, especially in\ncommunication-limited environments with complex topological structures.", "authors": ["Zihao Mao", "Yunheng Wang", "Yunting Ji", "Yi Yang", "Wenjie Song"], "categories": ["cs.RO", "cs.MA"], "published": "2025-10-10T09:33:20Z", "pdf": "https://arxiv.org/pdf/2510.09188v1", "abs": "https://arxiv.org/abs/2510.09188v1", "comment": null, "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u5206\u5c42\u76f8\u5bf9\u5bfc\u822a\u6846\u67b6\uff0c\u5728\u672a\u77e5\u3001\u7ed3\u6784\u53d7\u9650\u4e14\u65e0GPS\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u6218\u7565\u8fdc\u89c1\u548c\u6218\u672f\u654f\u6377\u6027\uff0c\u65e0\u9700\u7edf\u4e00\u5750\u6807\u7cfb\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5bfc\u822a\u5728\u5168\u5c40\u6218\u7565\u8fdc\u89c1\u548c\u5c40\u90e8\u6218\u672f\u654f\u6377\u6027\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e0b\u3002\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u901a\u4fe1\u5f00\u9500\u5927\uff0c\u5206\u5e03\u5f0f\u65b9\u6cd5\u7f3a\u4e4f\u5168\u5c40\u610f\u8bc6\u5bb9\u6613\u9677\u5165\u6b7b\u9501\u548c\u62d3\u6251\u9677\u9631\u3002", "method": "\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff1a\u6218\u7565\u5c42\u901a\u8fc7\u673a\u4f1a\u6027\u76f8\u9047\u6784\u5efa\u548c\u4ea4\u6362\u8f7b\u91cf\u7ea7\u62d3\u6251\u5730\u56fe\uff0c\u57f9\u517b\u6d8c\u73b0\u7684\u5168\u5c40\u610f\u8bc6\uff1b\u6218\u672f\u5c42\u57fa\u4e8e\u5c40\u90e8\u5ea6\u91cf\u4fe1\u606f\uff0c\u91c7\u7528\u57fa\u4e8e\u91c7\u6837\u7684\u9003\u9038\u70b9\u7b56\u7565\u5b9e\u65f6\u751f\u6210\u52a8\u6001\u53ef\u884c\u8f68\u8ff9\u3002", "result": "\u5e7f\u6cdb\u7684\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u6210\u529f\u7387\u548c\u5de5\u4f5c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u901a\u4fe1\u53d7\u9650\u4e14\u5177\u6709\u590d\u6742\u62d3\u6251\u7ed3\u6784\u7684\u73af\u5883\u4e2d\u3002", "conclusion": "\u8be5\u5206\u5c42\u76f8\u5bf9\u5bfc\u822a\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u6218\u7565\u8fdc\u89c1\u548c\u6218\u672f\u654f\u6377\u6027\u7684\u5e73\u8861\uff0c\u5728\u901a\u4fe1\u53d7\u9650\u7684\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09134v1", "title": "A Semantic Framework for Patient Digital Twins in Chronic Care", "summary": "Personalized chronic care requires the integration of multimodal health data\nto enable precise, adaptive, and preventive decision-making. Yet most current\ndigital twin (DT) applications remain organ-specific or tied to isolated data\ntypes, lacking a unified and privacy-preserving foundation. This paper\nintroduces the Patient Medical Digital Twin (PMDT), an ontology-driven in\nsilico patient framework that integrates physiological, psychosocial,\nbehavioral, and genomic information into a coherent, extensible model.\nImplemented in OWL 2.0, the PMDT ensures semantic interoperability, supports\nautomated reasoning, and enables reuse across diverse clinical contexts. Its\nontology is structured around modular Blueprints (patient, disease and\ndiagnosis, treatment and follow-up, trajectories, safety, pathways, and adverse\nevents), formalized through dedicated conceptual views. These were iteratively\nrefined and validated through expert workshops, questionnaires, and a pilot\nstudy in the EU H2020 QUALITOP project with real-world immunotherapy patients.\nEvaluation confirmed ontology coverage, reasoning correctness, usability, and\nGDPR compliance. Results demonstrate the PMDT's ability to unify heterogeneous\ndata, operationalize competency questions, and support descriptive, predictive,\nand prescriptive analytics in a federated, privacy-preserving manner. By\nbridging gaps in data fragmentation and semantic standardization, the PMDT\nprovides a validated foundation for next-generation digital health ecosystems,\ntransforming chronic care toward proactive, continuously optimized, and\nequitable management.", "authors": ["Amal Elgammal", "Bernd J. Kr\u00e4mer", "Michael P. Papazoglou", "Mira Raheem"], "categories": ["cs.SE", "cs.ET"], "published": "2025-10-10T08:34:55Z", "pdf": "https://arxiv.org/pdf/2510.09134v1", "abs": "https://arxiv.org/abs/2510.09134v1", "comment": "This manuscript is currently under review at Software and Systems\n  Modeling (SoSyM)", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u60a3\u8005\u533b\u7597\u6570\u5b57\u5b6a\u751f\uff08PMDT\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u672c\u4f53\u7684\u60a3\u8005\u6a21\u578b\uff0c\u6574\u5408\u4e86\u751f\u7406\u3001\u5fc3\u7406\u3001\u884c\u4e3a\u3001\u57fa\u56e0\u7ec4\u7b49\u591a\u6a21\u6001\u5065\u5eb7\u6570\u636e\uff0c\u652f\u6301\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u3001\u81ea\u52a8\u63a8\u7406\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u591a\u4e3a\u5668\u5b98\u7279\u5f02\u6027\u6216\u5c40\u9650\u4e8e\u5b64\u7acb\u6570\u636e\u7c7b\u578b\uff0c\u7f3a\u4e4f\u7edf\u4e00\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u6846\u67b6\u3002\u9700\u8981\u6574\u5408\u591a\u6a21\u6001\u5065\u5eb7\u6570\u636e\u4ee5\u5b9e\u73b0\u7cbe\u51c6\u3001\u81ea\u9002\u5e94\u548c\u9884\u9632\u6027\u51b3\u7b56\u3002", "method": "\u91c7\u7528OWL 2.0\u5b9e\u73b0\u672c\u4f53\u9a71\u52a8\u7684\u60a3\u8005\u6a21\u578b\uff0c\u56f4\u7ed5\u6a21\u5757\u5316\u84dd\u56fe\uff08\u60a3\u8005\u3001\u75be\u75c5\u8bca\u65ad\u3001\u6cbb\u7597\u968f\u8bbf\u3001\u8f68\u8ff9\u3001\u5b89\u5168\u3001\u8def\u5f84\u3001\u4e0d\u826f\u4e8b\u4ef6\uff09\u6784\u5efa\uff0c\u901a\u8fc7\u4e13\u5bb6\u7814\u8ba8\u4f1a\u3001\u95ee\u5377\u548c\u771f\u5b9e\u4e16\u754c\u514d\u75ab\u6cbb\u7597\u60a3\u8005\u8bd5\u70b9\u7814\u7a76\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u548c\u9a8c\u8bc1\u3002", "result": "\u8bc4\u4f30\u786e\u8ba4\u4e86\u672c\u4f53\u8986\u76d6\u5ea6\u3001\u63a8\u7406\u6b63\u786e\u6027\u3001\u53ef\u7528\u6027\u548cGDPR\u5408\u89c4\u6027\u3002PMDT\u80fd\u591f\u7edf\u4e00\u5f02\u6784\u6570\u636e\uff0c\u64cd\u4f5c\u80fd\u529b\u95ee\u9898\uff0c\u652f\u6301\u63cf\u8ff0\u6027\u3001\u9884\u6d4b\u6027\u548c\u89c4\u8303\u6027\u5206\u6790\u3002", "conclusion": "PMDT\u901a\u8fc7\u5f25\u5408\u6570\u636e\u788e\u7247\u5316\u548c\u8bed\u4e49\u6807\u51c6\u5316\u65b9\u9762\u7684\u5dee\u8ddd\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u6570\u5b57\u5065\u5eb7\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u57fa\u7840\uff0c\u5c06\u6162\u6027\u75c5\u62a4\u7406\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u3001\u6301\u7eed\u4f18\u5316\u548c\u516c\u5e73\u7684\u7ba1\u7406\u6a21\u5f0f\u3002"}}
