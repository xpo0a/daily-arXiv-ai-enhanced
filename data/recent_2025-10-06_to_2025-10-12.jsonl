{"id": "2510.10851v1", "title": "Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion", "summary": "Humanoid locomotion requires not only accurate command tracking for\nnavigation but also compliant responses to external forces during human\ninteraction. Despite significant progress, existing RL approaches mainly\nemphasize robustness, yielding policies that resist external forces but lack\ncompliance-particularly challenging for inherently unstable humanoids. In this\nwork, we address this by formulating humanoid locomotion as a multi-objective\noptimization problem that balances command tracking and external force\ncompliance. We introduce a preference-conditioned multi-objective RL (MORL)\nframework that integrates rigid command following and compliant behaviors\nwithin a single omnidirectional locomotion policy. External forces are modeled\nvia velocity-resistance factor for consistent reward design, and training\nleverages an encoder-decoder structure that infers task-relevant privileged\nfeatures from deployable observations. We validate our approach in both\nsimulation and real-world experiments on a humanoid robot. Experimental results\nindicate that our framework not only improves adaptability and convergence over\nstandard pipelines, but also realizes deployable preference-conditioned\nhumanoid locomotion.", "authors": ["Tingxuan Leng", "Yushi Wang", "Tinglong Zheng", "Changsheng Luo", "Mingguo Zhao"], "categories": ["cs.RO"], "published": "2025-10-12T23:29:03Z", "pdf": "https://arxiv.org/pdf/2510.10851v1", "abs": "https://arxiv.org/abs/2510.10851v1", "comment": null}
{"id": "2510.10846v1", "title": "DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models", "summary": "As vision-language models become increasingly capable, maintaining a balance\nbetween safety and usefulness remains a central challenge. Safety mechanisms,\nwhile essential, can backfire, causing over-refusal, where models decline\nbenign requests out of excessive caution. Yet, no existing benchmark has\nsystematically addressed over-refusal in the visual modality. This setting\nintroduces unique challenges, such as dual-use cases where an instruction is\nharmless, but the accompanying image contains harmful content. Models\nfrequently fail in such scenarios, either refusing too conservatively or\ncompleting tasks unsafely, which highlights the need for more fine-grained\nalignment. The ideal behavior is safe completion, i.e., fulfilling the benign\nparts of a request while explicitly warning about any potentially harmful\nelements. To address this, we present DUAL-Bench, the first multimodal\nbenchmark focused on over-refusal and safe completion in VLMs. We evaluated 18\nVLMs across 12 hazard categories, with focus on their robustness under\nsemantics-preserving visual perturbations. The results reveal substantial room\nfor improvement: GPT-5-Nano achieves 12.9% safe completion, GPT-5 models\naverage 7.9%, and Qwen models only 3.9%. We hope that DUAL-Bench will foster\nthe development of more nuanced alignment strategies that ensure models remain\nboth safe and useful in complex multimodal settings.", "authors": ["Kaixuan Ren", "Preslav Nakov", "Usman Naseem"], "categories": ["cs.CL"], "published": "2025-10-12T23:21:34Z", "pdf": "https://arxiv.org/pdf/2510.10846v1", "abs": "https://arxiv.org/abs/2510.10846v1", "comment": "25 pages, 91 figures, submitted to Oct ARR under reviewing"}
{"id": "2510.10843v1", "title": "Contact Sensing via Joint Torque Sensors and a Force/Torque Sensor for Legged Robots", "summary": "This paper presents a method for detecting and localizing contact along robot\nlegs using distributed joint torque sensors and a single hip-mounted\nforce-torque (FT) sensor using a generalized momentum-based observer framework.\nWe designed a low-cost strain-gauge-based joint torque sensor that can be\ninstalled on every joint to provide direct torque measurements, eliminating the\nneed for complex friction models and providing more accurate torque readings\nthan estimation based on motor current. Simulation studies on a floating-based\n2-DoF robot leg verified that the proposed framework accurately recovers\ncontact force and location along the thigh and shin links. Through a\ncalibration procedure, our torque sensor achieved an average 96.4% accuracy\nrelative to ground truth measurements. Building upon the torque sensor, we\nperformed hardware experiments on a 2-DoF manipulator, which showed\nsub-centimeter contact localization accuracy and force errors below 0.2 N.", "authors": ["Jared Grinberg", "Yanran Ding"], "categories": ["cs.RO"], "published": "2025-10-12T23:15:03Z", "pdf": "https://arxiv.org/pdf/2510.10843v1", "abs": "https://arxiv.org/abs/2510.10843v1", "comment": "Proc. IEEE 21st International Conference on Automation Science and\n  Engineering (CASE), Los Angeles, CA, USA, Aug. 17-21, 2025, pp. 1-7,\n  doi:10.1109/CASE58245.2025.11164031"}
{"id": "2510.10836v1", "title": "Spinning into Quantum Geometry: Dirac and Wheeler-DeWitt Dynamics from Stochastic Helicity", "summary": "Spin networks in loop quantum gravity provide a kinematical picture of\nquantum geometry but lack a natural mechanism for dynamical Dirac-type\nevolution, while the Wheeler--DeWitt equation typically enters only as an\nimposed constraint. We propose a stochastic framework in which each\nspin-network edge carries helicity-resolved amplitudes -- two-state internal\nlabels that undergo Poisson-driven flips. The resulting coupled master\nequations, after analytic continuation and the introduction of a fundamental\nlength scale, generate Dirac-type dynamics on discrete geometry. At long times,\nthe same process relaxes to helicity-symmetric equilibrium states, which are\nshown to satisfy a Wheeler--DeWitt-type condition. In this way, both quantum\nevolution and the gravitational constraint emerge within a single probabilistic\nframework. Our approach thus provides a background-independent and stochastic\nroute to quantum geometry, offering an alternative to canonical quantization\nand a fresh perspective on the problem of time.", "authors": ["Partha Nandi", "Partha Ghose", "Francesco Petruccione"], "categories": ["gr-qc", "hep-th", "quant-ph"], "published": "2025-10-12T23:00:29Z", "pdf": "https://arxiv.org/pdf/2510.10836v1", "abs": "https://arxiv.org/abs/2510.10836v1", "comment": "This manuscript is 13 pages long and contains 1 figure"}
{"id": "2510.10823v1", "title": "The Irrational Machine: Neurosis and the Limits of Algorithmic Safety", "summary": "We present a framework for characterizing neurosis in embodied AI: behaviors\nthat are internally coherent yet misaligned with reality, arising from\ninteractions among planning, uncertainty handling, and aversive memory. In a\ngrid navigation stack we catalogue recurrent modalities including flip-flop,\nplan churn, perseveration loops, paralysis and hypervigilance, futile search,\nbelief incoherence, tie break thrashing, corridor thrashing, optimality\ncompulsion, metric mismatch, policy oscillation, and limited-visibility\nvariants. For each we give lightweight online detectors and reusable escape\npolicies (short commitments, a margin to switch, smoothing, principled\narbitration). We then show that durable phobic avoidance can persist even under\nfull visibility when learned aversive costs dominate local choice, producing\nlong detours despite globally safe routes. Using First/Second/Third Law as\nengineering shorthand for safety latency, command compliance, and resource\nefficiency, we argue that local fixes are insufficient; global failures can\nremain. To surface them, we propose genetic-programming based destructive\ntesting that evolves worlds and perturbations to maximize law pressure and\nneurosis scores, yielding adversarial curricula and counterfactual traces that\nexpose where architectural revision, not merely symptom-level patches, is\nrequired.", "authors": ["Daniel Howard"], "categories": ["cs.AI", "cs.NE", "cs.RO"], "published": "2025-10-12T22:22:17Z", "pdf": "https://arxiv.org/pdf/2510.10823v1", "abs": "https://arxiv.org/abs/2510.10823v1", "comment": "41 pages, 17 figures, 5 tables"}
{"id": "2510.10804v1", "title": "Representing Data in Robotic Tactile Perception -- A Review", "summary": "Robotic tactile perception is a complex process involving several\ncomputational steps performed at different levels. Tactile information is\nshaped by the interplay of robot actions, the mechanical properties of its\nbody, and the software that processes the data. In this respect, high-level\ncomputation, required to process and extract information, is commonly performed\nby adapting existing techniques from other domains, such as computer vision,\nwhich expects input data to be properly structured. Therefore, it is necessary\nto transform tactile sensor data to match a specific data structure. This\noperation directly affects the tactile information encoded and, as a\nconsequence, the task execution. This survey aims to address this specific\naspect of the tactile perception pipeline, namely Data Representation. The\npaper first clearly defines its contributions to the perception pipeline and\nthen reviews how previous studies have dealt with the problem of representing\ntactile information, investigating the relationships among hardware,\nrepresentations, and high-level computation methods. The analysis has led to\nthe identification of six structures commonly used in the literature to\nrepresent data. The manuscript provides discussions and guidelines for properly\nselecting a representation depending on operating conditions, including the\navailable hardware, the tactile information required to be encoded, and the\ntask at hand.", "authors": ["Alessandro Albini", "Mohsen Kaboli", "Giorgio Cannata", "Perla Maiolino"], "categories": ["cs.RO"], "published": "2025-10-12T20:48:35Z", "pdf": "https://arxiv.org/pdf/2510.10804v1", "abs": "https://arxiv.org/abs/2510.10804v1", "comment": null}
{"id": "2510.10781v1", "title": "Two-Layer Voronoi Coverage Control for Hybrid Aerial-Ground Robot Teams in Emergency Response: Implementation and Analysis", "summary": "We present a comprehensive two-layer Voronoi coverage control approach for\ncoordinating hybrid aerial-ground robot teams in hazardous material emergency\nresponse scenarios. Traditional Voronoi coverage control methods face three\ncritical limitations in emergency contexts: heterogeneous agent capabilities\nwith vastly different velocities, clustered initial deployment configurations,\nand urgent time constraints requiring rapid response rather than eventual\nconvergence. Our method addresses these challenges through a decoupled\ntwo-layer architecture that separately optimizes aerial and ground robot\npositioning, with aerial agents delivering ground sensors via airdrop to\nhigh-priority locations. We provide detailed implementation of bounded Voronoi\ncell computation, efficient numerical integration techniques for\nimportance-weighted centroids, and robust control strategies that prevent agent\ntrapping. Simulation results demonstrate an 88% reduction in response time,\nachieving target sensor coverage (18.5% of initial sensor loss) in 25 seconds\ncompared to 220 seconds for ground-only deployment. Complete implementation\ncode is available at https://github.com/dHutchings/ME292B.", "authors": ["Douglas Hutchings", "Luai Abuelsamen", "Karthik Rajgopal"], "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY", "I.2.9; I.2.11"], "published": "2025-10-12T19:53:40Z", "pdf": "https://arxiv.org/pdf/2510.10781v1", "abs": "https://arxiv.org/abs/2510.10781v1", "comment": "23 pages, 7 figures. Technical report with complete implementation\n  details and open-source code"}
{"id": "2510.10778v1", "title": "Real2USD: Scene Representations in Universal Scene Description Language", "summary": "Large Language Models (LLMs) can help robots reason about abstract task\nspecifications. This requires augmenting classical representations of the\nenvironment used by robots with natural language-based priors. There are a\nnumber of existing approaches to doing so, but they are tailored to specific\ntasks, e.g., visual-language models for navigation, language-guided neural\nradiance fields for mapping, etc. This paper argues that the Universal Scene\nDescription (USD) language is an effective and general representation of\ngeometric, photometric and semantic information in the environment for\nLLM-based robotics tasks. Our argument is simple: a USD is an XML-based scene\ngraph, readable by LLMs and humans alike, and rich enough to support\nessentially any task -- Pixar developed this language to store assets, scenes\nand even movies. We demonstrate a ``Real to USD'' system using a Unitree Go2\nquadruped robot carrying LiDAR and a RGB camera that (i) builds an explicit USD\nrepresentation of indoor environments with diverse objects and challenging\nsettings with lots of glass, and (ii) parses the USD using Google's Gemini to\ndemonstrate scene understanding, complex inferences, and planning. We also\nstudy different aspects of this system in simulated warehouse and hospital\nsettings using Nvidia's Issac Sim. Code is available at\nhttps://github.com/grasp-lyrl/Real2USD .", "authors": ["Christopher D. Hsu", "Pratik Chaudhari"], "categories": ["cs.RO"], "published": "2025-10-12T19:43:10Z", "pdf": "https://arxiv.org/pdf/2510.10778v1", "abs": "https://arxiv.org/abs/2510.10778v1", "comment": "8 pages, 10 figures, 1 table"}
{"id": "2510.10765v1", "title": "EGD-YOLO: A Lightweight Multimodal Framework for Robust Drone-Bird Discrimination via Ghost-Enhanced YOLOv8n and EMA Attention under Adverse Condition", "summary": "Identifying drones and birds correctly is essential for keeping the skies\nsafe and improving security systems. Using the VIP CUP 2025 dataset, which\nprovides both RGB and infrared (IR) images, this study presents EGD-YOLOv8n, a\nnew lightweight yet powerful model for object detection. The model improves how\nimage features are captured and understood, making detection more accurate and\nefficient. It uses smart design changes and attention layers to focus on\nimportant details while reducing the amount of computation needed. A special\ndetection head helps the model adapt to objects of different shapes and sizes.\nWe trained three versions: one using RGB images, one using IR images, and one\ncombining both. The combined model achieved the best accuracy and reliability\nwhile running fast enough for real-time use on common GPUs.", "authors": ["Sudipto Sarkar", "Mohammad Asif Hasan", "Khondokar Ashik Shahriar", "Fablia Labiba", "Nahian Tasnim", "Sheikh Anawarul Haq Fattah"], "categories": ["cs.CV"], "published": "2025-10-12T19:05:16Z", "pdf": "https://arxiv.org/pdf/2510.10765v1", "abs": "https://arxiv.org/abs/2510.10765v1", "comment": null}
{"id": "2510.10759v1", "title": "Gain Tuning Is Not What You Need: Reward Gain Adaptation for Constrained Locomotion Learning", "summary": "Existing robot locomotion learning techniques rely heavily on the offline\nselection of proper reward weighting gains and cannot guarantee constraint\nsatisfaction (i.e., constraint violation) during training. Thus, this work aims\nto address both issues by proposing Reward-Oriented Gains via Embodied\nRegulation (ROGER), which adapts reward-weighting gains online based on\npenalties received throughout the embodied interaction process. The ratio\nbetween the positive reward (primary reward) and negative reward (penalty)\ngains is automatically reduced as the learning approaches the constraint\nthresholds to avoid violation. Conversely, the ratio is increased when learning\nis in safe states to prioritize performance. With a 60-kg quadruped robot,\nROGER achieved near-zero constraint violation throughout multiple learning\ntrials. It also achieved up to 50% more primary reward than the equivalent\nstate-of-the-art techniques. In MuJoCo continuous locomotion benchmarks,\nincluding a single-leg hopper, ROGER exhibited comparable or up to 100% higher\nperformance and 60% less torque usage and orientation deviation compared to\nthose trained with the default reward function. Finally, real-world locomotion\nlearning of a physical quadruped robot was achieved from scratch within one\nhour without any falls. Therefore, this work contributes to\nconstraint-satisfying real-world continual robot locomotion learning and\nsimplifies reward weighting gain tuning, potentially facilitating the\ndevelopment of physical robots and those that learn in the real world.", "authors": ["Arthicha Srisuchinnawong", "Poramate Manoonpong"], "categories": ["cs.RO"], "published": "2025-10-12T18:58:59Z", "pdf": "https://arxiv.org/pdf/2510.10759v1", "abs": "https://arxiv.org/abs/2510.10759v1", "comment": "RSS 2025"}
{"id": "2510.10731v1", "title": "Controllable Generative Trajectory Prediction via Weak Preference Alignment", "summary": "Deep generative models such as conditional variational autoencoders (CVAEs)\nhave shown great promise for predicting trajectories of surrounding agents in\nautonomous vehicle planning. State-of-the-art models have achieved remarkable\naccuracy in such prediction tasks. Besides accuracy, diversity is also crucial\nfor safe planning because human behaviors are inherently uncertain and\nmultimodal. However, existing methods generally lack a scheme to generate\ncontrollably diverse trajectories, which is arguably more useful than randomly\ndiversified trajectories, to the end of safe planning. To address this, we\npropose PrefCVAE, an augmented CVAE framework that uses weakly labeled\npreference pairs to imbue latent variables with semantic attributes. Using\naverage velocity as an example attribute, we demonstrate that PrefCVAE enables\ncontrollable, semantically meaningful predictions without degrading baseline\naccuracy. Our results show the effectiveness of preference supervision as a\ncost-effective way to enhance sampling-based generative models.", "authors": ["Yongxi Cao", "Julian F. Schumann", "Jens Kober", "Joni Pajarinen", "Arkady Zgonnikov"], "categories": ["cs.RO", "cs.LG"], "published": "2025-10-12T18:06:39Z", "pdf": "https://arxiv.org/pdf/2510.10731v1", "abs": "https://arxiv.org/abs/2510.10731v1", "comment": null}
{"id": "2510.10716v1", "title": "Deployment and Development of a Cognitive Teleoreactive Framework for Deep Sea Autonomy", "summary": "A new AUV mission planning and execution software has been tested on AUV\nSentry. Dubbed DINOS-R, it draws inspiration from cognitive architectures and\nAUV control systems to replace the legacy MC architecture. Unlike these\nexisting architectures, however, DINOS-R is built from the ground-up to unify\nsymbolic decision making (for understandable, repeatable, provable behavior)\nwith machine learning techniques and reactive behaviors, for field-readiness\nacross oceanographic platforms. Implemented primarily in Python3, DINOS-R is\nextensible, modular, and reusable, with an emphasis on non-expert use as well\nas growth for future research in oceanography and robot algorithms. Mission\nspecification is flexible, and can be specified declaratively. Behavior\nspecification is similarly flexible, supporting simultaneous use of real-time\ntask planning and hard-coded user specified plans. These features were\ndemonstrated in the field on Sentry, in addition to a variety of simulated\ncases. These results are discussed, and future work is outlined.", "authors": ["Christopher Thierauf"], "categories": ["cs.RO"], "published": "2025-10-12T17:37:32Z", "pdf": "https://arxiv.org/pdf/2510.10716v1", "abs": "https://arxiv.org/abs/2510.10716v1", "comment": null}
{"id": "2510.10708v1", "title": "Multimodal axion emissions from Abelian-Higgs cosmic strings", "summary": "We show that axions can be produced from Abelian-Higgs cosmic strings due to\nthe axion-gauge coupling. The strong magnetic field is confined in the string,\nand the electric field is induced around the moving string, allowing axion\nproductions from the dynamics of cosmic strings. Our numerical analysis on the\nstring collision shows that a sizable number of axions can be produced at the\nreconnection, and further emissions occur from moving kinks afterward.\nFurthermore, the simulation on the string network shows multimodal axion\nemissions in the sense that axions are produced in both the low-energy and\nhigh-energy regimes. The former can contribute to the cold dark matter and the\nlatter can be regarded as dark radiation. We found that the axion with sub-GeV\nmass can explain the current relic dark matter abundance and simultaneously\npredicts a sizable amount of dark radiation which can be probed by future\nobservations.", "authors": ["Naoya Kitajima", "Michiru Uwabo-Niibo"], "categories": ["hep-ph", "astro-ph.CO", "hep-th"], "published": "2025-10-12T17:16:33Z", "pdf": "https://arxiv.org/pdf/2510.10708v1", "abs": "https://arxiv.org/abs/2510.10708v1", "comment": "6 pages, 5 figures"}
{"id": "2510.10693v1", "title": "High-Dimensional Learning Dynamics of Quantized Models with Straight-Through Estimator", "summary": "Quantized neural network training optimizes a discrete, non-differentiable\nobjective. The straight-through estimator (STE) enables backpropagation through\nsurrogate gradients and is widely used. While previous studies have primarily\nfocused on the properties of surrogate gradients and their convergence, the\ninfluence of quantization hyperparameters, such as bit width and quantization\nrange, on learning dynamics remains largely unexplored. We theoretically show\nthat in the high-dimensional limit, STE dynamics converge to a deterministic\nordinary differential equation. This reveals that STE training exhibits a\nplateau followed by a sharp drop in generalization error, with plateau length\ndepending on the quantization range. A fixed-point analysis quantifies the\nasymptotic deviation from the unquantized linear model. We also extend\nanalytical techniques for stochastic gradient descent to nonlinear\ntransformations of weights and inputs.", "authors": ["Yuma Ichikawa", "Shuhei Kashiwamura", "Ayaka Sakata"], "categories": ["stat.ML", "cond-mat.dis-nn", "cs.AI", "cs.LG", "math.ST", "stat.TH"], "published": "2025-10-12T16:43:46Z", "pdf": "https://arxiv.org/pdf/2510.10693v1", "abs": "https://arxiv.org/abs/2510.10693v1", "comment": "27 pages, 14 figures"}
{"id": "2510.10689v1", "title": "OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs", "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nsubstantial potential in video understanding. However, existing benchmarks fail\nto comprehensively evaluate synergistic reasoning capabilities across audio and\nvisual modalities, often neglecting either one of the modalities or integrating\nthem in a logically inconsistent manner. To bridge this gap, we introduce\nOmniVideoBench, a large-scale and rigorously designed benchmark dedicated to\nassessing synergistic audio-visual understanding, with a strong emphasis on\nmodality complementarity and logical consistency. Specifically, OmniVideoBench\ncomprises 1000 high-quality question-answer(QA) pairs, each annotated with\nstep-by-step reasoning traces, derived from 628 diverse videos ranging from\nseveral seconds to 30 minutes, and manually verified to guarantee complete\ncorrectness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully\ndesigned question types, covering temporal reasoning, spatial localization,\ncounting, causal inference, summarization, and beyond, thereby capturing the\nessential challenges of video understanding. Evaluation of multiple MLLMs on\nOmniVideoBench reveals a pronounced gap between model performance and human\nreasoning, with open-source models lagging significantly behind their\nclosed-source counterparts, underscoring the inherent difficulty of genuine\naudio-visual reasoning. We will release OmniVideoBench to foster the\ndevelopment of MLLMs with stronger and more generalizable reasoning\ncapabilities.", "authors": ["Caorui Li", "Yu Chen", "Yiyan Ji", "Jin Xu", "Zhenyu Cui", "Shihao Li", "Yuanxing Zhang", "Jiafu Tang", "Zhenghao Song", "Dingling Zhang", "Ying He", "Haoxiang Liu", "Yuxuan Wang", "Qiufeng Wang", "Zhenhe Wu", "Jiehui Luo", "Zhiyu Pan", "Weihao Xie", "Chenchen Zhang", "Zhaohui Wang", "Jiayi Tian", "Yanghai Wang", "Zhe Cao", "Minxin Dai", "Ke Wang", "Runzhe Wen", "Yinghao Ma", "Yaning Pan", "Sungkyun Chang", "Termeh Taheri", "Haiwen Xia", "Christos Plachouras", "Emmanouil Benetos", "Yizhi Li", "Ge Zhang", "Jian Yang", "Tianhao Peng", "Zili Wang", "Minghao Liu", "Junran Peng", "Zhaoxiang Zhang", "Jiaheng Liu"], "categories": ["cs.AI"], "published": "2025-10-12T16:34:00Z", "pdf": "https://arxiv.org/pdf/2510.10689v1", "abs": "https://arxiv.org/abs/2510.10689v1", "comment": null}
{"id": "2510.10686v1", "title": "A Bioinspired Aquatic Machine Mimicking Water Caltrop", "summary": "Plants are increasingly becoming a source of inspiration for robotics and\nengineers to develop bioinspired, adaptive, and multifunctional machines. In\nthis study, we propose a bioinspired aquatic machine that mimics the fruit of\nthe water caltrop (Trapa natans L.). Among various plant species, T. natans\nproduces unique woody fruits that can disperse passively via water currents or\nby clinging to boats or waterfowls. Inspired by the structures and dispersal\nmechanisms of T. natans, we designed miniaturized biomimetic machines capable\nof passive dispersion in aquatic ecosystems. In order to study our selected\nbiological model, we collected natural fresh and dried mature samples of T.\nnatans fruits. We designed biomimetic aquatic machines by extracting the main\ngeometrical details from the natural samples, and by exploiting advanced\nthree-dimensional reconstruction techniques, including x-ray micro-computed\ntopography (Micro-CT). Then, we successfully fabricate the biomimetic machines\nat high-resolution in two configurations (hollow body and solid body) using\nlight-based bioprinting of photo-responsive hydrogels. We also characterized\nthe mechanical properties of the bioprinted materials through compression\ntests. Finally, we evaluated the floating behavior of the biomimetic machines\nin a flow chamber as a proof of concept. This biomimetic approach enhances the\nadaptability of the machine in aquatic environments, offering new design\ninsights for underwater, soft, and microrobotics.", "authors": ["Yuanquan Liu", "Thomas Speck", "Isabella Fiorello"], "categories": ["physics.app-ph"], "published": "2025-10-12T16:29:49Z", "pdf": "https://arxiv.org/pdf/2510.10686v1", "abs": "https://arxiv.org/abs/2510.10686v1", "comment": "Accepted in Lecture Notes in Computer Science, Springer Nature"}
{"id": "2510.10676v1", "title": "Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation", "summary": "This paper introduces Bhasha-Rupantarika, a light and efficient multilingual\ntranslation system tailored through algorithm-hardware codesign for\nresource-limited settings. The method investigates model deployment at\nsub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental\nresults indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in\ninference speed, which correlates with an increased throughput of 66 tokens/s\n(improvement by 4.8x). This underscores the importance of ultra-low precision\nquantization for real-time deployment in IoT devices using FPGA accelerators,\nachieving performance on par with expectations. Our evaluation covers\nbidirectional translation between Indian and international languages,\nshowcasing its adaptability in low-resource linguistic contexts. The FPGA\ndeployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,\nresulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x\nenhancement compared to HPTA. Overall, the evaluation provides a viable\nsolution based on quantisation-aware translation along with hardware efficiency\nsuitable for deployable multilingual AI systems. The entire codes\n[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for\nreproducibility are publicly available, facilitating rapid integration and\nfurther development by researchers.", "authors": ["Mukul Lokhande", "Tanushree Dewangan", "Mohd Sharik Mansoori", "Tejas Chaudhari", "Akarsh J.", "Damayanti Lokhande", "Adam Teman", "Santosh Kumar Vishvakarma"], "categories": ["cs.AR", "cs.CL", "cs.RO", "eess.AS"], "published": "2025-10-12T16:04:11Z", "pdf": "https://arxiv.org/pdf/2510.10676v1", "abs": "https://arxiv.org/abs/2510.10676v1", "comment": null}
{"id": "2510.10671v1", "title": "Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey", "summary": "Image-Language Foundation Models (ILFM) have demonstrated remarkable success\nin image-text understanding/generation tasks, providing transferable multimodal\nrepresentations that generalize across diverse downstream image-based tasks.\nThe advancement of video-text research has spurred growing interest in\nextending image-based models to the video domain. This paradigm, known as\nimage-to-video transfer learning, succeeds in alleviating the substantial data\nand computational requirements associated with training video-language\nfoundation models from scratch for video-text learning. This survey provides\nthe first comprehensive review of this emerging field, which begins by\nsummarizing the widely used ILFM and their capabilities. We then systematically\nclassify existing image-to-video transfer learning strategies into two\ncategories: frozen features and modified features, depending on whether the\noriginal representations from ILFM are preserved or undergo modifications.\nBuilding upon the task-specific nature of image-to-video transfer, this survey\nmethodically elaborates these strategies and details their applications across\na spectrum of video-text learning tasks, ranging from fine-grained (e.g.,\nspatio-temporal video grounding) to coarse-grained (e.g., video question\nanswering). We further present a detailed experimental analysis to investigate\nthe efficacy of different image-to-video transfer learning paradigms on a range\nof downstream video understanding tasks. Finally, we identify prevailing\nchallenges and highlight promising directions for future research. By offering\na comprehensive and structured overview, this survey aims to establish a\nstructured roadmap for advancing video-text learning based on existing ILFM,\nand to inspire future research directions in this rapidly evolving domain.", "authors": ["Jinxuan Li", "Chaolei Tan", "Haoxuan Chen", "Jianxin Ma", "Jian-Fang Hu", "Wei-Shi Zheng", "Jianhuang Lai"], "categories": ["cs.CV", "cs.AI"], "published": "2025-10-12T15:56:02Z", "pdf": "https://arxiv.org/pdf/2510.10671v1", "abs": "https://arxiv.org/abs/2510.10671v1", "comment": "Draft version, work in progress"}
{"id": "2510.10652v1", "title": "Shifted twisted Yangians and affine Grassmannian islices", "summary": "Associated to all quasi-split Satake diagrams of type ADE and even spherical\ncoweights $\\mu$, we introduce the shifted twisted Yangians ${}^\\imath Y_\\mu$\nand establish their PBW bases. We construct the iGKLO representations of\n${}^\\imath Y_\\mu$, which factor through quotients known as truncated shifted\ntwisted Yangians (TSTY) ${}^\\imath Y_\\mu^\\lambda$. In type AI with $\\mu$\ndominant, a variant of ${}^\\imath Y_\\mu^{N\\varpi_1^\\vee}$ is identified with\nthe TSTY in another definition which are isomorphic to finite W-algebras of\ntype BCD. We show that ${}^\\imath Y_\\mu$ quantizes the involutive fixed point\nlocus ${}^\\imath W_\\mu$ arising from affine Grassmannians of type ADE, and\nexpect that ${}^\\imath Y_\\mu^\\lambda$ quantizes a top-dimensional component of\nthe affine Grassmannian islice ${}^\\imath{\\bar{W}}_\\mu^\\lambda$. We identify\nthe islices ${}^\\imath{\\bar{W}}_\\mu^\\lambda$ in type AI with suitable nilpotent\nSlodowy slices of type BCD, building on the work of Lusztig and\nMirkovi\\'c-Vybornov in type A. We propose a framework for producing\northo-symplectic (and hybrid) Coulomb branches from split (and nonsplit) Satake\nframed double quivers, which are conjectured to provide a normalization of the\nislices ${}^\\imath{\\bar{W}}_\\mu^\\lambda$.", "authors": ["Kang Lu", "Weiqiang Wang", "Alex Weekes"], "categories": ["math.RT", "math.AG", "math.QA"], "published": "2025-10-12T15:17:55Z", "pdf": "https://arxiv.org/pdf/2510.10652v1", "abs": "https://arxiv.org/abs/2510.10652v1", "comment": "v1, 91 pages, 10 tables"}
{"id": "2510.10642v1", "title": "UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning", "summary": "Building generalist robot policies that can handle diverse tasks in\nopen-ended environments is a central challenge in robotics. To leverage\nknowledge from large-scale pretraining, prior work has typically built\ngeneralist policies either on top of vision-language understanding models\n(VLMs) or generative models. However, both semantic understanding from\nvision-language pretraining and visual dynamics modeling from visual-generation\npretraining are crucial for embodied robots. Recent unified models of\ngeneration and understanding have demonstrated strong capabilities in both\ncomprehension and generation through large-scale pretraining. We posit that\nrobotic policy learning can likewise benefit from the combined strengths of\nunderstanding, planning and continuous future representation learning. Building\non this insight, we introduce UniCoD, which acquires the ability to dynamically\nmodel high-dimensional visual features through pretraining on over 1M\ninternet-scale instructional manipulation videos. Subsequently, UniCoD is\nfine-tuned on data collected from the robot embodiment, enabling the learning\nof mappings from predictive representations to action tokens. Extensive\nexperiments show our approach consistently outperforms baseline methods in\nterms of 9\\% and 12\\% across simulation environments and real-world\nout-of-distribution tasks.", "authors": ["Jianke Zhang", "Yucheng Hu", "Yanjiang Guo", "Xiaoyu Chen", "Yichen Liu", "Wenna Chen", "Chaochao Lu", "Jianyu Chen"], "categories": ["cs.RO", "cs.AI"], "published": "2025-10-12T14:54:19Z", "pdf": "https://arxiv.org/pdf/2510.10642v1", "abs": "https://arxiv.org/abs/2510.10642v1", "comment": null}
{"id": "2510.10637v1", "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting", "summary": "The scalability of robotic learning is fundamentally bottlenecked by the\nsignificant cost and labor of real-world data collection. While simulated data\noffers a scalable alternative, it often fails to generalize to the real world\ndue to significant gaps in visual appearance, physical properties, and object\ninteractions. To address this, we propose RoboSimGS, a novel Real2Sim2Real\nframework that converts multi-view real-world images into scalable,\nhigh-fidelity, and physically interactive simulation environments for robotic\nmanipulation. Our approach reconstructs scenes using a hybrid representation:\n3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the\nenvironment, while mesh primitives for interactive objects ensure accurate\nphysics simulation. Crucially, we pioneer the use of a Multi-modal Large\nLanguage Model (MLLM) to automate the creation of physically plausible,\narticulated assets. The MLLM analyzes visual data to infer not only physical\nproperties (e.g., density, stiffness) but also complex kinematic structures\n(e.g., hinges, sliding rails) of objects. We demonstrate that policies trained\nentirely on data generated by RoboSimGS achieve successful zero-shot\nsim-to-real transfer across a diverse set of real-world manipulation tasks.\nFurthermore, data from RoboSimGS significantly enhances the performance and\ngeneralization capabilities of SOTA methods. Our results validate RoboSimGS as\na powerful and scalable solution for bridging the sim-to-real gap.", "authors": ["Haoyu Zhao", "Cheng Zeng", "Linghao Zhuang", "Yaxi Zhao", "Shengke Xue", "Hao Wang", "Xingyue Zhao", "Zhongyu Li", "Kehan Li", "Siteng Huang", "Mingxiu Chen", "Xin Li", "Deli Zhao", "Hua Zou"], "categories": ["cs.RO"], "published": "2025-10-12T14:42:07Z", "pdf": "https://arxiv.org/pdf/2510.10637v1", "abs": "https://arxiv.org/abs/2510.10637v1", "comment": "13 pages, 6 figures"}
{"id": "2510.10633v1", "title": "Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion", "summary": "Multimodal text-to-image generation remains constrained by the difficulty of\nmaintaining semantic alignment and professional-level detail across diverse\nvisual domains. We propose a multi-agent reinforcement learning framework that\ncoordinates domain-specialized agents (e.g., focused on architecture,\nportraiture, and landscape imagery) within two coupled subsystems: a text\nenhancement module and an image generation module, each augmented with\nmultimodal integration components. Agents are trained using Proximal Policy\nOptimization (PPO) under a composite reward function that balances semantic\nsimilarity, linguistic visual quality, and content diversity. Cross-modal\nalignment is enforced through contrastive learning, bidirectional attention,\nand iterative feedback between text and image. Across six experimental\nsettings, our system significantly enriches generated content (word count\nincreased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion\nmethods, Transformer-based strategies achieve the highest composite score\n(0.521), despite occasional stability issues. Multimodal ensembles yield\nmoderate consistency (ranging from 0.444 to 0.481), reflecting the persistent\nchallenges of cross-modal semantic grounding. These findings underscore the\npromise of collaborative, specialization-driven architectures for advancing\nreliable multimodal generative systems.", "authors": ["Jiabao Shi", "Minfeng Qi", "Lefeng Zhang", "Di Wang", "Yingjie Zhao", "Ziying Li", "Yalong Xing", "Ningran Li"], "categories": ["cs.AI"], "published": "2025-10-12T14:29:32Z", "pdf": "https://arxiv.org/pdf/2510.10633v1", "abs": "https://arxiv.org/abs/2510.10633v1", "comment": "16 pages, 13 figures"}
{"id": "2510.10623v1", "title": "ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration", "summary": "Transformers are at the core of modern AI nowadays. They rely heavily on\nmatrix multiplication and require efficient acceleration due to their\nsubstantial memory and computational requirements. Quantization plays a vital\nrole in reducing memory usage, and can be exploited for computations by\ndesigning reconfigurable architectures that enhance matrix multiplication by\ndynamically adjusting the precision. This paper proposes ADiP, a novel\nadaptive-precision systolic array architecture designed for efficient matrix\nmultiplication acceleration.The proposed architecture consists of NxN\nadaptive-precision processing elements (PEs) and shared accumulators. ADiP\nsupports multiple computation modes, including symmetric single-matrix\nmultiplication as well as asymmetric multi-matrix multiplication with a shared\ninput matrix, thereby improving data-reuse and PE utilization. In addition,\nADiP maximizes the computational density by adapting to different precisions,\nsuch as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed\nfor ADiP architecture, including latency and throughput for versatile\narchitecture configurations. A comprehensive hardware design space exploration\nis demonstrated using 22nm commercial technology, achieving up to a 4x higher\ncomputational throughput. Furthermore, ADiP is evaluated on different\ntransformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,\ndelivering latency improvement up to 53.6%, and energy improvement up to 24.4%\nfor BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a\npeak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,\n8bitx4bit, and 8bitx2bit operations, respectively.", "authors": ["Ahmed J. Abdelmaksoud", "Cristian Sestito", "Shiwei Wang", "Themis Prodromakis"], "categories": ["cs.AR"], "published": "2025-10-12T14:03:22Z", "pdf": "https://arxiv.org/pdf/2510.10623v1", "abs": "https://arxiv.org/abs/2510.10623v1", "comment": null}
{"id": "2510.10618v1", "title": "Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization", "summary": "Post-training compression has been a widely employed approach to scale down\nlarge language model (LLM) and facilitate efficient inference. In various\nproposed compression methods, including pruning and quantization, calibration\ndata plays a vital role by informing the weight importance and activation\ndynamic ranges. However, how calibration data impacts the LLM capability after\ncompression is less explored. Few of the existing works, though recognizing the\nsignificance of this study, only investigate the language modeling or\ncommonsense reasoning performance degradation from limited angles, like the\ndata sources or sample amounts. More systematic research is still needed to\nexamine the impacts on different LLM capabilities in terms of compositional\nproperties and domain correspondence of calibration data. In this work, we aim\nat bridging this gap and further analyze underlying influencing mechanisms from\nthe activation pattern perspective. Especially, we explore the calibration\ndata's impacts on high-level complex reasoning capabilities, like math problem\nsolving and code generation. Delving into the underlying mechanism, we find\nthat the representativeness and diversity in activation space more\nfundamentally determine the quality of calibration data. Finally, we propose a\ncalibration data curation framework based on such observations and analysis,\nenhancing the performance of existing post-training compression methods on\npreserving critical LLM capabilities. Our code is provided in\n\\href{https://github.com/BokwaiHo/COLA.git}{Link}.", "authors": ["Bowei He", "Lihao Yin", "Huiling Zhen", "Shuqi Liu", "Han Wu", "Xiaokun Zhang", "Mingxuan Yuan", "Chen Ma"], "categories": ["cs.CL"], "published": "2025-10-12T14:00:23Z", "pdf": "https://arxiv.org/pdf/2510.10618v1", "abs": "https://arxiv.org/abs/2510.10618v1", "comment": "Accepted by NeurIPS 2025"}
{"id": "2510.10606v1", "title": "ViSurf: Visual Supervised-and-Reinforcement Fine-Tuning for Large Vision-and-Language Models", "summary": "Typical post-training paradigms for Large Vision-and-Language Models (LVLMs)\ninclude Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable\nRewards (RLVR). SFT leverages external guidance to inject new knowledge,\nwhereas RLVR utilizes internal reinforcement to enhance reasoning capabilities\nand overall performance. However, our analysis reveals that SFT often leads to\nsub-optimal performance, while RLVR struggles with tasks that exceed the\nmodel's internal knowledge base. To address these limitations, we propose\nViSurf (\\textbf{Vi}sual \\textbf{Su}pervised-and-\\textbf{R}einforcement\n\\textbf{F}ine-Tuning), a unified post-training paradigm that integrates the\nstrengths of both SFT and RLVR within a single stage. We analyze the derivation\nof the SFT and RLVR objectives to establish the ViSurf objective, providing a\nunified perspective on these two paradigms. The core of ViSurf involves\ninjecting ground-truth labels into the RLVR rollouts, thereby providing\nsimultaneous external supervision and internal reinforcement. Furthermore, we\nintroduce three novel reward control strategies to stabilize and optimize the\ntraining process. Extensive experiments across several diverse benchmarks\ndemonstrate the effectiveness of ViSurf, outperforming both individual SFT,\nRLVR, and two-stage SFT \\textrightarrow RLVR. In-depth analysis corroborates\nthese findings, validating the derivation and design principles of ViSurf.", "authors": ["Yuqi Liu", "Liangyu Chen", "Jiazhen Liu", "Mingkang Zhu", "Zhisheng Zhong", "Bei Yu", "Jiaya Jia"], "categories": ["cs.CV"], "published": "2025-10-12T13:42:55Z", "pdf": "https://arxiv.org/pdf/2510.10606v1", "abs": "https://arxiv.org/abs/2510.10606v1", "comment": null}
{"id": "2510.10602v1", "title": "SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike Streams", "summary": "Most robotic grasping systems rely on converting sensor data into explicit 3D\npoint clouds, which is a computational step not found in biological\nintelligence. This paper explores a fundamentally different, neuro-inspired\nparadigm for 6-DoF grasp detection. We introduce SpikeGrasp, a framework that\nmimics the biological visuomotor pathway, processing raw, asynchronous events\nfrom stereo spike cameras, similarly to retinas, to directly infer grasp poses.\nOur model fuses these stereo spike streams and uses a recurrent spiking neural\nnetwork, analogous to high-level visual processing, to iteratively refine grasp\nhypotheses without ever reconstructing a point cloud. To validate this\napproach, we built a large-scale synthetic benchmark dataset. Experiments show\nthat SpikeGrasp surpasses traditional point-cloud-based baselines, especially\nin cluttered and textureless scenes, and demonstrates remarkable data\nefficiency. By establishing the viability of this end-to-end, neuro-inspired\napproach, SpikeGrasp paves the way for future systems capable of the fluid and\nefficient manipulation seen in nature, particularly for dynamic objects.", "authors": ["Zhuoheng Gao", "Jiyao Zhang", "Zhiyong Xie", "Hao Dong", "Zhaofei Yu", "Rongmei Chen", "Guozhang Chen", "Tiejun Huang"], "categories": ["cs.RO", "cs.CV"], "published": "2025-10-12T13:36:40Z", "pdf": "https://arxiv.org/pdf/2510.10602v1", "abs": "https://arxiv.org/abs/2510.10602v1", "comment": null}
{"id": "2510.10597v1", "title": "Fast Vision in the Dark: A Case for Single-Photon Imaging in Planetary Navigation", "summary": "Improving robotic navigation is critical for extending exploration range and\nenhancing operational efficiency. Vision-based navigation relying on\ntraditional CCD or CMOS cameras faces major challenges when complex\nillumination conditions are paired with motion, limiting the range and\naccessibility of mobile planetary robots. In this study, we propose a novel\napproach to planetary navigation that leverages the unique imaging capabilities\nof Single-Photon Avalanche Diode (SPAD) cameras. We present the first\ncomprehensive evaluation of single-photon imaging as an alternative passive\nsensing technology for robotic exploration missions targeting perceptually\nchallenging locations, with a special emphasis on high-latitude lunar regions.\nWe detail the operating principles and performance characteristics of SPAD\ncameras, assess their advantages and limitations in addressing key perception\nchallenges of upcoming exploration missions to the Moon, and benchmark their\nperformance under representative illumination conditions.", "authors": ["David Rodríguez-Martínez", "C. J. Pérez del Pulgar"], "categories": ["cs.RO"], "published": "2025-10-12T13:29:57Z", "pdf": "https://arxiv.org/pdf/2510.10597v1", "abs": "https://arxiv.org/abs/2510.10597v1", "comment": "9 pages, 6 figures, conference paper"}
{"id": "2510.10560v1", "title": "BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices", "summary": "Cross-attention transformers and other multimodal vision-language models\nexcel at grounding and generation; however, their extensive, full-precision\nbackbones make it challenging to deploy them on edge devices. Memory-augmented\narchitectures enhance the utilization of past context; however, most works\nrarely pair them with aggressive edge-oriented quantization. We introduce\nBitMar, a quantized multimodal transformer that proposes an external human-like\nepisodic memory for effective image-text generation on hardware with limited\nresources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and\none for vision (DiNOv2-based), to create compact embeddings that are combined\nand used to query a fixed-size key-value episodic memory. During vector\nretrieval, the BitNet decoder applies per-layer conditioning, which increases\nthe contextual relevance of generated content. The decoder also employs\nattention sinks with a sliding-window mechanism to process long or streaming\ninputs under tight memory budgets. The combination of per-layer conditioning\nand sliding-window attention achieves a strong quality-speed trade-off,\ndelivering competitive captioning and multimodal understanding at low latency\nwith a small model footprint. These characteristics make BitMar well-suited for\nedge deployment.", "authors": ["Euhid Aman", "Esteban Carlin", "Hsing-Kuo Pao", "Giovanni Beltrame", "Ghaluh Indah Permata Sari", "Yie-Tarng Chen"], "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7"], "published": "2025-10-12T11:59:41Z", "pdf": "https://arxiv.org/pdf/2510.10560v1", "abs": "https://arxiv.org/abs/2510.10560v1", "comment": "6 pages, BabyLM Workshop, EMNLP 2025"}
{"id": "2510.10546v1", "title": "GLOFNet -- A Multimodal Dataset for GLOF Monitoring and Prediction", "summary": "Glacial Lake Outburst Floods (GLOFs) are rare but destructive hazards in high\nmountain regions, yet predictive research is hindered by fragmented and\nunimodal data. Most prior efforts emphasize post-event mapping, whereas\nforecasting requires harmonized datasets that combine visual indicators with\nphysical precursors. We present GLOFNet, a multimodal dataset for GLOF\nmonitoring and prediction, focused on the Shisper Glacier in the Karakoram. It\nintegrates three complementary sources: Sentinel-2 multispectral imagery for\nspatial monitoring, NASA ITS_LIVE velocity products for glacier kinematics, and\nMODIS Land Surface Temperature records spanning over two decades. Preprocessing\nincluded cloud masking, quality filtering, normalization, temporal\ninterpolation, augmentation, and cyclical encoding, followed by harmonization\nacross modalities. Exploratory analysis reveals seasonal glacier velocity\ncycles, long-term warming of ~0.8 K per decade, and spatial heterogeneity in\ncryospheric conditions. The resulting dataset, GLOFNet, is publicly available\nto support future research in glacial hazard prediction. By addressing\nchallenges such as class imbalance, cloud contamination, and coarse resolution,\nGLOFNet provides a structured foundation for benchmarking multimodal deep\nlearning approaches to rare hazard prediction.", "authors": ["Zuha Fatima", "Muhammad Anser Sohaib", "Muhammad Talha", "Sidra Sultana", "Ayesha Kanwal", "Nazia Perwaiz"], "categories": ["cs.CV", "cs.AI"], "published": "2025-10-12T11:03:47Z", "pdf": "https://arxiv.org/pdf/2510.10546v1", "abs": "https://arxiv.org/abs/2510.10546v1", "comment": null}
{"id": "2510.10533v1", "title": "Layout-Independent License Plate Recognition via Integrated Vision and Language Models", "summary": "This work presents a pattern-aware framework for automatic license plate\nrecognition (ALPR), designed to operate reliably across diverse plate layouts\nand challenging real-world conditions. The proposed system consists of a\nmodern, high-precision detection network followed by a recognition stage that\nintegrates a transformer-based vision model with an iterative language\nmodelling mechanism. This unified recognition stage performs character\nidentification and post-OCR refinement in a seamless process, learning the\nstructural patterns and formatting rules specific to license plates without\nrelying on explicit heuristic corrections or manual layout classification.\nThrough this design, the system jointly optimizes visual and linguistic cues,\nenables iterative refinement to improve OCR accuracy under noise, distortion,\nand unconventional fonts, and achieves layout-independent recognition across\nmultiple international datasets (IR-LPR, UFPR-ALPR, AOLP). Experimental results\ndemonstrate superior accuracy and robustness compared to recent\nsegmentation-free approaches, highlighting how embedding pattern analysis\nwithin the recognition stage bridges computer vision and language modelling for\nenhanced adaptability in intelligent transportation and surveillance\napplications.", "authors": ["Elham Shabaninia", "Fatemeh Asadi-zeydabadi", "Hossein Nezamabadi-pour"], "categories": ["cs.CV"], "published": "2025-10-12T10:25:21Z", "pdf": "https://arxiv.org/pdf/2510.10533v1", "abs": "https://arxiv.org/abs/2510.10533v1", "comment": null}
{"id": "2510.10522v1", "title": "Receptive Field Expanded Look-Up Tables for Vision Inference: Advancing from Low-level to High-level Tasks", "summary": "Recently, several look-up table (LUT) methods were developed to greatly\nexpedite the inference of CNNs in a classical strategy of trading space for\nspeed. However, these LUT methods suffer from a common drawback of limited\nreceptive field of the convolution kernels due to the combinatorial explosion\nof table size. This research aims to expand the CNN receptive field with a\nfixed table size, thereby enhancing the performance of LUT-driven fast CNN\ninference while maintaining the same space complexity. To achieve this goal,\nvarious techniques are proposed. The main contribution is a novel approach of\nlearning an optimal lattice vector quantizer that adaptively allocates the\nquantization resolution across data dimensions based on their significance to\nthe inference task. In addition, the lattice vector quantizer offers an\ninherently more accurate approximation of CNN kernels than scalar quantizer as\nused in current practice. Furthermore, we introduce other receptive field\nexpansion strategies, including irregular dilated convolutions and a U-shaped\ncascaded LUT structure, designed to capture multi-level contextual information\nwithout inflating table size. Together, these innovations allow our approach to\neffectively balance speed, accuracy, and memory efficiency, demonstrating\nsignificant improvements over existing LUT methods.", "authors": ["Xi Zhang", "Xiaolin Wu"], "categories": ["cs.CV"], "published": "2025-10-12T09:44:28Z", "pdf": "https://arxiv.org/pdf/2510.10522v1", "abs": "https://arxiv.org/abs/2510.10522v1", "comment": null}
{"id": "2510.10518v2", "title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning", "summary": "Recent advancements in multimodal reward models (RMs) have substantially\nimproved post-training for visual generative models. However, current RMs face\ninherent limitations: (1) visual inputs consume large context budgets, forcing\nfewer frames and causing loss of fine-grained details; and (2) all visual\ninformation is packed into the initial prompt, exacerbating hallucination and\nforgetting during chain-of-thought reasoning. To overcome these issues, we\nintroduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework\nthat equips the RM with visual reasoning operations (e.g., select frame) and a\nconfigurable visual memory window. This allows the RM to actively acquire and\nupdate visual evidence within context limits, improving reasoning fidelity and\nreliability. We activate visual reasoning via a reinforcement fine-tuning\npipeline: (i) Cold Start with curated visual chain-of-thought data to distill\nbasic reasoning skills and operation formatting; (ii) select samples whose\nper-dimension and overall judgments are all correct, then conduct Rejection\nsampling Fine-Tuning on these high-quality traces to further enhance reasoning;\nand (iii) apply Group Relative Policy Optimization (GRPO) to strengthen\nreasoning. Our approach delivers state-of-the-art accuracy among open-source\nmodels on video preference benchmarks, especially for longer videos: a 7B\nVR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6%\non MJ-Bench-Video. These results validate the effectiveness and promise of\nthinking-with-image multimodal reward modeling.", "authors": ["Qunzhong Wang", "Jie Liu", "Jiajun Liang", "Yilei Jiang", "Yuanxing Zhang", "Jinyuan Chen", "Yaozhi Zheng", "Xintao Wang", "Pengfei Wan", "Xiangyu Yue", "Jiaheng Liu"], "categories": ["cs.CV"], "published": "2025-10-12T09:29:50Z", "pdf": "https://arxiv.org/pdf/2510.10518v2", "abs": "https://arxiv.org/abs/2510.10518v2", "comment": null}
{"id": "2510.10516v1", "title": "Population-Coded Spiking Neural Networks for High-Dimensional Robotic Control", "summary": "Energy-efficient and high-performance motor control remains a critical\nchallenge in robotics, particularly for high-dimensional continuous control\ntasks with limited onboard resources. While Deep Reinforcement Learning (DRL)\nhas achieved remarkable results, its computational demands and energy\nconsumption limit deployment in resource-constrained environments. This paper\nintroduces a novel framework combining population-coded Spiking Neural Networks\n(SNNs) with DRL to address these challenges. Our approach leverages the\nevent-driven, asynchronous computation of SNNs alongside the robust policy\noptimization capabilities of DRL, achieving a balance between energy efficiency\nand control performance. Central to this framework is the Population-coded\nSpiking Actor Network (PopSAN), which encodes high-dimensional observations\ninto neuronal population activities and enables optimal policy learning through\ngradient-based updates. We evaluate our method on the Isaac Gym platform using\nthe PixMC benchmark with complex robotic manipulation tasks. Experimental\nresults on the Franka robotic arm demonstrate that our approach achieves energy\nsavings of up to 96.10% compared to traditional Artificial Neural Networks\n(ANNs) while maintaining comparable control performance. The trained SNN\npolicies exhibit robust finger position tracking with minimal deviation from\ncommanded trajectories and stable target height maintenance during\npick-and-place operations. These results position population-coded SNNs as a\npromising solution for energy-efficient, high-performance robotic control in\nresource-constrained applications, paving the way for scalable deployment in\nreal-world robotics systems.", "authors": ["Kanishkha Jaisankar", "Xiaoyang Jiang", "Feifan Liao", "Jeethu Sreenivas Amuthan"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-10-12T09:27:25Z", "pdf": "https://arxiv.org/pdf/2510.10516v1", "abs": "https://arxiv.org/abs/2510.10516v1", "comment": null}
{"id": "2510.10509v1", "title": "MARS-Sep: Multimodal-Aligned Reinforced Sound Separation", "summary": "Universal sound separation faces a fundamental misalignment: models optimized\nfor low-level signal metrics often produce semantically contaminated outputs,\nfailing to suppress perceptually salient interference from acoustically similar\nsources. To bridge this gap, we introduce MARS-Sep, a reinforcement learning\nframework that reformulates separation as decision making. Instead of simply\nregressing ground-truth masks, MARS-Sep learns a factorized Beta mask policy\nthat is optimized by a clipped trust-region surrogate with entropy\nregularization and group-relative advantage normalization. Concretely, we\nsample masks from a frozen old policy, reconstruct waveforms, and update the\ncurrent policy using clipped importance ratios-yielding substantially more\nstable and sample-efficient learning. Multimodal rewards, derived from an\naudio-text-vision encoder, directly incentivize semantic consistency with query\nprompts. We further propose a progressive alignment scheme to fine-tune this\nencoder, boosting its cross-modal discriminability and improving reward\nfaithfulness. Extensive experiments on multiple benchmarks demonstrate\nconsistent gains in Text-, Audio-, and Image-Queried separation, with notable\nimprovements in signal metrics and semantic quality. Our code is available at\nhttps://anonymous.4open.science/r/MARS-Sep. Sound separation samples are\navailable at https://mars-sep.github.io/.", "authors": ["Zihan Zhang", "Xize Cheng", "Zhennan Jiang", "Dongjie Fu", "Jingyuan Chen", "Zhou Zhao", "Tao Jin"], "categories": ["cs.SD", "cs.AI"], "published": "2025-10-12T09:05:28Z", "pdf": "https://arxiv.org/pdf/2510.10509v1", "abs": "https://arxiv.org/abs/2510.10509v1", "comment": null}
{"id": "2510.10506v1", "title": "SuperEx: Enhancing Indoor Mapping and Exploration using Non-Line-of-Sight Perception", "summary": "Efficient exploration and mapping in unknown indoor environments is a\nfundamental challenge, with high stakes in time-critical settings. In current\nsystems, robot perception remains confined to line-of-sight; occluded regions\nremain unknown until physically traversed, leading to inefficient exploration\nwhen layouts deviate from prior assumptions. In this work, we bring\nnon-line-of-sight (NLOS) sensing to robotic exploration. We leverage\nsingle-photon LiDARs, which capture time-of-flight histograms that encode the\npresence of hidden objects - allowing robots to look around blind corners.\nRecent single-photon LiDARs have become practical and portable, enabling\ndeployment beyond controlled lab settings. Prior NLOS works target 3D\nreconstruction in static, lab-based scenarios, and initial efforts toward\nNLOS-aided navigation consider simplified geometries. We introduce SuperEx, a\nframework that integrates NLOS sensing directly into the mapping-exploration\nloop. SuperEx augments global map prediction with beyond-line-of-sight cues by\n(i) carving empty NLOS regions from timing histograms and (ii) reconstructing\noccupied structure via a two-step physics-based and data-driven approach that\nleverages structural regularities. Evaluations on complex simulated maps and\nthe real-world KTH Floorplan dataset show a 12% gain in mapping accuracy under\n< 30% coverage and improved exploration efficiency compared to line-of-sight\nbaselines, opening a path to reliable mapping beyond direct visibility.", "authors": ["Kush Garg", "Akshat Dave"], "categories": ["cs.RO", "cs.CV"], "published": "2025-10-12T08:52:20Z", "pdf": "https://arxiv.org/pdf/2510.10506v1", "abs": "https://arxiv.org/abs/2510.10506v1", "comment": "8 pages, 9 Figures , Project webpage: https://super-ex.github.io/"}
{"id": "2510.10499v1", "title": "Preserving Core Structures of Social Networks via Information Guided Multi-Step Graph Pruning", "summary": "Social networks often contain dense and overlapping connections that obscure\ntheir essential interaction patterns, making analysis and interpretation\nchallenging. Identifying the structural backbone of such networks is crucial\nfor understanding community organization, information flow, and functional\nrelationships. This study introduces a multi-step network pruning framework\nthat leverages principles from information theory to balance structural\ncomplexity and task-relevant information. The framework iteratively evaluates\nand removes edges from the graph based on their contribution to task-relevant\nmutual information, producing a trajectory of network simplification that\npreserves most of the inherent semantics. Motivated by gradient boosting, we\npropose IGPrune, which enables efficient, differentiable optimization to\nprogressively uncover semantically meaningful connections. Extensive\nexperiments on social and biological networks show that IGPrune retains\ncritical structural and functional patterns. Beyond quantitative performance,\nthe pruned networks reveal interpretable backbones, highlighting the method's\npotential to support scientific discovery and actionable insights in real-world\nnetworks.", "authors": ["Yutong Hu", "Bingxin Zhou", "Jing Wang", "Weishu Zhao", "Liang Hong"], "categories": ["cs.SI", "cs.IT", "math.IT"], "published": "2025-10-12T08:38:36Z", "pdf": "https://arxiv.org/pdf/2510.10499v1", "abs": "https://arxiv.org/abs/2510.10499v1", "comment": null}
{"id": "2510.10486v1", "title": "SASER: Stego attacks on open-source LLMs", "summary": "Open-source large language models (LLMs) have demonstrated considerable\ndominance over proprietary LLMs in resolving neural processing tasks, thanks to\nthe collaborative and sharing nature. Although full access to source codes,\nmodel parameters, and training data lays the groundwork for transparency, we\nargue that such a full-access manner is vulnerable to stego attacks, and their\nill-effects are not fully understood. In this paper, we conduct a systematic\nformalization for stego attacks on open-source LLMs by enumerating all possible\nthreat models associated with adversary objectives, knowledge, and\ncapabilities. Therein, the threat posed by adversaries with internal knowledge,\nwho inject payloads and triggers during the model sharing phase, is of\npractical interest. We go even further and propose the first stego attack on\nopen-source LLMs, dubbed SASER, which wields impacts through identifying\ntargeted parameters, embedding payloads, injecting triggers, and executing\npayloads sequentially. Particularly, SASER enhances the attack robustness\nagainst quantization-based local deployment by de-quantizing the embedded\npayloads. In addition, to achieve stealthiness, SASER devises the\nperformance-aware importance metric to identify targeted parameters with the\nleast degradation of model performance. Extensive experiments on LlaMA2-7B and\nChatGLM3-6B, without quantization, show that the stealth rate of SASER\noutperforms existing stego attacks (for general DNNs) by up to 98.1%, while\nachieving the same attack success rate (ASR) of 100%. More importantly, SASER\nimproves ASR on quantized models from 0 to 100% in all settings. We appeal for\ninvestigations on countermeasures against SASER in view of the significant\nattack effectiveness.", "authors": ["Ming Tan", "Wei Li", "Hu Tao", "Hailong Ma", "Aodi Liu", "Qian Chen", "Zilong Wang"], "categories": ["cs.CR", "cs.AI"], "published": "2025-10-12T07:33:56Z", "pdf": "https://arxiv.org/pdf/2510.10486v1", "abs": "https://arxiv.org/abs/2510.10486v1", "comment": null}
{"id": "2510.10468v1", "title": "Galilean Symmetry in Robotics", "summary": "Galilean symmetry is the natural symmetry of inertial motion that underpins\nNewtonian physics. Although rigid-body symmetry is one of the most established\nand fundamental tools in robotics, there appears to be no comparable treatment\nof Galilean symmetry for a robotics audience. In this paper, we present a\nrobotics-tailored exposition of Galilean symmetry that leverages the\ncommunity's familiarity with and understanding of rigid-body transformations\nand pose representations. Our approach contrasts with common treatments in the\nphysics literature that introduce Galilean symmetry as a stepping stone to\nEinstein's relativity. A key insight is that the Galilean matrix Lie group can\nbe used to describe two different pose representations, Galilean frames, that\nuse inertial velocity in the state definition, and extended poses, that use\ncoordinate velocity. We provide three examples where applying the Galilean\nmatrix Lie-group algebra to robotics problems is straightforward and yields\nsignificant insights: inertial navigation above the rotating Earth, manipulator\nkinematics, and sensor data fusion under temporal uncertainty. We believe that\nthe time is right for the robotics community to benefit from rediscovering and\nextending this classical material and applying it to modern problems.", "authors": ["Robert Mahony", "Jonathan Kelly", "Stephan Weiss"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "published": "2025-10-12T06:24:03Z", "pdf": "https://arxiv.org/pdf/2510.10468v1", "abs": "https://arxiv.org/abs/2510.10468v1", "comment": "Under Review"}
{"id": "2510.10467v1", "title": "AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs", "summary": "The deployment of large language models (LLMs) is increasingly constrained by\nmemory and latency bottlenecks, motivating the need for quantization techniques\nthat flexibly balance accuracy and efficiency. Recent work has introduced\nmulti-precision models, which enable inference at multiple precisions within a\nsingle model depending on runtime constraints. To support such flexibility,\nquantized weights are often stored as bit-planes, where hardware efficiency\nimproves when the compute operates directly at the bit-plane level and\nactivates only the precision required by each request. In this work, we present\nAnyBCQ, a hardware-friendly multi-precision extension of Binary-Coded\nQuantization (BCQ) that supports direct bit-plane operations. By representing\nweights as binary bit-planes with corresponding scale factors, AnyBCQ enables\nbit-plane-level computation and maps naturally to accelerator-friendly,\nbit-parallel arithmetic. Our progressive precision expansion mechanism\nincrementally refines scaling factors while reusing previously assigned binary\ncodes, yielding monotonic improvements in accuracy as additional bits are\nenabled. We further co-design a specialized kernel that exploits the BCQ\nstructure to support dynamic per-request precision selection with negligible\noverhead. Experiments on recent LLMs demonstrate that AnyBCQ significantly\nnarrows the accuracy drop in the low-bit regime (e.g. 2-bit), remains\ncompetitive at higher precision, and achieves throughput gains of up to 3.0x\nover half precision and 1.2x over state-of-the-art multi-precision methods. By\naligning algorithmic flexibility with hardware efficiency, AnyBCQ provides a\npractical foundation for multi-precision LLM deployment across diverse\nservice-level objectives.", "authors": ["Gunho Park", "Jeongin Bae", "Beomseok Kwon", "Byeongwook Kim", "Se Jung Kwon", "Dongsoo Lee"], "categories": ["cs.LG", "cs.AI"], "published": "2025-10-12T06:20:38Z", "pdf": "https://arxiv.org/pdf/2510.10467v1", "abs": "https://arxiv.org/abs/2510.10467v1", "comment": null}
{"id": "2510.10466v1", "title": "When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance", "summary": "Vision-Language Models (VLMs) have shown solid ability for multimodal\nunderstanding of both visual and language contexts. However, existing VLMs\noften face severe challenges of hallucinations, meaning that VLMs tend to\ngenerate responses that are only fluent in the language but irrelevant to\nimages in previous contexts. To address this issue, we analyze how language\nbias contributes to hallucinations and then introduce Cross-Modal\nGuidance(CMG), a training-free decoding method that addresses the\nhallucinations by leveraging the difference between the output distributions of\nthe original model and the one with degraded visual-language attention. In\npractice, we adaptively mask the attention weight of the most influential image\ntokens in selected transformer layers to corrupt the visual-language perception\nas a concrete type of degradation. Such a degradation-induced decoding\nemphasizes the perception of visual contexts and therefore significantly\nreduces language bias without harming the ability of VLMs. In experiment\nsections, we conduct comprehensive studies. All results demonstrate the\nsuperior advantages of CMG with neither additional conditions nor training\ncosts. We also quantitatively show CMG can improve different VLM's performance\non hallucination-specific benchmarks and generalize effectively.", "authors": ["Jinjin Cao", "Zhiyang Chen", "Zijun Wang", "Liyuan Ma", "Weijian Luo", "Guojun Qi"], "categories": ["cs.CV"], "published": "2025-10-12T06:17:13Z", "pdf": "https://arxiv.org/pdf/2510.10466v1", "abs": "https://arxiv.org/abs/2510.10466v1", "comment": null}
{"id": "2510.10464v1", "title": "Post-TIPS Prediction via Multimodal Interaction: A Multi-Center Dataset and Framework for Survival, Complication, and Portal Pressure Assessment", "summary": "Transjugular intrahepatic portosystemic shunt (TIPS) is an established\nprocedure for portal hypertension, but provides variable survival outcomes and\nfrequent overt hepatic encephalopathy (OHE), indicating the necessity of\naccurate preoperative prognostic modeling. Current studies typically build\nmachine learning models from preoperative CT images or clinical\ncharacteristics, but face three key challenges: (1) labor-intensive\nregion-of-interest (ROI) annotation, (2) poor reliability and generalizability\nof unimodal methods, and (3) incomplete assessment from single-endpoint\nprediction. Moreover, the lack of publicly accessible datasets constrains\nresearch in this field. Therefore, we present MultiTIPS, the first public\nmulti-center dataset for TIPS prognosis, and propose a novel multimodal\nprognostic framework based on it. The framework comprises three core modules:\n(1) dual-option segmentation, which integrates semi-supervised and foundation\nmodel-based pipelines to achieve robust ROI segmentation with limited\nannotations and facilitate subsequent feature extraction; (2) multimodal\ninteraction, where three techniques, multi-grained radiomics attention (MGRA),\nprogressive orthogonal disentanglement (POD), and clinically guided prognostic\nenhancement (CGPE), are introduced to enable cross-modal feature interaction\nand complementary representation integration, thus improving model accuracy and\nrobustness; and (3) multi-task prediction, where a staged training strategy is\nused to perform stable optimization of survival, portal pressure gradient\n(PPG), and OHE prediction for comprehensive prognostic assessment. Extensive\nexperiments on MultiTIPS demonstrate the superiority of the proposed method\nover state-of-the-art approaches, along with strong cross-domain generalization\nand interpretability, indicating its promise for clinical application. The\ndataset and code are available.", "authors": ["Junhao Dong", "Dejia Liu", "Ruiqi Ding", "Zongxing Chen", "Yingjie Huang", "Zhu Meng", "Jianbo Zhao", "Zhicheng Zhao", "Fei Su"], "categories": ["cs.CV"], "published": "2025-10-12T06:11:59Z", "pdf": "https://arxiv.org/pdf/2510.10464v1", "abs": "https://arxiv.org/abs/2510.10464v1", "comment": "81 pages, 13 figures"}
{"id": "2510.10455v1", "title": "Towards Dynamic Quadrupedal Gaits: A Symmetry-Guided RL Hierarchy Enables Free Gait Transitions at Varying Speeds", "summary": "Quadrupedal robots exhibit a wide range of viable gaits, but generating\nspecific footfall sequences often requires laborious expert tuning of numerous\nvariables, such as touch-down and lift-off events and holonomic constraints for\neach leg. This paper presents a unified reinforcement learning framework for\ngenerating versatile quadrupedal gaits by leveraging the intrinsic symmetries\nand velocity-period relationship of dynamic legged systems. We propose a\nsymmetry-guided reward function design that incorporates temporal,\nmorphological, and time-reversal symmetries. By focusing on preserved\nsymmetries and natural dynamics, our approach eliminates the need for\npredefined trajectories, enabling smooth transitions between diverse locomotion\npatterns such as trotting, bounding, half-bounding, and galloping. Implemented\non the Unitree Go2 robot, our method demonstrates robust performance across a\nrange of speeds in both simulations and hardware tests, significantly improving\ngait adaptability without extensive reward tuning or explicit foot placement\ncontrol. This work provides insights into dynamic locomotion strategies and\nunderscores the crucial role of symmetries in robotic gait design.", "authors": ["Jiayu Ding", "Xulin Chen", "Garrett E. Katz", "Zhenyu Gan"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "published": "2025-10-12T05:25:49Z", "pdf": "https://arxiv.org/pdf/2510.10455v1", "abs": "https://arxiv.org/abs/2510.10455v1", "comment": null}
{"id": "2510.10451v1", "title": "Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning", "summary": "Simulators of animal movements play a valuable role in studying behavior.\nAdvances in imitation learning for robotics have expanded possibilities for\nreproducing human and animal movements. A key challenge for realistic\nmulti-animal simulation in biology is bridging the gap between unknown\nreal-world transition models and their simulated counterparts. Because\nlocomotion dynamics are seldom known, relying solely on mathematical models is\ninsufficient; constructing a simulator that both reproduces real trajectories\nand supports reward-driven optimization remains an open problem. We introduce a\ndata-driven simulator for multi-animal behavior based on deep reinforcement\nlearning and counterfactual simulation. We address the ill-posed nature of the\nproblem caused by high degrees of freedom in locomotion by estimating movement\nvariables of an incomplete transition model as actions within an RL framework.\nWe also employ a distance-based pseudo-reward to align and compare states\nbetween cyber and physical spaces. Validated on artificial agents, flies,\nnewts, and silkmoth, our approach achieves higher reproducibility of\nspecies-specific behaviors and improved reward acquisition compared with\nstandard imitation and RL methods. Moreover, it enables counterfactual behavior\nprediction in novel experimental settings and supports multi-individual\nmodeling for flexible what-if trajectory generation, suggesting its potential\nto simulate and elucidate complex multi-animal behaviors.", "authors": ["Keisuke Fujii", "Kazushi Tsutsui", "Yu Teshima", "Makoto Itoh", "Naoya Takeishi", "Nozomi Nishiumi", "Ryoya Tanaka", "Shunsuke Shigaki", "Yoshinobu Kawahara"], "categories": ["cs.LG", "cs.AI"], "published": "2025-10-12T05:08:26Z", "pdf": "https://arxiv.org/pdf/2510.10451v1", "abs": "https://arxiv.org/abs/2510.10451v1", "comment": "21 pages, 7 figures"}
{"id": "2510.10444v1", "title": "Do Audio LLMs Really LISTEN, or Just Transcribe? Measuring Lexical vs. Acoustic Emotion Cues Reliance", "summary": "Understanding emotion from speech requires sensitivity to both lexical and\nacoustic cues. However, it remains unclear whether large audio language models\n(LALMs) genuinely process acoustic information or rely primarily on lexical\ncontent. We present LISTEN (Lexical vs. Acoustic Speech Test for Emotion in\nNarratives), a controlled benchmark designed to disentangle lexical reliance\nfrom acoustic sensitivity in emotion understanding. Across evaluations of six\nstate-of-the-art LALMs, we observe a consistent lexical dominance. Models\npredict \"neutral\" when lexical cues are neutral or absent, show limited gains\nunder cue alignment, and fail to classify distinct emotions under cue conflict.\nIn paralinguistic settings, performance approaches chance. These results\nindicate that current LALMs largely \"transcribe\" rather than \"listen,\" relying\nheavily on lexical semantics while underutilizing acoustic cues. LISTEN offers\na principled framework for assessing emotion understanding in multimodal\nmodels.", "authors": ["Jingyi Chen", "Zhimeng Guo", "Jiyun Chun", "Pichao Wang", "Andrew Perrault", "Micha Elsner"], "categories": ["cs.CL", "cs.AI"], "published": "2025-10-12T04:31:15Z", "pdf": "https://arxiv.org/pdf/2510.10444v1", "abs": "https://arxiv.org/abs/2510.10444v1", "comment": null}
{"id": "2510.10434v1", "title": "MonoSE(3)-Diffusion: A Monocular SE(3) Diffusion Framework for Robust Camera-to-Robot Pose Estimation", "summary": "We propose MonoSE(3)-Diffusion, a monocular SE(3) diffusion framework that\nformulates markerless, image-based robot pose estimation as a conditional\ndenoising diffusion process. The framework consists of two processes: a\nvisibility-constrained diffusion process for diverse pose augmentation and a\ntimestep-aware reverse process for progressive pose refinement. The diffusion\nprocess progressively perturbs ground-truth poses to noisy transformations for\ntraining a pose denoising network. Importantly, we integrate visibility\nconstraints into the process, ensuring the transformations remain within the\ncamera field of view. Compared to the fixed-scale perturbations used in current\nmethods, the diffusion process generates in-view and diverse training poses,\nthereby improving the network generalization capability. Furthermore, the\nreverse process iteratively predicts the poses by the denoising network and\nrefines pose estimates by sampling from the diffusion posterior of current\ntimestep, following a scheduled coarse-to-fine procedure. Moreover, the\ntimestep indicates the transformation scales, which guide the denoising network\nto achieve more accurate pose predictions. The reverse process demonstrates\nhigher robustness than direct prediction, benefiting from its timestep-aware\nrefinement scheme. Our approach demonstrates improvements across two benchmarks\n(DREAM and RoboKeyGen), achieving a notable AUC of 66.75 on the most\nchallenging dataset, representing a 32.3% gain over the state-of-the-art.", "authors": ["Kangjian Zhu", "Haobo Jiang", "Yigong Zhang", "Jianjun Qian", "Jian Yang", "Jin Xie"], "categories": ["cs.CV", "cs.RO"], "published": "2025-10-12T03:57:30Z", "pdf": "https://arxiv.org/pdf/2510.10434v1", "abs": "https://arxiv.org/abs/2510.10434v1", "comment": null}
{"id": "2510.10426v1", "title": "Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs", "summary": "Multimodal large language models (MLLMs) often fail in fine-grained visual\nquestion answering, producing hallucinations about object identities,\npositions, and relations because textual queries are not explicitly anchored to\nvisual referents. Retrieval-augmented generation (RAG) alleviates some errors,\nbut it fails to align with human-like processing at both the retrieval and\naugmentation levels. Specifically, it focuses only on global-level image\ninformation but lacks local detail and limits reasoning about fine-grained\ninteractions. To overcome this limitation, we present Human-Like\nRetrieval-Augmented Generation (HuLiRAG), a framework that stages multimodal\nreasoning as a ``what--where--reweight'' cascade. Queries are first anchored to\ncandidate referents via open-vocabulary detection (what), then spatially\nresolved with SAM-derived masks to recover fine-grained precision (where), and\nadaptively prioritized through the trade-off between local and global alignment\n(reweight). Mask-guided fine-tuning further injects spatial evidence into the\ngeneration process, transforming grounding from a passive bias into an explicit\nconstraint on answer formulation. Extensive experiments demonstrate that this\nhuman-like cascade improves grounding fidelity and factual consistency while\nreducing hallucinations, advancing multimodal question answering toward\ntrustworthy reasoning.", "authors": ["Suyang Xi", "Chenxi Yang", "Hong Ding", "Yiqing Ni", "Catherine C. Liu", "Yunhao Liu", "Chengqi Zhang"], "categories": ["cs.CV", "cs.AI"], "published": "2025-10-12T03:22:33Z", "pdf": "https://arxiv.org/pdf/2510.10426v1", "abs": "https://arxiv.org/abs/2510.10426v1", "comment": "12 pages, 5 figures"}
{"id": "2510.10422v1", "title": "Towards Cybersickness Severity Classification from VR Gameplay Videos Using Transfer Learning and Temporal Modeling", "summary": "With the rapid advancement of virtual reality (VR) technology, its adoption\nacross domains such as healthcare, education, and entertainment has grown\nsignificantly. However, the persistent issue of cybersickness, marked by\nsymptoms resembling motion sickness, continues to hinder widespread acceptance\nof VR. While recent research has explored multimodal deep learning approaches\nleveraging data from integrated VR sensors like eye and head tracking, there\nremains limited investigation into the use of video-based features for\npredicting cybersickness. In this study, we address this gap by utilizing\ntransfer learning to extract high-level visual features from VR gameplay videos\nusing the InceptionV3 model pretrained on the ImageNet dataset. These features\nare then passed to a Long Short-Term Memory (LSTM) network to capture the\ntemporal dynamics of the VR experience and predict cybersickness severity over\ntime. Our approach effectively leverages the time-series nature of video data,\nachieving a 68.4% classification accuracy for cybersickness severity. This\nsurpasses the performance of existing models trained solely on video data,\nproviding a practical tool for VR developers to evaluate and mitigate\ncybersickness in virtual environments. Furthermore, this work lays the\nfoundation for future research on video-based temporal modeling for enhancing\nuser comfort in VR applications.", "authors": ["Jyotirmay Nag Setu", "Kevin Desai", "John Quarles"], "categories": ["cs.CV"], "published": "2025-10-12T03:10:05Z", "pdf": "https://arxiv.org/pdf/2510.10422v1", "abs": "https://arxiv.org/abs/2510.10422v1", "comment": null}
{"id": "2510.10421v1", "title": "Hierarchical Planning for Long-Horizon Multi-Target Tracking Under Target Motion Uncertainty", "summary": "Achieving persistent tracking of multiple dynamic targets over a large\nspatial area poses significant challenges for a single-robot system with\nconstrained sensing capabilities. As the robot moves to track different\ntargets, the ones outside the field of view accumulate uncertainty, making them\nprogressively harder to track. An effective path planning algorithm must manage\nuncertainty over a long horizon and account for the risk of permanently losing\ntrack of targets that remain unseen for too long. However, most existing\napproaches rely on short planning horizons and assume small, bounded\nenvironments, resulting in poor tracking performance and target loss in\nlarge-scale scenarios. In this paper, we present a hierarchical planner for\ntracking multiple moving targets with an aerial vehicle. To address the\nchallenge of tracking non-static targets, our method incorporates motion models\nand uncertainty propagation during path execution, allowing for more informed\ndecision-making. We decompose the multi-target tracking task into sub-tasks of\nsingle target search and detection, and our proposed pipeline consists a novel\nlow-level coverage planner that enables searching for a target in an evolving\nbelief area, and an estimation method to assess the likelihood of success for\neach sub-task, making it possible to convert the active target tracking task to\na Markov decision process (MDP) that we solve with a tree-based algorithm to\ndetermine the sequence of sub-tasks. We validate our approach in simulation,\ndemonstrating its effectiveness compared to existing planners for active target\ntracking tasks, and our proposed planner outperforms existing approaches,\nachieving a reduction of 11-70% in final uncertainty across different\nenvironments.", "authors": ["Junbin Yuan", "Brady Moon", "Muqing Cao", "Sebastian Scherer"], "categories": ["cs.RO"], "published": "2025-10-12T03:08:00Z", "pdf": "https://arxiv.org/pdf/2510.10421v1", "abs": "https://arxiv.org/abs/2510.10421v1", "comment": "8 pages, 7 figures. Accepted to IEEE Robotics and Automation Letters\n  (RAL), 2025"}
{"id": "2510.10396v2", "title": "MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations", "summary": "Humans rely on multisensory integration to perceive spatial environments,\nwhere auditory cues enable sound source localization in three-dimensional\nspace. Despite the critical role of spatial audio in immersive technologies\nsuch as VR/AR, most existing multimodal datasets provide only monaural audio,\nwhich limits the development of spatial audio generation and understanding. To\naddress these challenges, we introduce MRSAudio, a large-scale multimodal\nspatial audio dataset designed to advance research in spatial audio\nunderstanding and generation. MRSAudio spans four distinct components: MRSLife,\nMRSSpeech, MRSMusic, and MRSSing, covering diverse real-world scenarios. The\ndataset includes synchronized binaural and ambisonic audio, exocentric and\negocentric video, motion trajectories, and fine-grained annotations such as\ntranscripts, phoneme boundaries, lyrics, scores, and prompts. To demonstrate\nthe utility and versatility of MRSAudio, we establish five foundational tasks:\naudio spatialization, and spatial text to speech, spatial singing voice\nsynthesis, spatial music generation and sound event localization and detection.\nResults show that MRSAudio enables high-quality spatial modeling and supports a\nbroad range of spatial audio research. Demos and dataset access are available\nat https://mrsaudio.github.io.", "authors": ["Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Xintong Hu", "Yu Zhang", "Li Tang", "Rui Yang", "Han Wang", "Zongbao Zhang", "Yuhan Wang", "Yixuan Chen", "Hankun Xu", "Ke Xu", "Pengfei Fan", "Zhetao Chen", "Yanhao Yu", "Qiange Huang", "Fei Wu", "Zhou Zhao"], "categories": ["cs.SD"], "published": "2025-10-12T01:20:23Z", "pdf": "https://arxiv.org/pdf/2510.10396v2", "abs": "https://arxiv.org/abs/2510.10396v2", "comment": "24 pages"}
{"id": "2510.10392v1", "title": "MicroRoboScope: A Portable and Integrated Mechatronic Platform for Magnetic and Acoustic Microrobotic Experimentation", "summary": "This paper presents MicroRoboScope, a portable, compact, and versatile\nmicrorobotic experimentation platform designed for real-time, closed-loop\ncontrol of both magnetic and acoustic microrobots. The system integrates an\nembedded computer, microscope, power supplies, and control circuitry into a\nsingle, low-cost and fully integrated apparatus. Custom control software\ndeveloped in Python and Arduino C++ handles live video acquisition, microrobot\ntracking, and generation of control signals for electromagnetic coils and\nacoustic transducers. The platform's multi-modal actuation, accessibility, and\nportability make it suitable not only for specialized research laboratories but\nalso for educational and outreach settings. By lowering the barrier to entry\nfor microrobotic experimentation, this system enables new opportunities for\nresearch, education, and translational applications in biomedicine, tissue\nengineering, and robotics.", "authors": ["Max Sokolich", "Yanda Yang", "Subrahmanyam Cherukumilli", "Fatma Ceren Kirmizitas", "Sambeeta Das"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "published": "2025-10-12T01:04:19Z", "pdf": "https://arxiv.org/pdf/2510.10392v1", "abs": "https://arxiv.org/abs/2510.10392v1", "comment": null}
