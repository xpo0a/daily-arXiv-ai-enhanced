{"id": "2510.08572v1", "title": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation", "summary": "Scaling data and models has played a pivotal role in the remarkable progress\nof computer vision and language. Inspired by these domains, recent efforts in\nrobotics have similarly focused on scaling both data and model size to develop\nmore generalizable and robust policies. However, unlike vision and language,\nrobotics lacks access to internet-scale demonstrations across diverse robotic\ntasks and environments. As a result, the scale of existing datasets typically\nsuffers from the need for manual data collection and curation. To address this\nproblem, here we propose BLAZER, a framework that learns manipulation policies\nfrom automatically generated training data. We build on the zero-shot\ncapabilities of LLM planners and automatically generate demonstrations for\ndiverse manipulation tasks in simulation. Successful examples are then used to\nfinetune an LLM and to improve its planning capabilities without human\nsupervision. Notably, while BLAZER training requires access to the simulator's\nstate, we demonstrate direct transfer of acquired skills to sensor-based\nmanipulation. Through extensive experiments, we show BLAZER to significantly\nimprove zero-shot manipulation in both simulated and real environments.\nMoreover, BLAZER improves on tasks outside of its training pool and enables\ndownscaling of LLM models. Our code and data will be made publicly available on\nthe project page.", "authors": ["Rocktim Jyoti Das", "Harsh Singh", "Diana Turmakhan", "Muhammad Abdullah Sohail", "Mingfei Han", "Preslav Nakov", "Fabio Pizzati", "Ivan Laptev"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-10-09T17:59:58Z", "pdf": "https://arxiv.org/pdf/2510.08572v1", "abs": "https://arxiv.org/abs/2510.08572v1", "comment": "11 pages, 8 figures"}
{"id": "2510.08571v1", "title": "Scalable Offline Metrics for Autonomous Driving", "summary": "Real-World evaluation of perception-based planning models for robotic\nsystems, such as autonomous vehicles, can be safely and inexpensively conducted\noffline, i.e., by computing model prediction error over a pre-collected\nvalidation dataset with ground-truth annotations. However, extrapolating from\noffline model performance to online settings remains a challenge. In these\nsettings, seemingly minor errors can compound and result in test-time\ninfractions or collisions. This relationship is understudied, particularly\nacross diverse closed-loop metrics and complex urban maneuvers. In this work,\nwe revisit this undervalued question in policy evaluation through an extensive\nset of experiments across diverse conditions and metrics. Based on analysis in\nsimulation, we find an even worse correlation between offline and online\nsettings than reported by prior studies, casting doubts on the validity of\ncurrent evaluation practices and metrics for driving policies. Next, we bridge\nthe gap between offline and online evaluation. We investigate an offline metric\nbased on epistemic uncertainty, which aims to capture events that are likely to\ncause errors in closed-loop settings. The resulting metric achieves over 13%\nimprovement in correlation compared to previous offline metrics. We further\nvalidate the generalization of our findings beyond the simulation environment\nin real-world settings, where even greater gains are observed.", "authors": ["Animikh Aich", "Adwait Kulkarni", "Eshed Ohn-Bar"], "categories": ["cs.RO", "cs.CV"], "published": "2025-10-09T17:59:57Z", "pdf": "https://arxiv.org/pdf/2510.08571v1", "abs": "https://arxiv.org/abs/2510.08571v1", "comment": "Accepted at IROS 2025 (IEEE/RSJ International Conference on\n  Intelligent Robots and Systems)"}
{"id": "2510.08568v1", "title": "NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos", "summary": "Enabling robots to execute novel manipulation tasks zero-shot is a central\ngoal in robotics. Most existing methods assume in-distribution tasks or rely on\nfine-tuning with embodiment-matched data, limiting transfer across platforms.\nWe present NovaFlow, an autonomous manipulation framework that converts a task\ndescription into an actionable plan for a target robot without any\ndemonstrations. Given a task description, NovaFlow synthesizes a video using a\nvideo generation model and distills it into 3D actionable object flow using\noff-the-shelf perception modules. From the object flow, it computes relative\nposes for rigid objects and realizes them as robot actions via grasp proposals\nand trajectory optimization. For deformable objects, this flow serves as a\ntracking objective for model-based planning with a particle-based dynamics\nmodel. By decoupling task understanding from low-level control, NovaFlow\nnaturally transfers across embodiments. We validate on rigid, articulated, and\ndeformable object manipulation tasks using a table-top Franka arm and a Spot\nquadrupedal mobile robot, and achieve effective zero-shot execution without\ndemonstrations or embodiment-specific training. Project website:\nhttps://novaflow.lhy.xyz/.", "authors": ["Hongyu Li", "Lingfeng Sun", "Yafei Hu", "Duy Ta", "Jennifer Barry", "George Konidaris", "Jiahui Fu"], "categories": ["cs.RO", "cs.AI", "cs.CV"], "published": "2025-10-09T17:59:55Z", "pdf": "https://arxiv.org/pdf/2510.08568v1", "abs": "https://arxiv.org/abs/2510.08568v1", "comment": null}
{"id": "2510.08567v1", "title": "MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning", "summary": "Vision language models (VLMs) are increasingly deployed as controllers with\naccess to external tools for complex reasoning and decision-making, yet their\neffectiveness remains limited by the scarcity of high-quality multimodal\ntrajectories and the cost of manual annotation. We address this challenge with\na vision-centric agent tuning framework that automatically synthesizes\nmultimodal trajectories, generates step-wise preference pairs, and trains a VLM\ncontroller for robust tool-use reasoning. Our pipeline first constructs\nM-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified\ntrajectories, enabling imitation-based trajectory tuning. Building on this, we\ndevelop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool\nreasoning. To achieve finer alignment, we further introduce Pref-X, a set of\n11K automatically generated preference pairs, and optimize MATRIX on it via\nstep-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,\nMATRIX consistently surpasses both open- and closed-source VLMs, demonstrating\nscalable and effective multimodal tool use. Our data and code is avaliable at\nhttps://github.com/mbzuai-oryx/MATRIX.", "authors": ["Tajamul Ashraf", "Umair Nawaz", "Abdelrahman M. Shaker", "Rao Anwer", "Philip Torr", "Fahad Shahbaz Khan", "Salman Khan"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-10-09T17:59:54Z", "pdf": "https://arxiv.org/pdf/2510.08567v1", "abs": "https://arxiv.org/abs/2510.08567v1", "comment": null}
{"id": "2510.08564v1", "title": "How to Teach Large Multimodal Models New Skills", "summary": "How can we teach large multimodal models (LMMs) new skills without erasing\nprior abilities? We study sequential fine-tuning on five target skills while\nmonitoring general ability on eight held-out benchmarks across three model\nfamilies. We observe that apparent \"forgetting\" on held-out tasks after narrow\nfine-tuning can partly recover at later stages. We trace this behavior to a\nmeasurable shift in the output token distribution, manifested through a simple\ncounting-bias probe that co-varies with forgetting. Guided by this picture, we\nidentify two simple, robust tuning recipes that learn strongly while limiting\ndrift: (i) updating only the self-attention projection layers, and (ii)\nupdating only the MLP Gate&Up while freezing the Down projection. Across models\nand tasks, these choices deliver strong target gains while largely preserving\nheld-out performance. Code is available at\nhttps://github.com/jessemelpolio/LMM_CL", "authors": ["Zhen Zhu", "Yiming Gong", "Yao Xiao", "Yaoyao Liu", "Derek Hoiem"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "published": "2025-10-09T17:59:37Z", "pdf": "https://arxiv.org/pdf/2510.08564v1", "abs": "https://arxiv.org/abs/2510.08564v1", "comment": "In submission. Code is available at\n  https://github.com/jessemelpolio/LMM_CL"}
{"id": "2510.08565v1", "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints", "summary": "Compositional training has been the de-facto paradigm in existing Multimodal\nLarge Language Models (MLLMs), where pre-trained vision encoders are connected\nwith pre-trained LLMs through continuous multimodal pre-training. However, the\nmultimodal scaling property of this paradigm remains difficult to explore due\nto the separated training. In this paper, we focus on the native training of\nMLLMs in an end-to-end manner and systematically study its design space and\nscaling property under a practical setting, i.e., data constraint. Through\ncareful study of various choices in MLLM, we obtain the optimal\nmeta-architecture that best balances performance and training cost. After that,\nwe further explore the scaling properties of the native MLLM and indicate the\npositively correlated scaling relationship between visual encoders and LLMs.\nBased on these findings, we propose a native MLLM called NaViL, combined with a\nsimple and cost-effective recipe. Experimental results on 14 multimodal\nbenchmarks confirm the competitive performance of NaViL against existing MLLMs.\nBesides that, our findings and results provide in-depth insights for the future\nstudy of native MLLMs.", "authors": ["Changyao Tian", "Hao Li", "Gen Luo", "Xizhou Zhu", "Weijie Su", "Hanming Deng", "Jinguo Zhu", "Jie Shao", "Ziran Zhu", "Yunpeng Liu", "Lewei Lu", "Wenhai Wang", "Hongsheng Li", "Jifeng Dai"], "categories": ["cs.CV"], "published": "2025-10-09T17:59:37Z", "pdf": "https://arxiv.org/pdf/2510.08565v1", "abs": "https://arxiv.org/abs/2510.08565v1", "comment": "Accepted by NeurIPS 2025. 22 pages, link:\n  https://github.com/OpenGVLab/NaViL"}
{"id": "2510.08559v1", "title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models", "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress across\nvarious capabilities; however, complex video reasoning in the scientific domain\nremains a significant and challenging frontier. Current video benchmarks\npredominantly target general scenarios where perception/recognition is heavily\nrelied on, while with relatively simple reasoning tasks, leading to saturation\nand thus failing to effectively evaluate advanced multimodal cognitive skills.\nTo address this critical gap, we introduce SciVideoBench, a rigorous benchmark\nspecifically designed to assess advanced video reasoning in scientific\ncontexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice\nquestions derived from cutting-edge scientific experimental videos spanning\nover 25 specialized academic subjects and verified by a semi-automatic system.\nEach question demands sophisticated domain-specific knowledge, precise\nspatiotemporal perception, and intricate logical reasoning, effectively\nchallenging models' higher-order cognitive abilities. Our evaluation highlights\nsignificant performance deficits in state-of-the-art proprietary and\nopen-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating\nsubstantial room for advancement in video reasoning capabilities. Detailed\nanalyses of critical factors such as reasoning complexity and visual grounding\nprovide valuable insights and clear direction for future developments in LMMs,\ndriving the evolution of truly capable multimodal AI co-scientists. We hope\nSciVideoBench could fit the interests of the community and help to push the\nboundary of cutting-edge AI for border science.", "authors": ["Andong Deng", "Taojiannan Yang", "Shoubin Yu", "Lincoln Spencer", "Mohit Bansal", "Chen Chen", "Serena Yeung-Levy", "Xiaohan Wang"], "categories": ["cs.CV", "cs.AI"], "published": "2025-10-09T17:59:23Z", "pdf": "https://arxiv.org/pdf/2510.08559v1", "abs": "https://arxiv.org/abs/2510.08559v1", "comment": null}
{"id": "2510.08556v1", "title": "DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model", "summary": "Achieving generalized in-hand object rotation remains a significant challenge\nin robotics, largely due to the difficulty of transferring policies from\nsimulation to the real world. The complex, contact-rich dynamics of dexterous\nmanipulation create a \"reality gap\" that has limited prior work to constrained\nscenarios involving simple geometries, limited object sizes and aspect ratios,\nconstrained wrist poses, or customized hands. We address this sim-to-real\nchallenge with a novel framework that enables a single policy, trained in\nsimulation, to generalize to a wide variety of objects and conditions in the\nreal world. The core of our method is a joint-wise dynamics model that learns\nto bridge the reality gap by effectively fitting limited amount of real-world\ncollected data and then adapting the sim policy's actions accordingly. The\nmodel is highly data-efficient and generalizable across different whole-hand\ninteraction distributions by factorizing dynamics across joints, compressing\nsystem-wide influences into low-dimensional variables, and learning each\njoint's evolution from its own dynamic profile, implicitly capturing these net\neffects. We pair this with a fully autonomous data collection strategy that\ngathers diverse, real-world interaction data with minimal human intervention.\nOur complete pipeline demonstrates unprecedented generality: a single policy\nsuccessfully rotates challenging objects with complex shapes (e.g., animals),\nhigh aspect ratios (up to 5.33), and small sizes, all while handling diverse\nwrist orientations and rotation axes. Comprehensive real-world evaluations and\na teleoperation application for complex tasks validate the effectiveness and\nrobustness of our approach. Website: https://meowuu7.github.io/DexNDM/", "authors": ["Xueyi Liu", "He Wang", "Li Yi"], "categories": ["cs.RO", "cs.CV"], "published": "2025-10-09T17:59:11Z", "pdf": "https://arxiv.org/pdf/2510.08556v1", "abs": "https://arxiv.org/abs/2510.08556v1", "comment": "Project Website: https://meowuu7.github.io/DexNDM/ Video:\n  https://youtu.be/tU2Mv8vWftU"}
{"id": "2510.08553v1", "title": "Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation", "summary": "Vision-and-Language Navigation (VLN) requires agents to follow natural\nlanguage instructions through environments, with memory-persistent variants\ndemanding progressive improvement through accumulated experience. Existing\napproaches for memory-persistent VLN face critical limitations: they lack\neffective memory access mechanisms, instead relying on entire memory\nincorporation or fixed-horizon lookup, and predominantly store only\nenvironmental observations while neglecting navigation behavioral patterns that\nencode valuable decision-making strategies. We present Memoir, which employs\nimagination as a retrieval mechanism grounded by explicit memory: a world model\nimagines future navigation states as queries to selectively retrieve relevant\nenvironmental observations and behavioral histories. The approach comprises: 1)\na language-conditioned world model that imagines future states serving dual\npurposes: encoding experiences for storage and generating retrieval queries; 2)\nHybrid Viewpoint-Level Memory that anchors both observations and behavioral\npatterns to viewpoints, enabling hybrid retrieval; and 3) an\nexperience-augmented navigation model that integrates retrieved knowledge\nthrough specialized encoders. Extensive evaluation across diverse\nmemory-persistent VLN benchmarks with 10 distinctive testing scenarios\ndemonstrates Memoir's effectiveness: significant improvements across all\nscenarios, with 5.4% SPL gains on IR2R over the best memory-persistent\nbaseline, accompanied by 8.3x training speedup and 74% inference memory\nreduction. The results validate that predictive retrieval of both environmental\nand behavioral memories enables more effective navigation, with analysis\nindicating substantial headroom (73.3% vs 93.4% upper bound) for this\nimagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.", "authors": ["Yunzhe Xu", "Yiyuan Pan", "Zhe Liu"], "categories": ["cs.CV", "cs.AI", "cs.RO"], "published": "2025-10-09T17:58:01Z", "pdf": "https://arxiv.org/pdf/2510.08553v1", "abs": "https://arxiv.org/abs/2510.08553v1", "comment": "14 pages, 6 figures, 13 tables"}
{"id": "2510.08551v1", "title": "ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation", "summary": "On-the-fly 3D reconstruction from monocular image sequences is a\nlong-standing challenge in computer vision, critical for applications such as\nreal-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff:\nper-scene optimization yields high fidelity but is computationally expensive,\nwhereas feed-forward foundation models enable real-time inference but struggle\nwith accuracy and robustness. In this work, we propose ARTDECO, a unified\nframework that combines the efficiency of feed-forward models with the\nreliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose\nestimation and point prediction, coupled with a Gaussian decoder that\ntransforms multi-scale features into structured 3D Gaussians. To sustain both\nfidelity and efficiency at scale, we design a hierarchical Gaussian\nrepresentation with a LoD-aware rendering strategy, which improves rendering\nfidelity while reducing redundancy. Experiments on eight diverse indoor and\noutdoor benchmarks show that ARTDECO delivers interactive performance\ncomparable to SLAM, robustness similar to feed-forward systems, and\nreconstruction quality close to per-scene optimization, providing a practical\npath toward on-the-fly digitization of real-world environments with both\naccurate geometry and high visual fidelity. Explore more demos on our project\npage: https://city-super.github.io/artdeco/.", "authors": ["Guanghao Li", "Kerui Ren", "Linning Xu", "Zhewen Zheng", "Changjian Jiang", "Xin Gao", "Bo Dai", "Jian Pu", "Mulin Yu", "Jiangmiao Pang"], "categories": ["cs.CV"], "published": "2025-10-09T17:57:38Z", "pdf": "https://arxiv.org/pdf/2510.08551v1", "abs": "https://arxiv.org/abs/2510.08551v1", "comment": null}
{"id": "2510.08547v1", "title": "R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation", "summary": "Towards the aim of generalized robotic manipulation, spatial generalization\nis the most fundamental capability that requires the policy to work robustly\nunder different spatial distribution of objects, environment and agent itself.\nTo achieve this, substantial human demonstrations need to be collected to cover\ndifferent spatial configurations for training a generalized visuomotor policy\nvia imitation learning. Prior works explore a promising direction that\nleverages data generation to acquire abundant spatially diverse data from\nminimal source demonstrations. However, most approaches face significant\nsim-to-real gap and are often limited to constrained settings, such as\nfixed-base scenarios and predefined camera viewpoints. In this paper, we\npropose a real-to-real 3D data generation framework (R2RGen) that directly\naugments the pointcloud observation-action pairs to generate real-world data.\nR2RGen is simulator- and rendering-free, thus being efficient and\nplug-and-play. Specifically, given a single source demonstration, we introduce\nan annotation mechanism for fine-grained parsing of scene and trajectory. A\ngroup-wise augmentation strategy is proposed to handle complex multi-object\ncompositions and diverse task constraints. We further present camera-aware\nprocessing to align the distribution of generated data with real-world 3D\nsensor. Empirically, R2RGen substantially enhances data efficiency on extensive\nexperiments and demonstrates strong potential for scaling and application on\nmobile manipulation.", "authors": ["Xiuwei Xu", "Angyuan Ma", "Hankun Li", "Bingyao Yu", "Zheng Zhu", "Jie Zhou", "Jiwen Lu"], "categories": ["cs.RO", "cs.CV"], "published": "2025-10-09T17:55:44Z", "pdf": "https://arxiv.org/pdf/2510.08547v1", "abs": "https://arxiv.org/abs/2510.08547v1", "comment": "Project page: https://r2rgen.github.io/"}
{"id": "2510.08540v1", "title": "MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization", "summary": "While current Multimodal Large Language Models (MLLMs) have demonstrated\nproficiency in reasoning tasks such as mathematics and logic, their capacity\nfor long-chain reflective reasoning, a prerequisite for solving complex\nreal-world problems, remains largely underexplored. In this work, we first\nconduct an extensive empirical investigation to evaluate this capability.\nLeveraging a carefully designed data synthesis engine, we construct MM-HELIX, a\nmultimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks\nthat require iterative thinking and backtracking. Empirical results on this\nbenchmark reveal that existing MLLMs exhibit significant performance deficits\nin long-chain reflective reasoning. To address this limitation, we generate\npost-training data and further explore learning paradigms for exploiting such\ndata. We first develop the Step-Elicited Response Generation pipeline to create\nMM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning\ntraces for instruction-tuning stage. Given that standard Reinforcement Learning\nfails on complex tasks due to sparse reward signals and catastrophic forgetting\nafter Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization\n(AHPO), a novel training strategy that dynamically unifies offline supervision\nand online optimization into a single stage. This strategy enables the model to\nlearn from expert data when rewards are sparse and conduct independent\nexploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our\nmethod achieves a +18.6\\% accuracy improvement on MM-HELIX benchmark and\ndemonstrates strong generalization with a +5.7\\% average performance gain on\ngeneral mathematic and logic tasks. Our work demonstrate that reflective\nreasoning in MLLMs can be effectively learned and generalized, paving the way\nfor developing more capable MLLMs.", "authors": ["Xiangyu Zhao", "Junming Lin", "Tianhao Liang", "Yifan Zhou", "Wenhao Chai", "Yuzhe Gu", "Weiyun Wang", "Kai Chen", "Gen Luo", "Wenwei Zhang", "Junchi Yan", "Hua Yang", "Haodong Duan", "Xue Yang"], "categories": ["cs.CV"], "published": "2025-10-09T17:53:58Z", "pdf": "https://arxiv.org/pdf/2510.08540v1", "abs": "https://arxiv.org/abs/2510.08540v1", "comment": null}
{"id": "2510.08531v1", "title": "SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models", "summary": "Spatial reasoning remains a fundamental challenge for Vision-Language Models\n(VLMs), with current approaches struggling to achieve robust performance\ndespite recent advances. We identify that this limitation stems from a critical\ngap: existing methods attempt to learn spatial reasoning directly without\nestablishing the hierarchical foundations of perception and understanding. To\naddress this challenge, we present a comprehensive methodology for building\nspatial intelligence progressively. We introduce SpatialLadder-26k, a\nmultimodal dataset containing 26,610 samples spanning object localization,\nsingle image, multi-view, and video spatial reasoning tasks, constructed\nthrough a standardized pipeline that ensures systematic coverage across\nmodalities. Building on this dataset, we design a three-stage progressive\ntraining framework that (1) establishes spatial perception through object\nlocalization, (2) develops spatial understanding through multi-dimensional\nspatial tasks, and (3) strengthens complex reasoning via reinforcement learning\nwith verifiable rewards. This approach yields SpatialLadder, a 3B-parameter\nmodel that achieves state-of-the-art performance on spatial reasoning\nbenchmarks, with 23.4% average improvement over the base model, surpassing\nGPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains\nstrong generalization with 7.2% improvement on out-of-domain benchmarks,\ndemonstrating that progressive training from perception to reasoning is\nessential for robust spatial intelligence.", "authors": ["Hongxing Li", "Dingming Li", "Zixuan Wang", "Yuchen Yan", "Hang Wu", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-10-09T17:50:54Z", "pdf": "https://arxiv.org/pdf/2510.08531v1", "abs": "https://arxiv.org/abs/2510.08531v1", "comment": "Project Page: https://zju-real.github.io/SpatialLadder/ Code:\n  https://github.com/ZJU-REAL/SpatialLadder"}
{"id": "2510.08530v1", "title": "X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering", "summary": "We present X2Video, the first diffusion model for rendering photorealistic\nvideos guided by intrinsic channels including albedo, normal, roughness,\nmetallicity, and irradiance, while supporting intuitive multi-modal controls\nwith reference images and text prompts for both global and local regions. The\nintrinsic guidance allows accurate manipulation of color, material, geometry,\nand lighting, while reference images and text prompts provide intuitive\nadjustments in the absence of intrinsic information. To enable these\nfunctionalities, we extend the intrinsic-guided image generation model XRGB to\nvideo generation by employing a novel and efficient Hybrid Self-Attention,\nwhich ensures temporal consistency across video frames and also enhances\nfidelity to reference images. We further develop a Masked Cross-Attention to\ndisentangle global and local text prompts, applying them effectively onto\nrespective local and global regions. For generating long videos, our novel\nRecursive Sampling method incorporates progressive frame sampling, combining\nkeyframe prediction and frame interpolation to maintain long-range temporal\nconsistency while preventing error accumulation. To support the training of\nX2Video, we assembled a video dataset named InteriorVideo, featuring 1,154\nrooms from 295 interior scenes, complete with reliable ground-truth intrinsic\nchannel sequences and smooth camera trajectories. Both qualitative and\nquantitative evaluations demonstrate that X2Video can produce long, temporally\nconsistent, and photorealistic videos guided by intrinsic conditions.\nAdditionally, X2Video effectively accommodates multi-modal controls with\nreference images, global and local text prompts, and simultaneously supports\nediting on color, material, geometry, and lighting through parametric tuning.\nProject page: https://luckyhzt.github.io/x2video", "authors": ["Zhitong Huang", "Mohan Zhang", "Renhan Wang", "Rui Tang", "Hao Zhu", "Jing Liao"], "categories": ["cs.GR", "cs.CV", "68U05", "I.3.3; I.3.6"], "published": "2025-10-09T17:50:31Z", "pdf": "https://arxiv.org/pdf/2510.08530v1", "abs": "https://arxiv.org/abs/2510.08530v1", "comment": "Code, model, and dataset will be released at project page soon:\n  https://luckyhzt.github.io/x2video"}
{"id": "2510.08527v1", "title": "FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control", "summary": "We present FlexTraj, a framework for image-to-video generation with flexible\npoint trajectory control. FlexTraj introduces a unified point-based motion\nrepresentation that encodes each point with a segmentation ID, a temporally\nconsistent trajectory ID, and an optional color channel for appearance cues,\nenabling both dense and sparse trajectory control. Instead of injecting\ntrajectory conditions into the video generator through token concatenation or\nControlNet, FlexTraj employs an efficient sequence-concatenation scheme that\nachieves faster convergence, stronger controllability, and more efficient\ninference, while maintaining robustness under unaligned conditions. To train\nsuch a unified point trajectory-controlled video generator, FlexTraj adopts an\nannealing training strategy that gradually reduces reliance on complete\nsupervision and aligned condition. Experimental results demonstrate that\nFlexTraj enables multi-granularity, alignment-agnostic trajectory control for\nvideo generation, supporting various applications such as motion cloning,\ndrag-based image-to-video, motion interpolation, camera redirection, flexible\naction control and mesh animations.", "authors": ["Zhiyuan Zhang", "Can Wang", "Dongdong Chen", "Jing Liao"], "categories": ["cs.CV"], "published": "2025-10-09T17:50:22Z", "pdf": "https://arxiv.org/pdf/2510.08527v1", "abs": "https://arxiv.org/abs/2510.08527v1", "comment": "Project Page: https://bestzzhang.github.io/FlexTraj"}
{"id": "2510.08525v1", "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression", "summary": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results.", "authors": ["Wenjie Du", "Li Jiang", "Keda Tao", "Xue Liu", "Huan Wang"], "categories": ["cs.CL"], "published": "2025-10-09T17:50:00Z", "pdf": "https://arxiv.org/pdf/2510.08525v1", "abs": "https://arxiv.org/abs/2510.08525v1", "comment": null}
{"id": "2510.08512v1", "title": "Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression", "summary": "Efficient transmission of 3D point cloud data is critical for advanced\nperception in centralized and decentralized multi-agent robotic systems,\nespecially nowadays with the growing reliance on edge and cloud-based\nprocessing. However, the large and complex nature of point clouds creates\nchallenges under bandwidth constraints and intermittent connectivity, often\ndegrading system performance. We propose a deep compression framework based on\nsemantic scene graphs. The method decomposes point clouds into semantically\ncoherent patches and encodes them into compact latent representations with\nsemantic-aware encoders conditioned by Feature-wise Linear Modulation (FiLM). A\nfolding-based decoder, guided by latent features and graph node attributes,\nenables structurally accurate reconstruction. Experiments on the SemanticKITTI\nand nuScenes datasets show that the framework achieves state-of-the-art\ncompression rates, reducing data size by up to 98% while preserving both\nstructural and semantic fidelity. In addition, it supports downstream\napplications such as multi-robot pose graph optimization and map merging,\nachieving trajectory accuracy and map alignment comparable to those obtained\nwith raw LiDAR scans.", "authors": ["Nikolaos Stathoulopoulos", "Christoforos Kanellakis", "George Nikolakopoulos"], "categories": ["cs.CV", "cs.RO"], "published": "2025-10-09T17:45:09Z", "pdf": "https://arxiv.org/pdf/2510.08512v1", "abs": "https://arxiv.org/abs/2510.08512v1", "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L). 8 pages, 6 figures"}
{"id": "2510.08508v1", "title": "MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration", "summary": "Real-world videos often suffer from complex degradations, such as noise,\ncompression artifacts, and low-light distortions, due to diverse acquisition\nand transmission conditions. Existing restoration methods typically require\nprofessional manual selection of specialized models or rely on monolithic\narchitectures that fail to generalize across varying degradations. Inspired by\nexpert experience, we propose MoA-VR, the first\n\\underline{M}ixture-\\underline{o}f-\\underline{A}gents \\underline{V}ideo\n\\underline{R}estoration system that mimics the reasoning and processing\nprocedures of human professionals through three coordinated agents: Degradation\nIdentification, Routing and Restoration, and Restoration Quality Assessment.\nSpecifically, we construct a large-scale and high-resolution video degradation\nrecognition benchmark and build a vision-language model (VLM) driven\ndegradation identifier. We further introduce a self-adaptive router powered by\nlarge language models (LLMs), which autonomously learns effective restoration\nstrategies by observing tool usage patterns. To assess intermediate and final\nprocessed video quality, we construct the \\underline{Res}tored\n\\underline{V}ideo \\underline{Q}uality (Res-VQ) dataset and design a dedicated\nVLM-based video quality assessment (VQA) model tailored for restoration tasks.\nExtensive experiments demonstrate that MoA-VR effectively handles diverse and\ncompound degradations, consistently outperforming existing baselines in terms\nof both objective metrics and perceptual quality. These results highlight the\npotential of integrating multimodal intelligence and modular reasoning in\ngeneral-purpose video restoration systems.", "authors": ["Lu Liu", "Chunlei Cai", "Shaocheng Shen", "Jianfeng Liang", "Weimin Ouyang", "Tianxiao Ye", "Jian Mao", "Huiyu Duan", "Jiangchao Yao", "Xiaoyun Zhang", "Qiang Hu", "Guangtao Zhai"], "categories": ["cs.CV"], "published": "2025-10-09T17:42:51Z", "pdf": "https://arxiv.org/pdf/2510.08508v1", "abs": "https://arxiv.org/abs/2510.08508v1", "comment": null}
{"id": "2510.08494v1", "title": "Quartic quantum speedups for community detection", "summary": "Community detection is a foundational problem in data science. Its natural\nextension to hypergraphs captures higher-order correlations beyond pairwise\ninteractions. In this work, we develop a quantum algorithm for hypergraph\ncommunity detection that achieves a quartic quantum speedup over the best known\nclassical algorithm, along with superpolynomial savings in space. Our algorithm\nis based on the Kikuchi method, which we extend beyond previously considered\nproblems such as Tensor PCA and $p$XORSAT to a broad family of generalized\nstochastic block models. To demonstrate (near) optimality of this method, we\nprove matching lower bounds (up to logarithmic factors) in the low-degree\nframework, showing that the algorithm saturates a smooth\nstatistical-computational tradeoff. The quantum speedup arises from a quantized\nversion of the Kikuchi method and is based on the efficient preparation of a\nguiding state correlated with the underlying community structure. Our work\nsuggests that prior quantum speedups using the Kikuchi method are sufficiently\nrobust to encompass a broader set of problems than previously believed; we\nconjecture that a quantity known as marginal order characterizes the existence\nof these quantum speedups.", "authors": ["Alexander Schmidhuber", "Alexander Zlokapa"], "categories": ["quant-ph", "cs.DS"], "published": "2025-10-09T17:35:17Z", "pdf": "https://arxiv.org/pdf/2510.08494v1", "abs": "https://arxiv.org/abs/2510.08494v1", "comment": "40 pages"}
{"id": "2510.08492v1", "title": "Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models", "summary": "Traditional multimodal learners find unified representations for tasks like\nvisual question answering, but rely heavily on paired datasets. However, an\noverlooked yet potentially powerful question is: can one leverage auxiliary\nunpaired multimodal data to directly enhance representation learning in a\ntarget modality? We introduce UML: Unpaired Multimodal Learner, a\nmodality-agnostic training paradigm in which a single model alternately\nprocesses inputs from different modalities while sharing parameters across\nthem. This design exploits the assumption that different modalities are\nprojections of a shared underlying reality, allowing the model to benefit from\ncross-modal structure without requiring explicit pairs. Theoretically, under\nlinear data-generating assumptions, we show that unpaired auxiliary data can\nyield representations strictly more informative about the data-generating\nprocess than unimodal training. Empirically, we show that using unpaired data\nfrom auxiliary modalities -- such as text, audio, or images -- consistently\nimproves downstream performance across diverse unimodal targets such as image\nand audio. Our project page: https://unpaired-multimodal.github.io/", "authors": ["Sharut Gupta", "Shobhita Sundaram", "Chenyu Wang", "Stefanie Jegelka", "Phillip Isola"], "categories": ["cs.LG", "cs.CV"], "published": "2025-10-09T17:32:23Z", "pdf": "https://arxiv.org/pdf/2510.08492v1", "abs": "https://arxiv.org/abs/2510.08492v1", "comment": "63 pages, 29 tables, and 47 figures"}
{"id": "2510.08485v1", "title": "InstructX: Towards Unified Visual Editing with MLLM Guidance", "summary": "With recent advances in Multimodal Large Language Models (MLLMs) showing\nstrong visual understanding and reasoning, interest is growing in using them to\nimprove the editing performance of diffusion models. Despite rapid progress,\nmost studies lack an in-depth analysis of MLLM design choices. Moreover, the\nintegration of MLLMs and diffusion models remains an open challenge in some\ndifficult tasks, such as video editing. In this paper, we present InstructX, a\nunified framework for image and video editing. Specifically, we conduct a\ncomprehensive study on integrating MLLMs and diffusion models for\ninstruction-driven editing across diverse tasks. Building on this study, we\nanalyze the cooperation and distinction between images and videos in unified\nmodeling. (1) We show that training on image data can lead to emergent video\nediting capabilities without explicit supervision, thereby alleviating the\nconstraints imposed by scarce video training data. (2) By incorporating\nmodality-specific MLLM features, our approach effectively unifies image and\nvideo editing tasks within a single model. Extensive experiments demonstrate\nthat our method can handle a broad range of image and video editing tasks and\nachieves state-of-the-art performance.", "authors": ["Chong Mou", "Qichao Sun", "Yanze Wu", "Pengze Zhang", "Xinghui Li", "Fulong Ye", "Songtao Zhao", "Qian He"], "categories": ["cs.CV"], "published": "2025-10-09T17:26:09Z", "pdf": "https://arxiv.org/pdf/2510.08485v1", "abs": "https://arxiv.org/abs/2510.08485v1", "comment": null}
{"id": "2510.08482v1", "title": "The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping", "summary": "Iconicity, the resemblance between linguistic form and meaning, is pervasive\nin signed languages, offering a natural testbed for visual grounding. For\nvision-language models (VLMs), the challenge is to recover such essential\nmappings from dynamic human motion rather than static context. We introduce the\n\\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts\npsycholinguistic measures to evaluate VLMs on three tasks: (i) phonological\nsign-form prediction (e.g., handshape, location), (ii) transparency (inferring\nmeaning from visual form), and (iii) graded iconicity ratings. We assess $13$\nstate-of-the-art VLMs in zero- and few-shot settings on Sign Language of the\nNetherlands and compare them to human baselines. On \\textit{phonological form\nprediction}, VLMs recover some handshape and location detail but remain below\nhuman performance; on \\textit{transparency}, they are far from human baselines;\nand only top models correlate moderately with human \\textit{iconicity ratings}.\nInterestingly, \\textit{models with stronger phonological form prediction\ncorrelate better with human iconicity judgment}, indicating shared sensitivity\nto visually grounded structure. Our findings validate these diagnostic tasks\nand motivate human-centric signals and embodied learning methods for modelling\niconicity and improving visual grounding in multimodal models.", "authors": ["Onur Keleş", "Aslı Özyürek", "Gerardo Ortega", "Kadir Gökgö", "Esam Ghaleb"], "categories": ["cs.CV", "cs.CL"], "published": "2025-10-09T17:21:59Z", "pdf": "https://arxiv.org/pdf/2510.08482v1", "abs": "https://arxiv.org/abs/2510.08482v1", "comment": null}
{"id": "2510.08480v1", "title": "Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools", "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\npotential in bridging visual and textual reasoning, yet their reliance on\ntext-centric priors often limits their ability to disentangle semantically\nsimilar actions in open-vocabulary scenarios. To address this, we propose\nVideo-STAR, a framework that harmonizes contextual sub-motion decomposition\nwith tool-augmented reinforcement learning for open-vocabulary action\nrecognition (OVAR). Unlike prior methods that treat actions as monolithic\nentities, our approach innovatively decomposes actions into discriminative\nsub-motions for fine-grained matching while dynamically invoking\ndomain-specific tools for cross-modal interleaving, thereby enabling\ncategory-specific reasoning capacity and reducing cross-modal hallucination.\nMoreover, by designing a hierarchical reward that balances tool-usage\nefficiency, sub-motion relevance, and structural coherence in reasoning, our\nmethod autonomously leverages external tools to prioritize sub-motion patterns\nwithout explicit supervision, transmitting from text-centric reasoning to\nvisually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2,\nKinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art\nperformance, outperforming existing methods in distinguishing fine-grained\nactions and handling cross-modal hallucination, validating our excellent\nrobustness and generalization.", "authors": ["Zhenlong Yuan", "Xiangyan Qu", "Chengxuan Qian", "Rui Chen", "Jing Tang", "Lei Sun", "Xiangxiang Chu", "Dapeng Zhang", "Yiwei Wang", "Yujun Cai", "Shuo Li"], "categories": ["cs.CV"], "published": "2025-10-09T17:20:44Z", "pdf": "https://arxiv.org/pdf/2510.08480v1", "abs": "https://arxiv.org/abs/2510.08480v1", "comment": null}
{"id": "2510.08475v1", "title": "DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos", "summary": "We present DexMan, an automated framework that converts human visual\ndemonstrations into bimanual dexterous manipulation skills for humanoid robots\nin simulation. Operating directly on third-person videos of humans manipulating\nrigid objects, DexMan eliminates the need for camera calibration, depth\nsensors, scanned 3D object assets, or ground-truth hand and object motion\nannotations. Unlike prior approaches that consider only simplified floating\nhands, it directly controls a humanoid robot and leverages novel contact-based\nrewards to improve policy learning from noisy hand-object poses estimated from\nin-the-wild videos.\n  DexMan achieves state-of-the-art performance in object pose estimation on the\nTACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.\nMeanwhile, its reinforcement learning policy surpasses previous methods by 19%\nin success rate on OakInk-v2. Furthermore, DexMan can generate skills from both\nreal and synthetic videos, without the need for manual data collection and\ncostly motion capture, and enabling the creation of large-scale, diverse\ndatasets for training generalist dexterous manipulation.", "authors": ["Jhen Hsieh", "Kuan-Hsun Tu", "Kuo-Han Hung", "Tsung-Wei Ke"], "categories": ["cs.RO", "cs.CV", "cs.LG"], "published": "2025-10-09T17:17:05Z", "pdf": "https://arxiv.org/pdf/2510.08475v1", "abs": "https://arxiv.org/abs/2510.08475v1", "comment": "Video results are available at:\n  https://embodiedai-ntu.github.io/dexman/index.html"}
{"id": "2510.08470v1", "title": "Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling", "summary": "Training vision-language models on cognitively-plausible amounts of data\nrequires rethinking how models integrate multimodal information. Within the\nconstraints of the Vision track for the BabyLM Challenge 2025, we propose a\nlightweight decoder-based architecture with (1) token-wise dynamic gating for\nadaptive fusion of linguistic and visual cues, (2) feature modulation and\nchannel attention to maximise the utility of limited visual information and (3)\nauxiliary contrastive objectives for visual grounding. Evaluation on five\nbenchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows\ncompetitive or superior performance to multimodal baselines. More notably, our\ndynamic gate discovers interpretable patterns without explicit supervision,\nfavouring visual cues for content words and linguistic cues for function words.\nWhile we identify limitations in the Challenge constraints, such as the\ninformation bottleneck created by global image embeddings and training\ninstability from the dataset split, our findings establish dynamic gating as a\npowerful tool for efficient multimodal learning, offering both interpretability\nand performance even under severe constraints.", "authors": ["Bianca-Mihaela Ganescu", "Suchir Salhan", "Andrew Caines", "Paula Buttery"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-10-09T17:10:36Z", "pdf": "https://arxiv.org/pdf/2510.08470v1", "abs": "https://arxiv.org/abs/2510.08470v1", "comment": "Accepted to the EMNLP 2025 BabyLM Workshop"}
{"id": "2510.08464v1", "title": "Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered", "summary": "Vision-Language-Action (VLA) models have advanced robotic capabilities but\nremain challenging to deploy on resource-limited hardware. Pruning has enabled\nefficient compression of large language models (LLMs), yet it is largely\nunderstudied in robotics. Surprisingly, we observe that pruning VLA models\nleads to drastic degradation and increased safety violations. We introduce\nGLUESTICK, a post-pruning recovery method that restores much of the original\nmodel's functionality while retaining sparsity benefits. Our method performs a\none-time interpolation between the dense and pruned models in weight-space to\ncompute a corrective term. This correction is used during inference by each\npruned layer to recover lost capabilities with minimal overhead. GLUESTICK\nrequires no additional training, is agnostic to the pruning algorithm, and\nintroduces a single hyperparameter that controls the tradeoff between\nefficiency and accuracy. Across diverse VLA architectures and tasks in\nmanipulation and navigation, GLUESTICK achieves competitive memory efficiency\nwhile substantially recovering success rates and reducing safety violations.\nAdditional material can be found at: https://gluestick-vla.github.io/.", "authors": ["Jason Jabbour", "Dong-Ki Kim", "Max Smith", "Jay Patrikar", "Radhika Ghosal", "Youhui Wang", "Ali Agha", "Vijay Janapa Reddi", "Shayegan Omidshafiei"], "categories": ["cs.RO", "cs.LG"], "published": "2025-10-09T17:07:30Z", "pdf": "https://arxiv.org/pdf/2510.08464v1", "abs": "https://arxiv.org/abs/2510.08464v1", "comment": null}
{"id": "2510.08457v1", "title": "ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping", "summary": "Recent advances in multimodal large reasoning models (MLRMs) have\nsubstantially improved their ability to solve complex textual and visual tasks.\nHowever, these models tend to overthink on simple problems, producing\nunnecessarily lengthy reasoning traces, while under-exploring on challenging\nones, leading to missed solutions. To address this imbalance, we propose ARES,\na unified open-source framework for adaptive reasoning that dynamically\nallocates exploration effort based on task difficulty. Our approach is\nmotivated by two key empirical findings: (i) while single-token entropy is\nnoisy, high window-entropy (HWE) tokens (token-level entropies averaged under a\nsliding window) can reliably capture reasoning-critical moments; and (ii)\nreducing HWE usage benefits easy problems, while increasing it is essential for\nsolving hard ones. Building on these insights, ARES introduces a two-stage\ntraining pipeline. In the Adaptive Cold-Start stage, we curate multimodal and\ntextual data paired with reasoning traces of length proportional to problem\ndifficulty, equipping the model with initial difficulty awareness. In the\nsecond stage, we develop Adaptive Entropy Policy Optimization (AEPO), which\nuses HWE tokens as exploration triggers to decide when to explore, and a\nhierarchical entropy reward with dynamic KL control to decide how much to\nexplore. Extensive experiments demonstrate that ARES achieves superior\nperformance and reasoning efficiency across diverse mathematical, logical, and\nmultimodal benchmarks, while closing the gap to leading commercial systems\nunder significantly lower inference costs.", "authors": ["Shuang Chen", "Yue Guo", "Yimeng Ye", "Shijue Huang", "Wenbo Hu", "Haoxi Li", "Manyuan Zhang", "Jiayu Chen", "Song Guo", "Nanyun Peng"], "categories": ["cs.CL"], "published": "2025-10-09T17:03:28Z", "pdf": "https://arxiv.org/pdf/2510.08457v1", "abs": "https://arxiv.org/abs/2510.08457v1", "comment": null}
{"id": "2510.08449v1", "title": "Hierarchical Spatial Algorithms for High-Resolution Image Quantization and Feature Extraction", "summary": "This study introduces a modular framework for spatial image processing,\nintegrating grayscale quantization, color and brightness enhancement, image\nsharpening, bidirectional transformation pipelines, and geometric feature\nextraction. A stepwise intensity transformation quantizes grayscale images into\neight discrete levels, producing a posterization effect that simplifies\nrepresentation while preserving structural detail. Color enhancement is\nachieved via histogram equalization in both RGB and YCrCb color spaces, with\nthe latter improving contrast while maintaining chrominance fidelity.\nBrightness adjustment is implemented through HSV value-channel manipulation,\nand image sharpening is performed using a 3 * 3 convolution kernel to enhance\nhigh-frequency details. A bidirectional transformation pipeline that integrates\nunsharp masking, gamma correction, and noise amplification achieved accuracy\nlevels of 76.10% and 74.80% for the forward and reverse processes,\nrespectively. Geometric feature extraction employed Canny edge detection,\nHough-based line estimation (e.g., 51.50{\\deg} for billiard cue alignment),\nHarris corner detection, and morphological window localization. Cue isolation\nfurther yielded 81.87\\% similarity against ground truth images. Experimental\nevaluation across diverse datasets demonstrates robust and deterministic\nperformance, highlighting its potential for real-time image analysis and\ncomputer vision.", "authors": ["Noor Islam S. Mohammad"], "categories": ["cs.CV", "68T45, 68U10", "I.4.8; I.2.10"], "published": "2025-10-09T16:56:24Z", "pdf": "https://arxiv.org/pdf/2510.08449v1", "abs": "https://arxiv.org/abs/2510.08449v1", "comment": "There are 14 pages journal paper"}
{"id": "2510.08419v1", "title": "Continuous Variable Hamiltonian Learning at Heisenberg Limit via Displacement-Random Unitary Transformation", "summary": "Characterizing the Hamiltonians of continuous-variable (CV) quantum systems\nis a fundamental challenge laden with difficulties arising from\ninfinite-dimensional Hilbert spaces and unbounded operators. Existing protocols\nfor achieving the Heisenberg limit precision are often restricted to specific\nHamiltonian structures or demand experimentally challenging resources. In this\nwork, we introduce an efficient and experimentally accessible protocol, the\nDisplacement-Random Unitary Transformation (D-RUT), that learns the\ncoefficients of general, arbitrary finite-order bosonic Hamiltonians with a\ntotal evolution time scaling as $O(1/\\epsilon)$ for a target precision\n$\\epsilon$ robust to SPAM error. For multi-mode systems, we develop a\nhierarchical coefficients recovering strategy with superior statistical\nefficiency. Furthermore, we extend our protocol to first quantization, enabling\nthe learning of fundamental physical parameters from Hamiltonians expressed in\nposition and momentum operators at the Heisenberg limit.", "authors": ["Xi Huang", "Lixing Zhang", "Di Luo"], "categories": ["quant-ph"], "published": "2025-10-09T16:37:47Z", "pdf": "https://arxiv.org/pdf/2510.08419v1", "abs": "https://arxiv.org/abs/2510.08419v1", "comment": null}
{"id": "2510.08381v1", "title": "Airy: Reading Robot Intent through Height and Sky", "summary": "As industrial robots move into shared human spaces, their opaque decision\nmaking threatens safety, trust, and public oversight. This artwork, Airy, asks\nwhether complex multi agent AI can become intuitively understandable by staging\na competition between two reinforcement trained robot arms that snap a bedsheet\nskyward. Building on three design principles, competition as a clear metric\n(who lifts higher), embodied familiarity (audiences recognize fabric snapping),\nand sensor to sense mapping (robot cooperation or rivalry shown through forest\nand weather projections), the installation gives viewers a visceral way to read\nmachine intent. Observations from five international exhibitions indicate that\naudiences consistently read the robots' strategies, conflict, and cooperation\nin real time, with emotional reactions that mirror the system's internal state.\nThe project shows how sensory metaphors can turn a black box into a public\ninterface.", "authors": ["Baoyang Chen", "Xian Xu", "Huamin Qu"], "categories": ["cs.RO", "cs.AI"], "published": "2025-10-09T16:07:30Z", "pdf": "https://arxiv.org/pdf/2510.08381v1", "abs": "https://arxiv.org/abs/2510.08381v1", "comment": null}
{"id": "2510.08377v1", "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos", "summary": "Unified multimodal models have shown promising results in multimodal content\ngeneration and editing but remain largely limited to the image domain. In this\nwork, we present UniVideo, a versatile framework that extends unified modeling\nto the video domain. UniVideo adopts a dual-stream design, combining a\nMultimodal Large Language Model (MLLM) for instruction understanding with a\nMultimodal DiT (MMDiT) for video generation. This design enables accurate\ninterpretation of complex multimodal instructions while preserving visual\nconsistency. Built on this architecture, UniVideo unifies diverse video\ngeneration and editing tasks under a single multimodal instruction paradigm and\nis jointly trained across them. Extensive experiments demonstrate that UniVideo\nmatches or surpasses state-of-the-art task-specific baselines in\ntext/image-to-video generation, in-context video generation and in-context\nvideo editing. Notably, the unified design of UniVideo enables two forms of\ngeneralization. First, UniVideo supports task composition, such as combining\nediting with style transfer, by integrating multiple capabilities within a\nsingle instruction. Second, even without explicit training on free-form video\nediting, UniVideo transfers its editing capability from large-scale image\nediting data to this setting, handling unseen instructions such as\ngreen-screening characters or changing materials within a video. Beyond these\ncore capabilities, UniVideo also supports visual-prompt-based video generation,\nwhere the MLLM interprets visual prompts and guides the MMDiT during synthesis.\nTo foster future research, we will release our model and code.", "authors": ["Cong Wei", "Quande Liu", "Zixuan Ye", "Qiulin Wang", "Xintao Wang", "Pengfei Wan", "Kun Gai", "Wenhu Chen"], "categories": ["cs.CV"], "published": "2025-10-09T16:01:30Z", "pdf": "https://arxiv.org/pdf/2510.08377v1", "abs": "https://arxiv.org/abs/2510.08377v1", "comment": "Project Website https://congwei1230.github.io/UniVideo/"}
{"id": "2510.08368v1", "title": "Co-design is powerful and not free", "summary": "Robotic performance emerges from the coupling of body and controller, yet it\nremains unclear when morphology-control co-design is necessary. We present a\nunified framework that embeds morphology and control parameters within a single\nneural network, enabling end-to-end joint optimization. Through case studies in\nstatic-obstacle-constrained reaching, we evaluate trajectory error, success\nrate, and collision probability. The results show that co-design provides clear\nbenefits when morphology is poorly matched to the task, such as near obstacles\nor workspace boundaries, where structural adaptation simplifies control.\nConversely, when the baseline morphology already affords sufficient capability,\ncontrol-only optimization often matches or exceeds co-design. By clarifying\nwhen control is enough and when it is not, this work advances the understanding\nof embodied intelligence and offers practical guidance for embodiment-aware\nrobot design.", "authors": ["Yi Zhang", "Yue Xie", "Tao Sun", "Fumiya Iida"], "categories": ["cs.NE", "cs.RO"], "published": "2025-10-09T15:52:48Z", "pdf": "https://arxiv.org/pdf/2510.08368v1", "abs": "https://arxiv.org/abs/2510.08368v1", "comment": null}
{"id": "2510.08366v1", "title": "A data fusion approach for mobility hub impact assessment and location selection: integrating hub usage data into a large-scale mode choice model", "summary": "As cities grapple with traffic congestion and service inequities, mobility\nhubs offer a scalable solution to align increasing travel demand with\nsustainability goals. However, evaluating their impacts remains challenging due\nto the lack of behavioral models that integrate large-scale travel patterns\nwith real-world hub usage. This study presents a novel data fusion approach\nthat incorporates observed mobility hub usage into a mode choice model\nestimated with synthetic trip data. We identify trips potentially affected by\nmobility hubs and construct a multimodal sub-choice set, then calibrate\nhub-specific parameters using on-site survey data and ground truth trip counts.\nThe enhanced model is used to evaluate mobility hub impacts on potential\ndemand, mode shift, reduced vehicle miles traveled (VMT), and increased\nconsumer surplus (CS). We apply this method to a case study in the Capital\nDistrict, NY, using data from a survey conducted by the Capital District\nTransportation Authority (CDTA) and a mode choice model estimated using Replica\nInc. synthetic data. The two implemented hubs located near UAlbany Downtown\nCampus and in Downtown Cohoes are projected to generate 8.83 and 6.17\nmultimodal trips per day, reduce annual VMT by 20.37 and 13.16 thousand miles,\nand increase daily CS by $4,000 and $1,742, respectively. An evaluation of\npotential hub candidates in the Albany-Schenectady-Troy metropolitan area with\nthe estimated models demonstrates that hubs located along intercity corridors\nand at urban peripheries, supporting park-and-ride P+R patterns, yield the most\nsignificant behavioral impacts.", "authors": ["Xiyuan Ren", "Joseph Y. J. Chow"], "categories": ["econ.GN", "q-fin.EC"], "published": "2025-10-09T15:51:22Z", "pdf": "https://arxiv.org/pdf/2510.08366v1", "abs": "https://arxiv.org/abs/2510.08366v1", "comment": null}
{"id": "2510.08359v1", "title": "Doubly Robust Estimation with Stabilized Weights for Binary Proximal Outcomes in Micro-Randomized Trials", "summary": "Micro-randomized trials (MRTs) are increasingly used to evaluate mobile\nhealth interventions with binary proximal outcomes. Standard inverse\nprobability weighting (IPW) estimators are unbiased but unstable in small\nsamples or under extreme randomization. Estimated mean excursion effect (EMEE)\nimproves efficiency but lacks double robustness. We propose a doubly robust\nEMEE (DR-EMEE) with stabilized and truncated weights, combining per-decision\nIPW and outcome regression. We prove double robustness, asymptotic efficiency,\nand provide finite-sample variance corrections, with extensions to machine\nlearning nuisance estimators. In simulations, DR-EMEE reduces root mean squared\nerror, improves coverage, and achieves up to twofold efficiency gains over IPW\nand five to ten percent over EMEE. Applications to HeartSteps, PAMAP2, and\nmHealth datasets confirm stable and efficient inference across both randomized\nand observational settings.", "authors": ["Jinho Cha", "Eunchan Cha"], "categories": ["stat.ME", "math.ST", "stat.TH"], "published": "2025-10-09T15:44:16Z", "pdf": "https://arxiv.org/pdf/2510.08359v1", "abs": "https://arxiv.org/abs/2510.08359v1", "comment": "32 pages, 7 figures, planned to submit to Biostatistics"}
{"id": "2510.08321v1", "title": "Quantum variance and fluctuations for Walsh-quantized baker's maps", "summary": "The Walsh-quantized baker's maps are models for quantum chaos on the torus.\nWe show that for all baker's map scaling factors $D\\ge2$ except for $D=4$,\ntypically (in the sense of Haar measure on the eigenspaces, which are\ndegenerate) the empirical distribution of the scaled matrix element\nfluctuations $\\sqrt{N}\\{\\langle\n\\varphi^{(j)}|\\operatorname{Op}_{k,\\ell}(a)|\\varphi^{(j)}\\rangle-\\int_{\\mathbb{T}^2}a\\}_{j=1}^{N}$\nfor a random eigenbasis $\\{\\varphi^{(j)}\\}_{j=1}^{N}$ is asymptotically\nGaussian in the semiclassical limit $N\\to\\infty$, with variance given in terms\nof classical baker's map correlations. This determines the precise rate of\nconvergence in the quantum ergodic theorem for these eigenbases. We obtain a\nversion of the Eigenstate Thermalization Hypothesis (ETH) for these\neigenstates, including a limiting complex Gaussian distribution for the\noff-diagonal matrix elements, with variances also given in terms of classical\ncorrelations. The presence of the classical correlations highlights that these\neigenstates, while random, have microscopic correlations that differentiate\nthem from Haar random vectors. For the single value $D=4$, the Gaussianity of\nthe matrix element fluctuations depends on the values of the classical\nobservable on a fractal subset of the torus.", "authors": ["Laura Shou"], "categories": ["math-ph", "math.MP", "math.PR", "nlin.CD", "quant-ph"], "published": "2025-10-09T15:08:29Z", "pdf": "https://arxiv.org/pdf/2510.08321v1", "abs": "https://arxiv.org/abs/2510.08321v1", "comment": "48 pages"}
{"id": "2510.08316v1", "title": "Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge", "summary": "Affordance segmentation aims to parse 3D objects into functionally distinct\nparts, bridging recognition and interaction for applications in robotic\nmanipulation, embodied AI, and AR. While recent studies leverage visual or\ntextual prompts to guide this process, they often rely on point cloud encoders\nas generic feature extractors, overlooking the intrinsic challenges of 3D data\nsuch as sparsity, noise, and geometric ambiguity. As a result, 3D features\nlearned in isolation frequently lack clear and semantically consistent\nfunctional boundaries. To address this bottleneck, we propose a\nsemantic-grounded learning paradigm that transfers rich semantic knowledge from\nlarge-scale 2D Vision Foundation Models (VFMs) into the 3D domain.\nSpecifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-training\nstrategy that aligns a 3D encoder with lifted 2D semantics and jointly\noptimizes reconstruction, affinity, and diversity to yield semantically\norganized representations. Building on this backbone, we further design the\nCross-modal Affordance Segmentation Transformer (CAST), which integrates\nmulti-modal prompts with CMAT-pretrained features to generate precise,\nprompt-aware segmentation maps. Extensive experiments on standard benchmarks\ndemonstrate that our framework establishes new state-of-the-art results for 3D\naffordance segmentation.", "authors": ["Yu Huang", "Zelin Peng", "Changsong Wen", "Xiaokang Yang", "Wei Shen"], "categories": ["cs.CV"], "published": "2025-10-09T15:01:26Z", "pdf": "https://arxiv.org/pdf/2510.08316v1", "abs": "https://arxiv.org/abs/2510.08316v1", "comment": "Work in process"}
{"id": "2510.08304v1", "title": "Bayesian Profile Regression with Linear Mixed Models (Profile-LMM) applied to Longitudinal Exposome Data", "summary": "Exposure to diverse non-genetic factors, known as the exposome, is a critical\ndeterminant of health outcomes. However, analyzing the exposome presents\nsignificant methodological challenges, including: high collinearity among\nexposures, the longitudinal nature of repeated measurements, and potential\ncomplex interactions with individual characteristics. In this paper, we address\nthese challenges by proposing a novel statistical framework that extends\nBayesian profile regression. Our method integrates profile regression, which\nhandles collinearity by clustering exposures into latent profiles, into a\nlinear mixed model (LMM), a framework for longitudinal data analysis. This\nprofile-LMM approach effectively accounts for within-person variability over\ntime while also incorporating interactions between the latent exposure clusters\nand individual characteristics. We validate our method using simulated data,\ndemonstrating its ability to accurately identify model parameters and recover\nthe true latent exposure cluster structure. Finally, we apply this approach to\na large longitudinal data set from the Lifelines cohort to identify\ncombinations of exposures that are significantly associated with diastolic\nblood pressure.", "authors": ["Matteo Amestoy", "Mark van de Wiel", "Jeroen Lakerveld", "Wessel van Wieringen"], "categories": ["stat.ME"], "published": "2025-10-09T14:55:31Z", "pdf": "https://arxiv.org/pdf/2510.08304v1", "abs": "https://arxiv.org/abs/2510.08304v1", "comment": null}
{"id": "2510.08278v1", "title": "A Multimodal Depth-Aware Method For Embodied Reference Understanding", "summary": "Embodied Reference Understanding requires identifying a target object in a\nvisual scene based on both language instructions and pointing cues. While prior\nworks have shown progress in open-vocabulary object detection, they often fail\nin ambiguous scenarios where multiple candidate objects exist in the scene. To\naddress these challenges, we propose a novel ERU framework that jointly\nleverages LLM-based data augmentation, depth-map modality, and a depth-aware\ndecision module. This design enables robust integration of linguistic and\nembodied cues, improving disambiguation in complex or cluttered environments.\nExperimental results on two datasets demonstrate that our approach\nsignificantly outperforms existing baselines, achieving more accurate and\nreliable referent detection.", "authors": ["Fevziye Irem Eyiokur", "Dogucan Yaman", "Hazım Kemal Ekenel", "Alexander Waibel"], "categories": ["cs.CV", "cs.HC", "cs.RO"], "published": "2025-10-09T14:32:21Z", "pdf": "https://arxiv.org/pdf/2510.08278v1", "abs": "https://arxiv.org/abs/2510.08278v1", "comment": null}
{"id": "2510.08270v1", "title": "Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots", "summary": "This study evaluates the performance of classical and modern control methods\nfor real-world Cable-Driven Parallel Robots (CDPRs), focusing on\nunderconstrained systems with limited time discretization. A comparative\nanalysis is conducted between classical PID controllers and modern\nreinforcement learning algorithms, including Deep Deterministic Policy Gradient\n(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy\nOptimization (TRPO). The results demonstrate that TRPO outperforms other\nmethods, achieving the lowest root mean square (RMS) errors across various\ntrajectories and exhibiting robustness to larger time intervals between control\nupdates. TRPO's ability to balance exploration and exploitation enables stable\ncontrol in noisy, real-world environments, reducing reliance on high-frequency\nsensor feedback and computational demands. These findings highlight TRPO's\npotential as a robust solution for complex robotic control tasks, with\nimplications for dynamic environments and future applications in sensor fusion\nor hybrid control strategies.", "authors": ["Damir Nurtdinov", "Aliaksei Korshuk", "Alexei Kornaev", "Alexander Maloletov"], "categories": ["cs.RO"], "published": "2025-10-09T14:28:58Z", "pdf": "https://arxiv.org/pdf/2510.08270v1", "abs": "https://arxiv.org/abs/2510.08270v1", "comment": null}
{"id": "2510.08238v1", "title": "Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness", "summary": "The rapid deployment of large language model (LLM)-based agents in real-world\napplications has raised serious concerns about their trustworthiness. In this\nwork, we reveal the security and robustness vulnerabilities of these agents\nthrough backdoor attacks. Distinct from traditional backdoors limited to\nsingle-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a\nmulti-step backdoor attack designed for long-horizon agentic control. CoTri\nrelies on an ordered sequence. It starts with an initial trigger, and\nsubsequent ones are drawn from the environment, allowing multi-step\nmanipulation that diverts the agent from its intended task. Experimental\nresults show that CoTri achieves a near-perfect attack success rate (ASR) while\nmaintaining a near-zero false trigger rate (FTR). Due to training data modeling\nthe stochastic nature of the environment, the implantation of CoTri\nparadoxically enhances the agent's performance on benign tasks and even\nimproves its robustness against environmental distractions. We further validate\nCoTri on vision-language models (VLMs), confirming its scalability to\nmultimodal agents. Our work highlights that CoTri achieves stable, multi-step\ncontrol within agents, improving their inherent robustness and task\ncapabilities, which ultimately makes the attack more stealthy and raises\npotential safty risks.", "authors": ["Jiyang Qiu", "Xinbei Ma", "Yunqing Xu", "Zhuosheng Zhang", "Hai Zhao"], "categories": ["cs.AI"], "published": "2025-10-09T14:01:43Z", "pdf": "https://arxiv.org/pdf/2510.08238v1", "abs": "https://arxiv.org/abs/2510.08238v1", "comment": null}
{"id": "2510.08227v1", "title": "Practicing a Second Language Without Fear: Mixed Reality Agents for Interactive Group Conversation", "summary": "Developing speaking proficiency in a second language can be cognitively\ndemanding and emotionally taxing, often triggering fear of making mistakes or\nbeing excluded from larger groups. While current learning tools show promise\nfor speaking practice, most focus on dyadic, scripted scenarios, limiting\nopportunities for dynamic group interactions. To address this gap, we present\nConversAR, a Mixed Reality system that leverages Generative AI and XR to\nsupport situated and personalized group conversations. It integrates embodied\nAI agents, scene recognition, and generative 3D props anchored to real-world\nsurroundings. Based on a formative study with experts in language acquisition,\nwe developed and tested this system with a user study with 21 second-language\nlearners. Results indicate that the system enhanced learner engagement,\nincreased willingness to communicate, and offered a safe space for speaking. We\ndiscuss the implications for integrating Generative AI and XR into the design\nof future language learning applications.", "authors": ["Mariana Fernandez-Espinosa", "Kai Zhang", "Jad Bendarkawi", "Ashley Ponce", "Sean Chidozie Mata", "Aminah Aliu", "Lei Zhang", "Francisco Fernandez Medina", "Elena Mangione-Lora", "Andres Monroy-Hernandez", "Diego Gomez-Zara"], "categories": ["cs.HC"], "published": "2025-10-09T13:50:42Z", "pdf": "https://arxiv.org/pdf/2510.08227v1", "abs": "https://arxiv.org/abs/2510.08227v1", "comment": "22 pages"}
{"id": "2510.08218v1", "title": "Expressive Value Learning for Scalable Offline Reinforcement Learning", "summary": "Reinforcement learning (RL) is a powerful paradigm for learning to make\nsequences of decisions. However, RL has yet to be fully leveraged in robotics,\nprincipally due to its lack of scalability. Offline RL offers a promising\navenue by training agents on large, diverse datasets, avoiding the costly\nreal-world interactions of online RL. Scaling offline RL to increasingly\ncomplex datasets requires expressive generative models such as diffusion and\nflow matching. However, existing methods typically depend on either\nbackpropagation through time (BPTT), which is computationally prohibitive, or\npolicy distillation, which introduces compounding errors and limits scalability\nto larger base policies. In this paper, we consider the question of how to\ndevelop a scalable offline RL approach without relying on distillation or\nbackpropagation through time. We introduce Expressive Value Learning for\nOffline Reinforcement Learning (EVOR): a scalable offline RL approach that\nintegrates both expressive policies and expressive value functions. EVOR learns\nan optimal, regularized Q-function via flow matching during training. At\ninference-time, EVOR performs inference-time policy extraction via rejection\nsampling against the expressive value function, enabling efficient\noptimization, regularization, and compute-scalable search without retraining.\nEmpirically, we show that EVOR outperforms baselines on a diverse set of\noffline RL tasks, demonstrating the benefit of integrating expressive value\nlearning into offline RL.", "authors": ["Nicolas Espinosa-Dice", "Kiante Brantley", "Wen Sun"], "categories": ["cs.LG", "cs.AI", "I.2.6"], "published": "2025-10-09T13:42:20Z", "pdf": "https://arxiv.org/pdf/2510.08218v1", "abs": "https://arxiv.org/abs/2510.08218v1", "comment": "24 pages, 5 figures"}
{"id": "2510.08202v1", "title": "Sentiment Matters: An Analysis of 200 Human-SAV Interactions", "summary": "Shared Autonomous Vehicles (SAVs) are likely to become an important part of\nthe transportation system, making effective human-SAV interactions an important\narea of research. This paper introduces a dataset of 200 human-SAV interactions\nto further this area of study. We present an open-source human-SAV\nconversational dataset, comprising both textual data (e.g., 2,136 human-SAV\nexchanges) and empirical data (e.g., post-interaction survey results on a range\nof psychological factors). The dataset's utility is demonstrated through two\nbenchmark case studies: First, using random forest modeling and chord diagrams,\nwe identify key predictors of SAV acceptance and perceived service quality,\nhighlighting the critical influence of response sentiment polarity (i.e.,\nperceived positivity). Second, we benchmark the performance of an LLM-based\nsentiment analysis tool against the traditional lexicon-based TextBlob method.\nResults indicate that even simple zero-shot LLM prompts more closely align with\nuser-reported sentiment, though limitations remain. This study provides novel\ninsights for designing conversational SAV interfaces and establishes a\nfoundation for further exploration into advanced sentiment modeling, adaptive\nuser interactions, and multimodal conversational systems.", "authors": ["Lirui Guo", "Michael G. Burke", "Wynita M. Griggs"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.ET"], "published": "2025-10-09T13:30:23Z", "pdf": "https://arxiv.org/pdf/2510.08202v1", "abs": "https://arxiv.org/abs/2510.08202v1", "comment": "Accepted for presentation at IEEE ITSC 2025 and for publication in\n  its Proceedings. \\c{opyright} 2025 IEEE. Personal use permitted; other uses\n  require permission from IEEE, including reprinting, republishing, or reuse of\n  any copyrighted component of this work"}
{"id": "2510.08186v1", "title": "Multimodal Topological Textures Arising from Coupled Structural Orders in SrTiO$_3$", "summary": "Magnetic spin topological textures recently found their electrical\ncounterparts in polar topologies emerging from the condensation of\ninhomogeneous polar atomic distortions. Here, we further extend the concept to\nother non-polar atomic degrees of freedom. Taking SrTiO$_3$ as a prototypical\nexample, we investigate from second-principles atomistic simulations, the\nequilibrium domain structures and topological textures associated with the\nnatural antiferrodistortive rotations of its oxygen octahedra. % Besides the\ncommon 90$^\\circ$ antiferrodistortive domain walls (twin boundaries), we\nidentify new metastable 180$^\\circ$ domain walls oriented along the\n$\\lbrace100\\rbrace_\\mathrm{pc}$ direction, when compressive epitaxial strain is\napplied. These domains exhibit complex antiferrodistortive Bloch- and\nN\\'eel-like configurations with the later being the most favorable. We also\nstabilize antiferrodistortive vortex and antivortex structures which are\naccompanied by co-localized polarization vortices and a complex pattern of the\nlocal strain field, giving rise to a trimodal topological structures. Our\nresults extends the concept of topological ordering to non-polar structural\ndegrees of freedom and highlights the role of lattice-mediated couplings in\nstabilizing complex textures in perovskite oxides.", "authors": ["Fernando Gómez-Ortiz", "Louis Bastogne", "Philippe Ghosez"], "categories": ["cond-mat.mtrl-sci"], "published": "2025-10-09T13:14:07Z", "pdf": "https://arxiv.org/pdf/2510.08186v1", "abs": "https://arxiv.org/abs/2510.08186v1", "comment": null}
{"id": "2510.08176v1", "title": "Leveraging Whisper Embeddings for Audio-based Lyrics Matching", "summary": "Audio-based lyrics matching can be an appealing alternative to other\ncontent-based retrieval approaches, but existing methods often suffer from\nlimited reproducibility and inconsistent baselines. In this work, we introduce\nWEALY, a fully reproducible pipeline that leverages Whisper decoder embeddings\nfor lyrics matching tasks. WEALY establishes robust and transparent baselines,\nwhile also exploring multimodal extensions that integrate textual and acoustic\nfeatures. Through extensive experiments on standard datasets, we demonstrate\nthat WEALY achieves a performance comparable to state-of-the-art methods that\nlack reproducibility. In addition, we provide ablation studies and analyses on\nlanguage robustness, loss functions, and embedding strategies. This work\ncontributes a reliable benchmark for future research, and underscores the\npotential of speech technologies for music information retrieval tasks.", "authors": ["Eleonora Mancini", "Joan Serrà", "Paolo Torroni", "Yuki Mitsufuji"], "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "published": "2025-10-09T13:03:34Z", "pdf": "https://arxiv.org/pdf/2510.08176v1", "abs": "https://arxiv.org/abs/2510.08176v1", "comment": null}
{"id": "2510.08173v1", "title": "NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions", "summary": "Instruction-following navigation is a key step toward embodied intelligence.\nPrior benchmarks mainly focus on semantic understanding but overlook\nsystematically evaluating navigation agents' spatial perception and reasoning\ncapabilities. In this work, we introduce the NavSpace benchmark, which contains\nsix task categories and 1,228 trajectory-instruction pairs designed to probe\nthe spatial intelligence of navigation agents. On this benchmark, we\ncomprehensively evaluate 22 navigation agents, including state-of-the-art\nnavigation models and multimodal large language models. The evaluation results\nlift the veil on spatial intelligence in embodied navigation. Furthermore, we\npropose SNav, a new spatially intelligent navigation model. SNav outperforms\nexisting navigation agents on NavSpace and real robot tests, establishing a\nstrong baseline for future work.", "authors": ["Haolin Yang", "Yuxing Long", "Zhuoyuan Yu", "Zihan Yang", "Minghan Wang", "Jiapeng Xu", "Yihan Wang", "Ziyan Yu", "Wenzhe Cai", "Lei Kang", "Hao Dong"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "published": "2025-10-09T12:59:19Z", "pdf": "https://arxiv.org/pdf/2510.08173v1", "abs": "https://arxiv.org/abs/2510.08173v1", "comment": null}
{"id": "2510.08163v1", "title": "ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code", "summary": "Large Reasoning Models (LRMs) often suffer from the ``over-thinking''\nproblem, generating unnecessarily long reasoning on simple tasks. Some\nstrategies have been proposed to mitigate this issue, such as length penalties\nor routing mechanisms, but they are typically heuristic and task-specific,\nlacking a general framework for adaptive reasoning. In this paper, we present\nARM2, a unified model that adaptively balances reasoning performance and\nefficiency across multiple formats through a reinforcement learning framework\naugmented with length-aware optimization. Beyond conventional natural language\ninference, ARM2 integrates vision understanding, extending its applicability to\nmultimodal. Moreover, ARM2 integrates executable code into reasoning, enabling\nsubstantial reductions in token cost while preserving task performance compared\nto long CoT. Experiments demonstrate that ARM2 achieves performance on par with\ntraditional reasoning models trained with GRPO, while reducing token usage by\nover 70% on average. We further conduct extensive analyses to validate the\neffectiveness of ARM2 and the soundness of its design.", "authors": ["Jian Xie", "Zhendong Chu", "Aoxiao Zhong", "Kai Zhang", "Mingzhe Han", "Xin Fang", "Jialie Shen", "Qingsong Wen"], "categories": ["cs.CL"], "published": "2025-10-09T12:49:34Z", "pdf": "https://arxiv.org/pdf/2510.08163v1", "abs": "https://arxiv.org/abs/2510.08163v1", "comment": "Work in Progress"}
{"id": "2510.08157v1", "title": "Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing", "summary": "Image editing with natural language has gained significant popularity, yet\nexisting methods struggle with intricate object intersections and fine-grained\nspatial relationships due to the lack of an explicit reasoning process. While\nChain-of-Thought (CoT) has been explored to enhance reasoning, purely textual\nCoT or CoT augmented with coordinate information is fundamentally limited in\nits ability to represent intricate visual layouts and lacks the necessary\nvisual cues to guide the generation of fine-grained, pixel-level details. To\naddress these challenges, we propose Multimodal Reasoning Edit (MURE), a novel\nframework that shifts the visual editing process from purely text-based\nreasoning to a series of interleaved textual and visual rationales. Our\nframework performs image editing using a natively multimodal, interleaved\ntext-image CoT. This approach generates a step-by-step chain of reasoning where\na textual description is followed by a corresponding visual cue, such as a\npositional mask that defined intended edited regions or a representation of new\ncontent. Furthermore, to mitigate the hallucination phenomenon of large\nlanguage models, we introduce Multimodal Deep Confidence (MMDC) reasoning\nparadigm. This paradigm explores a tree of visual reasoning paths at each step.\nBy pruning low-quality branches using a deep confidence score from a reward\nmodel, it ensures the model consistently follows a high-quality trajectory\ntowards the final edited result. The proposed method decomposes complex editing\ntasks into interdependent sub-tasks, achieving greater precision at each stage\nand yielding high-fidelity edited results. We define the formulation for\ninterleaved text-image chains and release the first CoT-Edit-14K dataset,\ncomprising 14K high-quality editing examples. Extensive experiments show that\nour method yields significant improvements across three image editing\nbenchmarks.", "authors": ["Zhentao Zou", "Zhengrong Yue", "Kunpeng Du", "Binlei Bao", "Hanting Li", "Haizhen Xie", "Guozheng Xu", "Yue Zhou", "Yali Wang", "Jie Hu", "Xue Jiang", "Xinghao Chen"], "categories": ["cs.CV"], "published": "2025-10-09T12:36:51Z", "pdf": "https://arxiv.org/pdf/2510.08157v1", "abs": "https://arxiv.org/abs/2510.08157v1", "comment": "25pages,20figures"}
{"id": "2510.08118v1", "title": "Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)", "summary": "Robotic Process Mining focuses on the identification of the routine types\nperformed by human resources through a User Interface. The ultimate goal is to\ndiscover routine-type models to enable robotic process automation. The\ndiscovery of routine-type models requires the provision of a routine log.\nUnfortunately, the vast majority of existing works do not directly focus on\nenabling the model discovery, limiting themselves to extracting the set of\nactions that are part of the routines. They were also not evaluated in\nscenarios characterized by inconsistent routine execution, hereafter referred\nto as noise, which reflects natural variability and occasional errors in human\nperformance. This paper presents a clustering-based technique that aims to\nextract routine logs. Experiments were conducted on nine UI logs from the\nliterature with different levels of injected noise. Our technique was compared\nwith existing techniques, most of which are not meant to discover routine logs\nbut were adapted for the purpose. The results were evaluated through standard\nstate-of-the-art metrics, showing that we can extract more accurate routine\nlogs than what the state of the art could, especially in the presence of noise.", "authors": ["Massimiliano de Leoni", "Faizan Ahmed Khan", "Simone Agostinelli"], "categories": ["cs.RO", "cs.SE"], "published": "2025-10-09T12:02:28Z", "pdf": "https://arxiv.org/pdf/2510.08118v1", "abs": "https://arxiv.org/abs/2510.08118v1", "comment": "16 pages, 5 figures"}
{"id": "2510.08106v1", "title": "Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography", "summary": "Liver disease is a major global health burden. While ultrasound is the\nfirst-line diagnostic tool, liver sonography requires locating multiple\nnon-continuous planes from positions where target structures are often not\nvisible, for biometric assessment and lesion detection, requiring significant\nexpertise. However, expert sonographers are severely scarce in resource-limited\nregions. Here, we develop an autonomous lightweight ultrasound robot comprising\nan AI agent that integrates multi-modal perception with memory attention for\nlocalization of unseen target structures, and a 588-gram 6-degrees-of-freedom\ncable-driven robot. By mounting on the abdomen, the system enhances robustness\nagainst motion. Our robot can autonomously acquire expert-level standard liver\nultrasound planes and detect pathology in patients, including two from Xining,\na 2261-meter-altitude city with limited medical resources. Our system performs\neffectively on rapid-motion individuals and in wilderness environments. This\nwork represents the first demonstration of autonomous sonography across\nmultiple challenging scenarios, potentially transforming access to expert-level\ndiagnostics in underserved regions.", "authors": ["Zihan Li", "Yixiao Xu", "Lei Zhang", "Taiyu Han", "Xinshan Yang", "Yingni Wang", "Mingxuan Liu", "Shenghai Xin", "Linxun Liu", "Hongen Liao", "Guochen Ning"], "categories": ["cs.RO"], "published": "2025-10-09T11:43:29Z", "pdf": "https://arxiv.org/pdf/2510.08106v1", "abs": "https://arxiv.org/abs/2510.08106v1", "comment": null}
{"id": "2510.08572v1", "title": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation", "summary": "Scaling data and models has played a pivotal role in the remarkable progress\nof computer vision and language. Inspired by these domains, recent efforts in\nrobotics have similarly focused on scaling both data and model size to develop\nmore generalizable and robust policies. However, unlike vision and language,\nrobotics lacks access to internet-scale demonstrations across diverse robotic\ntasks and environments. As a result, the scale of existing datasets typically\nsuffers from the need for manual data collection and curation. To address this\nproblem, here we propose BLAZER, a framework that learns manipulation policies\nfrom automatically generated training data. We build on the zero-shot\ncapabilities of LLM planners and automatically generate demonstrations for\ndiverse manipulation tasks in simulation. Successful examples are then used to\nfinetune an LLM and to improve its planning capabilities without human\nsupervision. Notably, while BLAZER training requires access to the simulator's\nstate, we demonstrate direct transfer of acquired skills to sensor-based\nmanipulation. Through extensive experiments, we show BLAZER to significantly\nimprove zero-shot manipulation in both simulated and real environments.\nMoreover, BLAZER improves on tasks outside of its training pool and enables\ndownscaling of LLM models. Our code and data will be made publicly available on\nthe project page.", "authors": ["Rocktim Jyoti Das", "Harsh Singh", "Diana Turmakhan", "Muhammad Abdullah Sohail", "Mingfei Han", "Preslav Nakov", "Fabio Pizzati", "Ivan Laptev"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-10-09T17:59:58Z", "pdf": "https://arxiv.org/pdf/2510.08572v1", "abs": "https://arxiv.org/abs/2510.08572v1", "comment": "11 pages, 8 figures"}
{"id": "2510.08571v1", "title": "Scalable Offline Metrics for Autonomous Driving", "summary": "Real-World evaluation of perception-based planning models for robotic\nsystems, such as autonomous vehicles, can be safely and inexpensively conducted\noffline, i.e., by computing model prediction error over a pre-collected\nvalidation dataset with ground-truth annotations. However, extrapolating from\noffline model performance to online settings remains a challenge. In these\nsettings, seemingly minor errors can compound and result in test-time\ninfractions or collisions. This relationship is understudied, particularly\nacross diverse closed-loop metrics and complex urban maneuvers. In this work,\nwe revisit this undervalued question in policy evaluation through an extensive\nset of experiments across diverse conditions and metrics. Based on analysis in\nsimulation, we find an even worse correlation between offline and online\nsettings than reported by prior studies, casting doubts on the validity of\ncurrent evaluation practices and metrics for driving policies. Next, we bridge\nthe gap between offline and online evaluation. We investigate an offline metric\nbased on epistemic uncertainty, which aims to capture events that are likely to\ncause errors in closed-loop settings. The resulting metric achieves over 13%\nimprovement in correlation compared to previous offline metrics. We further\nvalidate the generalization of our findings beyond the simulation environment\nin real-world settings, where even greater gains are observed.", "authors": ["Animikh Aich", "Adwait Kulkarni", "Eshed Ohn-Bar"], "categories": ["cs.RO", "cs.CV"], "published": "2025-10-09T17:59:57Z", "pdf": "https://arxiv.org/pdf/2510.08571v1", "abs": "https://arxiv.org/abs/2510.08571v1", "comment": "Accepted at IROS 2025 (IEEE/RSJ International Conference on\n  Intelligent Robots and Systems)"}
{"id": "2510.08568v1", "title": "NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos", "summary": "Enabling robots to execute novel manipulation tasks zero-shot is a central\ngoal in robotics. Most existing methods assume in-distribution tasks or rely on\nfine-tuning with embodiment-matched data, limiting transfer across platforms.\nWe present NovaFlow, an autonomous manipulation framework that converts a task\ndescription into an actionable plan for a target robot without any\ndemonstrations. Given a task description, NovaFlow synthesizes a video using a\nvideo generation model and distills it into 3D actionable object flow using\noff-the-shelf perception modules. From the object flow, it computes relative\nposes for rigid objects and realizes them as robot actions via grasp proposals\nand trajectory optimization. For deformable objects, this flow serves as a\ntracking objective for model-based planning with a particle-based dynamics\nmodel. By decoupling task understanding from low-level control, NovaFlow\nnaturally transfers across embodiments. We validate on rigid, articulated, and\ndeformable object manipulation tasks using a table-top Franka arm and a Spot\nquadrupedal mobile robot, and achieve effective zero-shot execution without\ndemonstrations or embodiment-specific training. Project website:\nhttps://novaflow.lhy.xyz/.", "authors": ["Hongyu Li", "Lingfeng Sun", "Yafei Hu", "Duy Ta", "Jennifer Barry", "George Konidaris", "Jiahui Fu"], "categories": ["cs.RO", "cs.AI", "cs.CV"], "published": "2025-10-09T17:59:55Z", "pdf": "https://arxiv.org/pdf/2510.08568v1", "abs": "https://arxiv.org/abs/2510.08568v1", "comment": null}
{"id": "2510.08567v1", "title": "MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning", "summary": "Vision language models (VLMs) are increasingly deployed as controllers with\naccess to external tools for complex reasoning and decision-making, yet their\neffectiveness remains limited by the scarcity of high-quality multimodal\ntrajectories and the cost of manual annotation. We address this challenge with\na vision-centric agent tuning framework that automatically synthesizes\nmultimodal trajectories, generates step-wise preference pairs, and trains a VLM\ncontroller for robust tool-use reasoning. Our pipeline first constructs\nM-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified\ntrajectories, enabling imitation-based trajectory tuning. Building on this, we\ndevelop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool\nreasoning. To achieve finer alignment, we further introduce Pref-X, a set of\n11K automatically generated preference pairs, and optimize MATRIX on it via\nstep-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,\nMATRIX consistently surpasses both open- and closed-source VLMs, demonstrating\nscalable and effective multimodal tool use. Our data and code is avaliable at\nhttps://github.com/mbzuai-oryx/MATRIX.", "authors": ["Tajamul Ashraf", "Umair Nawaz", "Abdelrahman M. Shaker", "Rao Anwer", "Philip Torr", "Fahad Shahbaz Khan", "Salman Khan"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-10-09T17:59:54Z", "pdf": "https://arxiv.org/pdf/2510.08567v1", "abs": "https://arxiv.org/abs/2510.08567v1", "comment": null}
{"id": "2510.08564v1", "title": "How to Teach Large Multimodal Models New Skills", "summary": "How can we teach large multimodal models (LMMs) new skills without erasing\nprior abilities? We study sequential fine-tuning on five target skills while\nmonitoring general ability on eight held-out benchmarks across three model\nfamilies. We observe that apparent \"forgetting\" on held-out tasks after narrow\nfine-tuning can partly recover at later stages. We trace this behavior to a\nmeasurable shift in the output token distribution, manifested through a simple\ncounting-bias probe that co-varies with forgetting. Guided by this picture, we\nidentify two simple, robust tuning recipes that learn strongly while limiting\ndrift: (i) updating only the self-attention projection layers, and (ii)\nupdating only the MLP Gate&Up while freezing the Down projection. Across models\nand tasks, these choices deliver strong target gains while largely preserving\nheld-out performance. Code is available at\nhttps://github.com/jessemelpolio/LMM_CL", "authors": ["Zhen Zhu", "Yiming Gong", "Yao Xiao", "Yaoyao Liu", "Derek Hoiem"], "categories": ["cs.AI", "cs.CV", "cs.LG"], "published": "2025-10-09T17:59:37Z", "pdf": "https://arxiv.org/pdf/2510.08564v1", "abs": "https://arxiv.org/abs/2510.08564v1", "comment": "In submission. Code is available at\n  https://github.com/jessemelpolio/LMM_CL"}
{"id": "2510.08565v1", "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints", "summary": "Compositional training has been the de-facto paradigm in existing Multimodal\nLarge Language Models (MLLMs), where pre-trained vision encoders are connected\nwith pre-trained LLMs through continuous multimodal pre-training. However, the\nmultimodal scaling property of this paradigm remains difficult to explore due\nto the separated training. In this paper, we focus on the native training of\nMLLMs in an end-to-end manner and systematically study its design space and\nscaling property under a practical setting, i.e., data constraint. Through\ncareful study of various choices in MLLM, we obtain the optimal\nmeta-architecture that best balances performance and training cost. After that,\nwe further explore the scaling properties of the native MLLM and indicate the\npositively correlated scaling relationship between visual encoders and LLMs.\nBased on these findings, we propose a native MLLM called NaViL, combined with a\nsimple and cost-effective recipe. Experimental results on 14 multimodal\nbenchmarks confirm the competitive performance of NaViL against existing MLLMs.\nBesides that, our findings and results provide in-depth insights for the future\nstudy of native MLLMs.", "authors": ["Changyao Tian", "Hao Li", "Gen Luo", "Xizhou Zhu", "Weijie Su", "Hanming Deng", "Jinguo Zhu", "Jie Shao", "Ziran Zhu", "Yunpeng Liu", "Lewei Lu", "Wenhai Wang", "Hongsheng Li", "Jifeng Dai"], "categories": ["cs.CV"], "published": "2025-10-09T17:59:37Z", "pdf": "https://arxiv.org/pdf/2510.08565v1", "abs": "https://arxiv.org/abs/2510.08565v1", "comment": "Accepted by NeurIPS 2025. 22 pages, link:\n  https://github.com/OpenGVLab/NaViL"}
{"id": "2510.08559v1", "title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models", "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress across\nvarious capabilities; however, complex video reasoning in the scientific domain\nremains a significant and challenging frontier. Current video benchmarks\npredominantly target general scenarios where perception/recognition is heavily\nrelied on, while with relatively simple reasoning tasks, leading to saturation\nand thus failing to effectively evaluate advanced multimodal cognitive skills.\nTo address this critical gap, we introduce SciVideoBench, a rigorous benchmark\nspecifically designed to assess advanced video reasoning in scientific\ncontexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice\nquestions derived from cutting-edge scientific experimental videos spanning\nover 25 specialized academic subjects and verified by a semi-automatic system.\nEach question demands sophisticated domain-specific knowledge, precise\nspatiotemporal perception, and intricate logical reasoning, effectively\nchallenging models' higher-order cognitive abilities. Our evaluation highlights\nsignificant performance deficits in state-of-the-art proprietary and\nopen-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating\nsubstantial room for advancement in video reasoning capabilities. Detailed\nanalyses of critical factors such as reasoning complexity and visual grounding\nprovide valuable insights and clear direction for future developments in LMMs,\ndriving the evolution of truly capable multimodal AI co-scientists. We hope\nSciVideoBench could fit the interests of the community and help to push the\nboundary of cutting-edge AI for border science.", "authors": ["Andong Deng", "Taojiannan Yang", "Shoubin Yu", "Lincoln Spencer", "Mohit Bansal", "Chen Chen", "Serena Yeung-Levy", "Xiaohan Wang"], "categories": ["cs.CV", "cs.AI"], "published": "2025-10-09T17:59:23Z", "pdf": "https://arxiv.org/pdf/2510.08559v1", "abs": "https://arxiv.org/abs/2510.08559v1", "comment": null}
{"id": "2510.08556v1", "title": "DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model", "summary": "Achieving generalized in-hand object rotation remains a significant challenge\nin robotics, largely due to the difficulty of transferring policies from\nsimulation to the real world. The complex, contact-rich dynamics of dexterous\nmanipulation create a \"reality gap\" that has limited prior work to constrained\nscenarios involving simple geometries, limited object sizes and aspect ratios,\nconstrained wrist poses, or customized hands. We address this sim-to-real\nchallenge with a novel framework that enables a single policy, trained in\nsimulation, to generalize to a wide variety of objects and conditions in the\nreal world. The core of our method is a joint-wise dynamics model that learns\nto bridge the reality gap by effectively fitting limited amount of real-world\ncollected data and then adapting the sim policy's actions accordingly. The\nmodel is highly data-efficient and generalizable across different whole-hand\ninteraction distributions by factorizing dynamics across joints, compressing\nsystem-wide influences into low-dimensional variables, and learning each\njoint's evolution from its own dynamic profile, implicitly capturing these net\neffects. We pair this with a fully autonomous data collection strategy that\ngathers diverse, real-world interaction data with minimal human intervention.\nOur complete pipeline demonstrates unprecedented generality: a single policy\nsuccessfully rotates challenging objects with complex shapes (e.g., animals),\nhigh aspect ratios (up to 5.33), and small sizes, all while handling diverse\nwrist orientations and rotation axes. Comprehensive real-world evaluations and\na teleoperation application for complex tasks validate the effectiveness and\nrobustness of our approach. Website: https://meowuu7.github.io/DexNDM/", "authors": ["Xueyi Liu", "He Wang", "Li Yi"], "categories": ["cs.RO", "cs.CV"], "published": "2025-10-09T17:59:11Z", "pdf": "https://arxiv.org/pdf/2510.08556v1", "abs": "https://arxiv.org/abs/2510.08556v1", "comment": "Project Website: https://meowuu7.github.io/DexNDM/ Video:\n  https://youtu.be/tU2Mv8vWftU"}
{"id": "2510.08553v1", "title": "Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation", "summary": "Vision-and-Language Navigation (VLN) requires agents to follow natural\nlanguage instructions through environments, with memory-persistent variants\ndemanding progressive improvement through accumulated experience. Existing\napproaches for memory-persistent VLN face critical limitations: they lack\neffective memory access mechanisms, instead relying on entire memory\nincorporation or fixed-horizon lookup, and predominantly store only\nenvironmental observations while neglecting navigation behavioral patterns that\nencode valuable decision-making strategies. We present Memoir, which employs\nimagination as a retrieval mechanism grounded by explicit memory: a world model\nimagines future navigation states as queries to selectively retrieve relevant\nenvironmental observations and behavioral histories. The approach comprises: 1)\na language-conditioned world model that imagines future states serving dual\npurposes: encoding experiences for storage and generating retrieval queries; 2)\nHybrid Viewpoint-Level Memory that anchors both observations and behavioral\npatterns to viewpoints, enabling hybrid retrieval; and 3) an\nexperience-augmented navigation model that integrates retrieved knowledge\nthrough specialized encoders. Extensive evaluation across diverse\nmemory-persistent VLN benchmarks with 10 distinctive testing scenarios\ndemonstrates Memoir's effectiveness: significant improvements across all\nscenarios, with 5.4% SPL gains on IR2R over the best memory-persistent\nbaseline, accompanied by 8.3x training speedup and 74% inference memory\nreduction. The results validate that predictive retrieval of both environmental\nand behavioral memories enables more effective navigation, with analysis\nindicating substantial headroom (73.3% vs 93.4% upper bound) for this\nimagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.", "authors": ["Yunzhe Xu", "Yiyuan Pan", "Zhe Liu"], "categories": ["cs.CV", "cs.AI", "cs.RO"], "published": "2025-10-09T17:58:01Z", "pdf": "https://arxiv.org/pdf/2510.08553v1", "abs": "https://arxiv.org/abs/2510.08553v1", "comment": "14 pages, 6 figures, 13 tables"}
{"id": "2510.08551v1", "title": "ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation", "summary": "On-the-fly 3D reconstruction from monocular image sequences is a\nlong-standing challenge in computer vision, critical for applications such as\nreal-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff:\nper-scene optimization yields high fidelity but is computationally expensive,\nwhereas feed-forward foundation models enable real-time inference but struggle\nwith accuracy and robustness. In this work, we propose ARTDECO, a unified\nframework that combines the efficiency of feed-forward models with the\nreliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose\nestimation and point prediction, coupled with a Gaussian decoder that\ntransforms multi-scale features into structured 3D Gaussians. To sustain both\nfidelity and efficiency at scale, we design a hierarchical Gaussian\nrepresentation with a LoD-aware rendering strategy, which improves rendering\nfidelity while reducing redundancy. Experiments on eight diverse indoor and\noutdoor benchmarks show that ARTDECO delivers interactive performance\ncomparable to SLAM, robustness similar to feed-forward systems, and\nreconstruction quality close to per-scene optimization, providing a practical\npath toward on-the-fly digitization of real-world environments with both\naccurate geometry and high visual fidelity. Explore more demos on our project\npage: https://city-super.github.io/artdeco/.", "authors": ["Guanghao Li", "Kerui Ren", "Linning Xu", "Zhewen Zheng", "Changjian Jiang", "Xin Gao", "Bo Dai", "Jian Pu", "Mulin Yu", "Jiangmiao Pang"], "categories": ["cs.CV"], "published": "2025-10-09T17:57:38Z", "pdf": "https://arxiv.org/pdf/2510.08551v1", "abs": "https://arxiv.org/abs/2510.08551v1", "comment": null}
{"id": "2510.08547v1", "title": "R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation", "summary": "Towards the aim of generalized robotic manipulation, spatial generalization\nis the most fundamental capability that requires the policy to work robustly\nunder different spatial distribution of objects, environment and agent itself.\nTo achieve this, substantial human demonstrations need to be collected to cover\ndifferent spatial configurations for training a generalized visuomotor policy\nvia imitation learning. Prior works explore a promising direction that\nleverages data generation to acquire abundant spatially diverse data from\nminimal source demonstrations. However, most approaches face significant\nsim-to-real gap and are often limited to constrained settings, such as\nfixed-base scenarios and predefined camera viewpoints. In this paper, we\npropose a real-to-real 3D data generation framework (R2RGen) that directly\naugments the pointcloud observation-action pairs to generate real-world data.\nR2RGen is simulator- and rendering-free, thus being efficient and\nplug-and-play. Specifically, given a single source demonstration, we introduce\nan annotation mechanism for fine-grained parsing of scene and trajectory. A\ngroup-wise augmentation strategy is proposed to handle complex multi-object\ncompositions and diverse task constraints. We further present camera-aware\nprocessing to align the distribution of generated data with real-world 3D\nsensor. Empirically, R2RGen substantially enhances data efficiency on extensive\nexperiments and demonstrates strong potential for scaling and application on\nmobile manipulation.", "authors": ["Xiuwei Xu", "Angyuan Ma", "Hankun Li", "Bingyao Yu", "Zheng Zhu", "Jie Zhou", "Jiwen Lu"], "categories": ["cs.RO", "cs.CV"], "published": "2025-10-09T17:55:44Z", "pdf": "https://arxiv.org/pdf/2510.08547v1", "abs": "https://arxiv.org/abs/2510.08547v1", "comment": "Project page: https://r2rgen.github.io/"}
{"id": "2510.08540v1", "title": "MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization", "summary": "While current Multimodal Large Language Models (MLLMs) have demonstrated\nproficiency in reasoning tasks such as mathematics and logic, their capacity\nfor long-chain reflective reasoning, a prerequisite for solving complex\nreal-world problems, remains largely underexplored. In this work, we first\nconduct an extensive empirical investigation to evaluate this capability.\nLeveraging a carefully designed data synthesis engine, we construct MM-HELIX, a\nmultimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks\nthat require iterative thinking and backtracking. Empirical results on this\nbenchmark reveal that existing MLLMs exhibit significant performance deficits\nin long-chain reflective reasoning. To address this limitation, we generate\npost-training data and further explore learning paradigms for exploiting such\ndata. We first develop the Step-Elicited Response Generation pipeline to create\nMM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning\ntraces for instruction-tuning stage. Given that standard Reinforcement Learning\nfails on complex tasks due to sparse reward signals and catastrophic forgetting\nafter Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization\n(AHPO), a novel training strategy that dynamically unifies offline supervision\nand online optimization into a single stage. This strategy enables the model to\nlearn from expert data when rewards are sparse and conduct independent\nexploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our\nmethod achieves a +18.6\\% accuracy improvement on MM-HELIX benchmark and\ndemonstrates strong generalization with a +5.7\\% average performance gain on\ngeneral mathematic and logic tasks. Our work demonstrate that reflective\nreasoning in MLLMs can be effectively learned and generalized, paving the way\nfor developing more capable MLLMs.", "authors": ["Xiangyu Zhao", "Junming Lin", "Tianhao Liang", "Yifan Zhou", "Wenhao Chai", "Yuzhe Gu", "Weiyun Wang", "Kai Chen", "Gen Luo", "Wenwei Zhang", "Junchi Yan", "Hua Yang", "Haodong Duan", "Xue Yang"], "categories": ["cs.CV"], "published": "2025-10-09T17:53:58Z", "pdf": "https://arxiv.org/pdf/2510.08540v1", "abs": "https://arxiv.org/abs/2510.08540v1", "comment": null}
{"id": "2510.08531v1", "title": "SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models", "summary": "Spatial reasoning remains a fundamental challenge for Vision-Language Models\n(VLMs), with current approaches struggling to achieve robust performance\ndespite recent advances. We identify that this limitation stems from a critical\ngap: existing methods attempt to learn spatial reasoning directly without\nestablishing the hierarchical foundations of perception and understanding. To\naddress this challenge, we present a comprehensive methodology for building\nspatial intelligence progressively. We introduce SpatialLadder-26k, a\nmultimodal dataset containing 26,610 samples spanning object localization,\nsingle image, multi-view, and video spatial reasoning tasks, constructed\nthrough a standardized pipeline that ensures systematic coverage across\nmodalities. Building on this dataset, we design a three-stage progressive\ntraining framework that (1) establishes spatial perception through object\nlocalization, (2) develops spatial understanding through multi-dimensional\nspatial tasks, and (3) strengthens complex reasoning via reinforcement learning\nwith verifiable rewards. This approach yields SpatialLadder, a 3B-parameter\nmodel that achieves state-of-the-art performance on spatial reasoning\nbenchmarks, with 23.4% average improvement over the base model, surpassing\nGPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains\nstrong generalization with 7.2% improvement on out-of-domain benchmarks,\ndemonstrating that progressive training from perception to reasoning is\nessential for robust spatial intelligence.", "authors": ["Hongxing Li", "Dingming Li", "Zixuan Wang", "Yuchen Yan", "Hang Wu", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-10-09T17:50:54Z", "pdf": "https://arxiv.org/pdf/2510.08531v1", "abs": "https://arxiv.org/abs/2510.08531v1", "comment": "Project Page: https://zju-real.github.io/SpatialLadder/ Code:\n  https://github.com/ZJU-REAL/SpatialLadder"}
{"id": "2510.08530v1", "title": "X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering", "summary": "We present X2Video, the first diffusion model for rendering photorealistic\nvideos guided by intrinsic channels including albedo, normal, roughness,\nmetallicity, and irradiance, while supporting intuitive multi-modal controls\nwith reference images and text prompts for both global and local regions. The\nintrinsic guidance allows accurate manipulation of color, material, geometry,\nand lighting, while reference images and text prompts provide intuitive\nadjustments in the absence of intrinsic information. To enable these\nfunctionalities, we extend the intrinsic-guided image generation model XRGB to\nvideo generation by employing a novel and efficient Hybrid Self-Attention,\nwhich ensures temporal consistency across video frames and also enhances\nfidelity to reference images. We further develop a Masked Cross-Attention to\ndisentangle global and local text prompts, applying them effectively onto\nrespective local and global regions. For generating long videos, our novel\nRecursive Sampling method incorporates progressive frame sampling, combining\nkeyframe prediction and frame interpolation to maintain long-range temporal\nconsistency while preventing error accumulation. To support the training of\nX2Video, we assembled a video dataset named InteriorVideo, featuring 1,154\nrooms from 295 interior scenes, complete with reliable ground-truth intrinsic\nchannel sequences and smooth camera trajectories. Both qualitative and\nquantitative evaluations demonstrate that X2Video can produce long, temporally\nconsistent, and photorealistic videos guided by intrinsic conditions.\nAdditionally, X2Video effectively accommodates multi-modal controls with\nreference images, global and local text prompts, and simultaneously supports\nediting on color, material, geometry, and lighting through parametric tuning.\nProject page: https://luckyhzt.github.io/x2video", "authors": ["Zhitong Huang", "Mohan Zhang", "Renhan Wang", "Rui Tang", "Hao Zhu", "Jing Liao"], "categories": ["cs.GR", "cs.CV", "68U05", "I.3.3; I.3.6"], "published": "2025-10-09T17:50:31Z", "pdf": "https://arxiv.org/pdf/2510.08530v1", "abs": "https://arxiv.org/abs/2510.08530v1", "comment": "Code, model, and dataset will be released at project page soon:\n  https://luckyhzt.github.io/x2video"}
{"id": "2510.08527v1", "title": "FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control", "summary": "We present FlexTraj, a framework for image-to-video generation with flexible\npoint trajectory control. FlexTraj introduces a unified point-based motion\nrepresentation that encodes each point with a segmentation ID, a temporally\nconsistent trajectory ID, and an optional color channel for appearance cues,\nenabling both dense and sparse trajectory control. Instead of injecting\ntrajectory conditions into the video generator through token concatenation or\nControlNet, FlexTraj employs an efficient sequence-concatenation scheme that\nachieves faster convergence, stronger controllability, and more efficient\ninference, while maintaining robustness under unaligned conditions. To train\nsuch a unified point trajectory-controlled video generator, FlexTraj adopts an\nannealing training strategy that gradually reduces reliance on complete\nsupervision and aligned condition. Experimental results demonstrate that\nFlexTraj enables multi-granularity, alignment-agnostic trajectory control for\nvideo generation, supporting various applications such as motion cloning,\ndrag-based image-to-video, motion interpolation, camera redirection, flexible\naction control and mesh animations.", "authors": ["Zhiyuan Zhang", "Can Wang", "Dongdong Chen", "Jing Liao"], "categories": ["cs.CV"], "published": "2025-10-09T17:50:22Z", "pdf": "https://arxiv.org/pdf/2510.08527v1", "abs": "https://arxiv.org/abs/2510.08527v1", "comment": "Project Page: https://bestzzhang.github.io/FlexTraj"}
{"id": "2510.08525v1", "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression", "summary": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results.", "authors": ["Wenjie Du", "Li Jiang", "Keda Tao", "Xue Liu", "Huan Wang"], "categories": ["cs.CL"], "published": "2025-10-09T17:50:00Z", "pdf": "https://arxiv.org/pdf/2510.08525v1", "abs": "https://arxiv.org/abs/2510.08525v1", "comment": null}
{"id": "2510.08512v1", "title": "Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression", "summary": "Efficient transmission of 3D point cloud data is critical for advanced\nperception in centralized and decentralized multi-agent robotic systems,\nespecially nowadays with the growing reliance on edge and cloud-based\nprocessing. However, the large and complex nature of point clouds creates\nchallenges under bandwidth constraints and intermittent connectivity, often\ndegrading system performance. We propose a deep compression framework based on\nsemantic scene graphs. The method decomposes point clouds into semantically\ncoherent patches and encodes them into compact latent representations with\nsemantic-aware encoders conditioned by Feature-wise Linear Modulation (FiLM). A\nfolding-based decoder, guided by latent features and graph node attributes,\nenables structurally accurate reconstruction. Experiments on the SemanticKITTI\nand nuScenes datasets show that the framework achieves state-of-the-art\ncompression rates, reducing data size by up to 98% while preserving both\nstructural and semantic fidelity. In addition, it supports downstream\napplications such as multi-robot pose graph optimization and map merging,\nachieving trajectory accuracy and map alignment comparable to those obtained\nwith raw LiDAR scans.", "authors": ["Nikolaos Stathoulopoulos", "Christoforos Kanellakis", "George Nikolakopoulos"], "categories": ["cs.CV", "cs.RO"], "published": "2025-10-09T17:45:09Z", "pdf": "https://arxiv.org/pdf/2510.08512v1", "abs": "https://arxiv.org/abs/2510.08512v1", "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L). 8 pages, 6 figures"}
{"id": "2510.08508v1", "title": "MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration", "summary": "Real-world videos often suffer from complex degradations, such as noise,\ncompression artifacts, and low-light distortions, due to diverse acquisition\nand transmission conditions. Existing restoration methods typically require\nprofessional manual selection of specialized models or rely on monolithic\narchitectures that fail to generalize across varying degradations. Inspired by\nexpert experience, we propose MoA-VR, the first\n\\underline{M}ixture-\\underline{o}f-\\underline{A}gents \\underline{V}ideo\n\\underline{R}estoration system that mimics the reasoning and processing\nprocedures of human professionals through three coordinated agents: Degradation\nIdentification, Routing and Restoration, and Restoration Quality Assessment.\nSpecifically, we construct a large-scale and high-resolution video degradation\nrecognition benchmark and build a vision-language model (VLM) driven\ndegradation identifier. We further introduce a self-adaptive router powered by\nlarge language models (LLMs), which autonomously learns effective restoration\nstrategies by observing tool usage patterns. To assess intermediate and final\nprocessed video quality, we construct the \\underline{Res}tored\n\\underline{V}ideo \\underline{Q}uality (Res-VQ) dataset and design a dedicated\nVLM-based video quality assessment (VQA) model tailored for restoration tasks.\nExtensive experiments demonstrate that MoA-VR effectively handles diverse and\ncompound degradations, consistently outperforming existing baselines in terms\nof both objective metrics and perceptual quality. These results highlight the\npotential of integrating multimodal intelligence and modular reasoning in\ngeneral-purpose video restoration systems.", "authors": ["Lu Liu", "Chunlei Cai", "Shaocheng Shen", "Jianfeng Liang", "Weimin Ouyang", "Tianxiao Ye", "Jian Mao", "Huiyu Duan", "Jiangchao Yao", "Xiaoyun Zhang", "Qiang Hu", "Guangtao Zhai"], "categories": ["cs.CV"], "published": "2025-10-09T17:42:51Z", "pdf": "https://arxiv.org/pdf/2510.08508v1", "abs": "https://arxiv.org/abs/2510.08508v1", "comment": null}
{"id": "2510.08494v1", "title": "Quartic quantum speedups for community detection", "summary": "Community detection is a foundational problem in data science. Its natural\nextension to hypergraphs captures higher-order correlations beyond pairwise\ninteractions. In this work, we develop a quantum algorithm for hypergraph\ncommunity detection that achieves a quartic quantum speedup over the best known\nclassical algorithm, along with superpolynomial savings in space. Our algorithm\nis based on the Kikuchi method, which we extend beyond previously considered\nproblems such as Tensor PCA and $p$XORSAT to a broad family of generalized\nstochastic block models. To demonstrate (near) optimality of this method, we\nprove matching lower bounds (up to logarithmic factors) in the low-degree\nframework, showing that the algorithm saturates a smooth\nstatistical-computational tradeoff. The quantum speedup arises from a quantized\nversion of the Kikuchi method and is based on the efficient preparation of a\nguiding state correlated with the underlying community structure. Our work\nsuggests that prior quantum speedups using the Kikuchi method are sufficiently\nrobust to encompass a broader set of problems than previously believed; we\nconjecture that a quantity known as marginal order characterizes the existence\nof these quantum speedups.", "authors": ["Alexander Schmidhuber", "Alexander Zlokapa"], "categories": ["quant-ph", "cs.DS"], "published": "2025-10-09T17:35:17Z", "pdf": "https://arxiv.org/pdf/2510.08494v1", "abs": "https://arxiv.org/abs/2510.08494v1", "comment": "40 pages"}
{"id": "2510.08492v1", "title": "Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models", "summary": "Traditional multimodal learners find unified representations for tasks like\nvisual question answering, but rely heavily on paired datasets. However, an\noverlooked yet potentially powerful question is: can one leverage auxiliary\nunpaired multimodal data to directly enhance representation learning in a\ntarget modality? We introduce UML: Unpaired Multimodal Learner, a\nmodality-agnostic training paradigm in which a single model alternately\nprocesses inputs from different modalities while sharing parameters across\nthem. This design exploits the assumption that different modalities are\nprojections of a shared underlying reality, allowing the model to benefit from\ncross-modal structure without requiring explicit pairs. Theoretically, under\nlinear data-generating assumptions, we show that unpaired auxiliary data can\nyield representations strictly more informative about the data-generating\nprocess than unimodal training. Empirically, we show that using unpaired data\nfrom auxiliary modalities -- such as text, audio, or images -- consistently\nimproves downstream performance across diverse unimodal targets such as image\nand audio. Our project page: https://unpaired-multimodal.github.io/", "authors": ["Sharut Gupta", "Shobhita Sundaram", "Chenyu Wang", "Stefanie Jegelka", "Phillip Isola"], "categories": ["cs.LG", "cs.CV"], "published": "2025-10-09T17:32:23Z", "pdf": "https://arxiv.org/pdf/2510.08492v1", "abs": "https://arxiv.org/abs/2510.08492v1", "comment": "63 pages, 29 tables, and 47 figures"}
{"id": "2510.08485v1", "title": "InstructX: Towards Unified Visual Editing with MLLM Guidance", "summary": "With recent advances in Multimodal Large Language Models (MLLMs) showing\nstrong visual understanding and reasoning, interest is growing in using them to\nimprove the editing performance of diffusion models. Despite rapid progress,\nmost studies lack an in-depth analysis of MLLM design choices. Moreover, the\nintegration of MLLMs and diffusion models remains an open challenge in some\ndifficult tasks, such as video editing. In this paper, we present InstructX, a\nunified framework for image and video editing. Specifically, we conduct a\ncomprehensive study on integrating MLLMs and diffusion models for\ninstruction-driven editing across diverse tasks. Building on this study, we\nanalyze the cooperation and distinction between images and videos in unified\nmodeling. (1) We show that training on image data can lead to emergent video\nediting capabilities without explicit supervision, thereby alleviating the\nconstraints imposed by scarce video training data. (2) By incorporating\nmodality-specific MLLM features, our approach effectively unifies image and\nvideo editing tasks within a single model. Extensive experiments demonstrate\nthat our method can handle a broad range of image and video editing tasks and\nachieves state-of-the-art performance.", "authors": ["Chong Mou", "Qichao Sun", "Yanze Wu", "Pengze Zhang", "Xinghui Li", "Fulong Ye", "Songtao Zhao", "Qian He"], "categories": ["cs.CV"], "published": "2025-10-09T17:26:09Z", "pdf": "https://arxiv.org/pdf/2510.08485v1", "abs": "https://arxiv.org/abs/2510.08485v1", "comment": null}
{"id": "2510.08482v1", "title": "The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping", "summary": "Iconicity, the resemblance between linguistic form and meaning, is pervasive\nin signed languages, offering a natural testbed for visual grounding. For\nvision-language models (VLMs), the challenge is to recover such essential\nmappings from dynamic human motion rather than static context. We introduce the\n\\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts\npsycholinguistic measures to evaluate VLMs on three tasks: (i) phonological\nsign-form prediction (e.g., handshape, location), (ii) transparency (inferring\nmeaning from visual form), and (iii) graded iconicity ratings. We assess $13$\nstate-of-the-art VLMs in zero- and few-shot settings on Sign Language of the\nNetherlands and compare them to human baselines. On \\textit{phonological form\nprediction}, VLMs recover some handshape and location detail but remain below\nhuman performance; on \\textit{transparency}, they are far from human baselines;\nand only top models correlate moderately with human \\textit{iconicity ratings}.\nInterestingly, \\textit{models with stronger phonological form prediction\ncorrelate better with human iconicity judgment}, indicating shared sensitivity\nto visually grounded structure. Our findings validate these diagnostic tasks\nand motivate human-centric signals and embodied learning methods for modelling\niconicity and improving visual grounding in multimodal models.", "authors": ["Onur Keleş", "Aslı Özyürek", "Gerardo Ortega", "Kadir Gökgö", "Esam Ghaleb"], "categories": ["cs.CV", "cs.CL"], "published": "2025-10-09T17:21:59Z", "pdf": "https://arxiv.org/pdf/2510.08482v1", "abs": "https://arxiv.org/abs/2510.08482v1", "comment": null}
{"id": "2510.08480v1", "title": "Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools", "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\npotential in bridging visual and textual reasoning, yet their reliance on\ntext-centric priors often limits their ability to disentangle semantically\nsimilar actions in open-vocabulary scenarios. To address this, we propose\nVideo-STAR, a framework that harmonizes contextual sub-motion decomposition\nwith tool-augmented reinforcement learning for open-vocabulary action\nrecognition (OVAR). Unlike prior methods that treat actions as monolithic\nentities, our approach innovatively decomposes actions into discriminative\nsub-motions for fine-grained matching while dynamically invoking\ndomain-specific tools for cross-modal interleaving, thereby enabling\ncategory-specific reasoning capacity and reducing cross-modal hallucination.\nMoreover, by designing a hierarchical reward that balances tool-usage\nefficiency, sub-motion relevance, and structural coherence in reasoning, our\nmethod autonomously leverages external tools to prioritize sub-motion patterns\nwithout explicit supervision, transmitting from text-centric reasoning to\nvisually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2,\nKinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art\nperformance, outperforming existing methods in distinguishing fine-grained\nactions and handling cross-modal hallucination, validating our excellent\nrobustness and generalization.", "authors": ["Zhenlong Yuan", "Xiangyan Qu", "Chengxuan Qian", "Rui Chen", "Jing Tang", "Lei Sun", "Xiangxiang Chu", "Dapeng Zhang", "Yiwei Wang", "Yujun Cai", "Shuo Li"], "categories": ["cs.CV"], "published": "2025-10-09T17:20:44Z", "pdf": "https://arxiv.org/pdf/2510.08480v1", "abs": "https://arxiv.org/abs/2510.08480v1", "comment": null}
{"id": "2510.08475v1", "title": "DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos", "summary": "We present DexMan, an automated framework that converts human visual\ndemonstrations into bimanual dexterous manipulation skills for humanoid robots\nin simulation. Operating directly on third-person videos of humans manipulating\nrigid objects, DexMan eliminates the need for camera calibration, depth\nsensors, scanned 3D object assets, or ground-truth hand and object motion\nannotations. Unlike prior approaches that consider only simplified floating\nhands, it directly controls a humanoid robot and leverages novel contact-based\nrewards to improve policy learning from noisy hand-object poses estimated from\nin-the-wild videos.\n  DexMan achieves state-of-the-art performance in object pose estimation on the\nTACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.\nMeanwhile, its reinforcement learning policy surpasses previous methods by 19%\nin success rate on OakInk-v2. Furthermore, DexMan can generate skills from both\nreal and synthetic videos, without the need for manual data collection and\ncostly motion capture, and enabling the creation of large-scale, diverse\ndatasets for training generalist dexterous manipulation.", "authors": ["Jhen Hsieh", "Kuan-Hsun Tu", "Kuo-Han Hung", "Tsung-Wei Ke"], "categories": ["cs.RO", "cs.CV", "cs.LG"], "published": "2025-10-09T17:17:05Z", "pdf": "https://arxiv.org/pdf/2510.08475v1", "abs": "https://arxiv.org/abs/2510.08475v1", "comment": "Video results are available at:\n  https://embodiedai-ntu.github.io/dexman/index.html"}
{"id": "2510.08470v1", "title": "Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling", "summary": "Training vision-language models on cognitively-plausible amounts of data\nrequires rethinking how models integrate multimodal information. Within the\nconstraints of the Vision track for the BabyLM Challenge 2025, we propose a\nlightweight decoder-based architecture with (1) token-wise dynamic gating for\nadaptive fusion of linguistic and visual cues, (2) feature modulation and\nchannel attention to maximise the utility of limited visual information and (3)\nauxiliary contrastive objectives for visual grounding. Evaluation on five\nbenchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows\ncompetitive or superior performance to multimodal baselines. More notably, our\ndynamic gate discovers interpretable patterns without explicit supervision,\nfavouring visual cues for content words and linguistic cues for function words.\nWhile we identify limitations in the Challenge constraints, such as the\ninformation bottleneck created by global image embeddings and training\ninstability from the dataset split, our findings establish dynamic gating as a\npowerful tool for efficient multimodal learning, offering both interpretability\nand performance even under severe constraints.", "authors": ["Bianca-Mihaela Ganescu", "Suchir Salhan", "Andrew Caines", "Paula Buttery"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "published": "2025-10-09T17:10:36Z", "pdf": "https://arxiv.org/pdf/2510.08470v1", "abs": "https://arxiv.org/abs/2510.08470v1", "comment": "Accepted to the EMNLP 2025 BabyLM Workshop"}
{"id": "2510.08464v1", "title": "Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered", "summary": "Vision-Language-Action (VLA) models have advanced robotic capabilities but\nremain challenging to deploy on resource-limited hardware. Pruning has enabled\nefficient compression of large language models (LLMs), yet it is largely\nunderstudied in robotics. Surprisingly, we observe that pruning VLA models\nleads to drastic degradation and increased safety violations. We introduce\nGLUESTICK, a post-pruning recovery method that restores much of the original\nmodel's functionality while retaining sparsity benefits. Our method performs a\none-time interpolation between the dense and pruned models in weight-space to\ncompute a corrective term. This correction is used during inference by each\npruned layer to recover lost capabilities with minimal overhead. GLUESTICK\nrequires no additional training, is agnostic to the pruning algorithm, and\nintroduces a single hyperparameter that controls the tradeoff between\nefficiency and accuracy. Across diverse VLA architectures and tasks in\nmanipulation and navigation, GLUESTICK achieves competitive memory efficiency\nwhile substantially recovering success rates and reducing safety violations.\nAdditional material can be found at: https://gluestick-vla.github.io/.", "authors": ["Jason Jabbour", "Dong-Ki Kim", "Max Smith", "Jay Patrikar", "Radhika Ghosal", "Youhui Wang", "Ali Agha", "Vijay Janapa Reddi", "Shayegan Omidshafiei"], "categories": ["cs.RO", "cs.LG"], "published": "2025-10-09T17:07:30Z", "pdf": "https://arxiv.org/pdf/2510.08464v1", "abs": "https://arxiv.org/abs/2510.08464v1", "comment": null}
{"id": "2510.08457v1", "title": "ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping", "summary": "Recent advances in multimodal large reasoning models (MLRMs) have\nsubstantially improved their ability to solve complex textual and visual tasks.\nHowever, these models tend to overthink on simple problems, producing\nunnecessarily lengthy reasoning traces, while under-exploring on challenging\nones, leading to missed solutions. To address this imbalance, we propose ARES,\na unified open-source framework for adaptive reasoning that dynamically\nallocates exploration effort based on task difficulty. Our approach is\nmotivated by two key empirical findings: (i) while single-token entropy is\nnoisy, high window-entropy (HWE) tokens (token-level entropies averaged under a\nsliding window) can reliably capture reasoning-critical moments; and (ii)\nreducing HWE usage benefits easy problems, while increasing it is essential for\nsolving hard ones. Building on these insights, ARES introduces a two-stage\ntraining pipeline. In the Adaptive Cold-Start stage, we curate multimodal and\ntextual data paired with reasoning traces of length proportional to problem\ndifficulty, equipping the model with initial difficulty awareness. In the\nsecond stage, we develop Adaptive Entropy Policy Optimization (AEPO), which\nuses HWE tokens as exploration triggers to decide when to explore, and a\nhierarchical entropy reward with dynamic KL control to decide how much to\nexplore. Extensive experiments demonstrate that ARES achieves superior\nperformance and reasoning efficiency across diverse mathematical, logical, and\nmultimodal benchmarks, while closing the gap to leading commercial systems\nunder significantly lower inference costs.", "authors": ["Shuang Chen", "Yue Guo", "Yimeng Ye", "Shijue Huang", "Wenbo Hu", "Haoxi Li", "Manyuan Zhang", "Jiayu Chen", "Song Guo", "Nanyun Peng"], "categories": ["cs.CL"], "published": "2025-10-09T17:03:28Z", "pdf": "https://arxiv.org/pdf/2510.08457v1", "abs": "https://arxiv.org/abs/2510.08457v1", "comment": null}
{"id": "2510.08449v1", "title": "Hierarchical Spatial Algorithms for High-Resolution Image Quantization and Feature Extraction", "summary": "This study introduces a modular framework for spatial image processing,\nintegrating grayscale quantization, color and brightness enhancement, image\nsharpening, bidirectional transformation pipelines, and geometric feature\nextraction. A stepwise intensity transformation quantizes grayscale images into\neight discrete levels, producing a posterization effect that simplifies\nrepresentation while preserving structural detail. Color enhancement is\nachieved via histogram equalization in both RGB and YCrCb color spaces, with\nthe latter improving contrast while maintaining chrominance fidelity.\nBrightness adjustment is implemented through HSV value-channel manipulation,\nand image sharpening is performed using a 3 * 3 convolution kernel to enhance\nhigh-frequency details. A bidirectional transformation pipeline that integrates\nunsharp masking, gamma correction, and noise amplification achieved accuracy\nlevels of 76.10% and 74.80% for the forward and reverse processes,\nrespectively. Geometric feature extraction employed Canny edge detection,\nHough-based line estimation (e.g., 51.50{\\deg} for billiard cue alignment),\nHarris corner detection, and morphological window localization. Cue isolation\nfurther yielded 81.87\\% similarity against ground truth images. Experimental\nevaluation across diverse datasets demonstrates robust and deterministic\nperformance, highlighting its potential for real-time image analysis and\ncomputer vision.", "authors": ["Noor Islam S. Mohammad"], "categories": ["cs.CV", "68T45, 68U10", "I.4.8; I.2.10"], "published": "2025-10-09T16:56:24Z", "pdf": "https://arxiv.org/pdf/2510.08449v1", "abs": "https://arxiv.org/abs/2510.08449v1", "comment": "There are 14 pages journal paper"}
{"id": "2510.08419v1", "title": "Continuous Variable Hamiltonian Learning at Heisenberg Limit via Displacement-Random Unitary Transformation", "summary": "Characterizing the Hamiltonians of continuous-variable (CV) quantum systems\nis a fundamental challenge laden with difficulties arising from\ninfinite-dimensional Hilbert spaces and unbounded operators. Existing protocols\nfor achieving the Heisenberg limit precision are often restricted to specific\nHamiltonian structures or demand experimentally challenging resources. In this\nwork, we introduce an efficient and experimentally accessible protocol, the\nDisplacement-Random Unitary Transformation (D-RUT), that learns the\ncoefficients of general, arbitrary finite-order bosonic Hamiltonians with a\ntotal evolution time scaling as $O(1/\\epsilon)$ for a target precision\n$\\epsilon$ robust to SPAM error. For multi-mode systems, we develop a\nhierarchical coefficients recovering strategy with superior statistical\nefficiency. Furthermore, we extend our protocol to first quantization, enabling\nthe learning of fundamental physical parameters from Hamiltonians expressed in\nposition and momentum operators at the Heisenberg limit.", "authors": ["Xi Huang", "Lixing Zhang", "Di Luo"], "categories": ["quant-ph"], "published": "2025-10-09T16:37:47Z", "pdf": "https://arxiv.org/pdf/2510.08419v1", "abs": "https://arxiv.org/abs/2510.08419v1", "comment": null}
{"id": "2510.08381v1", "title": "Airy: Reading Robot Intent through Height and Sky", "summary": "As industrial robots move into shared human spaces, their opaque decision\nmaking threatens safety, trust, and public oversight. This artwork, Airy, asks\nwhether complex multi agent AI can become intuitively understandable by staging\na competition between two reinforcement trained robot arms that snap a bedsheet\nskyward. Building on three design principles, competition as a clear metric\n(who lifts higher), embodied familiarity (audiences recognize fabric snapping),\nand sensor to sense mapping (robot cooperation or rivalry shown through forest\nand weather projections), the installation gives viewers a visceral way to read\nmachine intent. Observations from five international exhibitions indicate that\naudiences consistently read the robots' strategies, conflict, and cooperation\nin real time, with emotional reactions that mirror the system's internal state.\nThe project shows how sensory metaphors can turn a black box into a public\ninterface.", "authors": ["Baoyang Chen", "Xian Xu", "Huamin Qu"], "categories": ["cs.RO", "cs.AI"], "published": "2025-10-09T16:07:30Z", "pdf": "https://arxiv.org/pdf/2510.08381v1", "abs": "https://arxiv.org/abs/2510.08381v1", "comment": null}
{"id": "2510.08377v1", "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos", "summary": "Unified multimodal models have shown promising results in multimodal content\ngeneration and editing but remain largely limited to the image domain. In this\nwork, we present UniVideo, a versatile framework that extends unified modeling\nto the video domain. UniVideo adopts a dual-stream design, combining a\nMultimodal Large Language Model (MLLM) for instruction understanding with a\nMultimodal DiT (MMDiT) for video generation. This design enables accurate\ninterpretation of complex multimodal instructions while preserving visual\nconsistency. Built on this architecture, UniVideo unifies diverse video\ngeneration and editing tasks under a single multimodal instruction paradigm and\nis jointly trained across them. Extensive experiments demonstrate that UniVideo\nmatches or surpasses state-of-the-art task-specific baselines in\ntext/image-to-video generation, in-context video generation and in-context\nvideo editing. Notably, the unified design of UniVideo enables two forms of\ngeneralization. First, UniVideo supports task composition, such as combining\nediting with style transfer, by integrating multiple capabilities within a\nsingle instruction. Second, even without explicit training on free-form video\nediting, UniVideo transfers its editing capability from large-scale image\nediting data to this setting, handling unseen instructions such as\ngreen-screening characters or changing materials within a video. Beyond these\ncore capabilities, UniVideo also supports visual-prompt-based video generation,\nwhere the MLLM interprets visual prompts and guides the MMDiT during synthesis.\nTo foster future research, we will release our model and code.", "authors": ["Cong Wei", "Quande Liu", "Zixuan Ye", "Qiulin Wang", "Xintao Wang", "Pengfei Wan", "Kun Gai", "Wenhu Chen"], "categories": ["cs.CV"], "published": "2025-10-09T16:01:30Z", "pdf": "https://arxiv.org/pdf/2510.08377v1", "abs": "https://arxiv.org/abs/2510.08377v1", "comment": "Project Website https://congwei1230.github.io/UniVideo/"}
{"id": "2510.08368v1", "title": "Co-design is powerful and not free", "summary": "Robotic performance emerges from the coupling of body and controller, yet it\nremains unclear when morphology-control co-design is necessary. We present a\nunified framework that embeds morphology and control parameters within a single\nneural network, enabling end-to-end joint optimization. Through case studies in\nstatic-obstacle-constrained reaching, we evaluate trajectory error, success\nrate, and collision probability. The results show that co-design provides clear\nbenefits when morphology is poorly matched to the task, such as near obstacles\nor workspace boundaries, where structural adaptation simplifies control.\nConversely, when the baseline morphology already affords sufficient capability,\ncontrol-only optimization often matches or exceeds co-design. By clarifying\nwhen control is enough and when it is not, this work advances the understanding\nof embodied intelligence and offers practical guidance for embodiment-aware\nrobot design.", "authors": ["Yi Zhang", "Yue Xie", "Tao Sun", "Fumiya Iida"], "categories": ["cs.NE", "cs.RO"], "published": "2025-10-09T15:52:48Z", "pdf": "https://arxiv.org/pdf/2510.08368v1", "abs": "https://arxiv.org/abs/2510.08368v1", "comment": null}
{"id": "2510.08366v1", "title": "A data fusion approach for mobility hub impact assessment and location selection: integrating hub usage data into a large-scale mode choice model", "summary": "As cities grapple with traffic congestion and service inequities, mobility\nhubs offer a scalable solution to align increasing travel demand with\nsustainability goals. However, evaluating their impacts remains challenging due\nto the lack of behavioral models that integrate large-scale travel patterns\nwith real-world hub usage. This study presents a novel data fusion approach\nthat incorporates observed mobility hub usage into a mode choice model\nestimated with synthetic trip data. We identify trips potentially affected by\nmobility hubs and construct a multimodal sub-choice set, then calibrate\nhub-specific parameters using on-site survey data and ground truth trip counts.\nThe enhanced model is used to evaluate mobility hub impacts on potential\ndemand, mode shift, reduced vehicle miles traveled (VMT), and increased\nconsumer surplus (CS). We apply this method to a case study in the Capital\nDistrict, NY, using data from a survey conducted by the Capital District\nTransportation Authority (CDTA) and a mode choice model estimated using Replica\nInc. synthetic data. The two implemented hubs located near UAlbany Downtown\nCampus and in Downtown Cohoes are projected to generate 8.83 and 6.17\nmultimodal trips per day, reduce annual VMT by 20.37 and 13.16 thousand miles,\nand increase daily CS by $4,000 and $1,742, respectively. An evaluation of\npotential hub candidates in the Albany-Schenectady-Troy metropolitan area with\nthe estimated models demonstrates that hubs located along intercity corridors\nand at urban peripheries, supporting park-and-ride P+R patterns, yield the most\nsignificant behavioral impacts.", "authors": ["Xiyuan Ren", "Joseph Y. J. Chow"], "categories": ["econ.GN", "q-fin.EC"], "published": "2025-10-09T15:51:22Z", "pdf": "https://arxiv.org/pdf/2510.08366v1", "abs": "https://arxiv.org/abs/2510.08366v1", "comment": null}
{"id": "2510.08359v1", "title": "Doubly Robust Estimation with Stabilized Weights for Binary Proximal Outcomes in Micro-Randomized Trials", "summary": "Micro-randomized trials (MRTs) are increasingly used to evaluate mobile\nhealth interventions with binary proximal outcomes. Standard inverse\nprobability weighting (IPW) estimators are unbiased but unstable in small\nsamples or under extreme randomization. Estimated mean excursion effect (EMEE)\nimproves efficiency but lacks double robustness. We propose a doubly robust\nEMEE (DR-EMEE) with stabilized and truncated weights, combining per-decision\nIPW and outcome regression. We prove double robustness, asymptotic efficiency,\nand provide finite-sample variance corrections, with extensions to machine\nlearning nuisance estimators. In simulations, DR-EMEE reduces root mean squared\nerror, improves coverage, and achieves up to twofold efficiency gains over IPW\nand five to ten percent over EMEE. Applications to HeartSteps, PAMAP2, and\nmHealth datasets confirm stable and efficient inference across both randomized\nand observational settings.", "authors": ["Jinho Cha", "Eunchan Cha"], "categories": ["stat.ME", "math.ST", "stat.TH"], "published": "2025-10-09T15:44:16Z", "pdf": "https://arxiv.org/pdf/2510.08359v1", "abs": "https://arxiv.org/abs/2510.08359v1", "comment": "32 pages, 7 figures, planned to submit to Biostatistics"}
{"id": "2510.08321v1", "title": "Quantum variance and fluctuations for Walsh-quantized baker's maps", "summary": "The Walsh-quantized baker's maps are models for quantum chaos on the torus.\nWe show that for all baker's map scaling factors $D\\ge2$ except for $D=4$,\ntypically (in the sense of Haar measure on the eigenspaces, which are\ndegenerate) the empirical distribution of the scaled matrix element\nfluctuations $\\sqrt{N}\\{\\langle\n\\varphi^{(j)}|\\operatorname{Op}_{k,\\ell}(a)|\\varphi^{(j)}\\rangle-\\int_{\\mathbb{T}^2}a\\}_{j=1}^{N}$\nfor a random eigenbasis $\\{\\varphi^{(j)}\\}_{j=1}^{N}$ is asymptotically\nGaussian in the semiclassical limit $N\\to\\infty$, with variance given in terms\nof classical baker's map correlations. This determines the precise rate of\nconvergence in the quantum ergodic theorem for these eigenbases. We obtain a\nversion of the Eigenstate Thermalization Hypothesis (ETH) for these\neigenstates, including a limiting complex Gaussian distribution for the\noff-diagonal matrix elements, with variances also given in terms of classical\ncorrelations. The presence of the classical correlations highlights that these\neigenstates, while random, have microscopic correlations that differentiate\nthem from Haar random vectors. For the single value $D=4$, the Gaussianity of\nthe matrix element fluctuations depends on the values of the classical\nobservable on a fractal subset of the torus.", "authors": ["Laura Shou"], "categories": ["math-ph", "math.MP", "math.PR", "nlin.CD", "quant-ph"], "published": "2025-10-09T15:08:29Z", "pdf": "https://arxiv.org/pdf/2510.08321v1", "abs": "https://arxiv.org/abs/2510.08321v1", "comment": "48 pages"}
{"id": "2510.08316v1", "title": "Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge", "summary": "Affordance segmentation aims to parse 3D objects into functionally distinct\nparts, bridging recognition and interaction for applications in robotic\nmanipulation, embodied AI, and AR. While recent studies leverage visual or\ntextual prompts to guide this process, they often rely on point cloud encoders\nas generic feature extractors, overlooking the intrinsic challenges of 3D data\nsuch as sparsity, noise, and geometric ambiguity. As a result, 3D features\nlearned in isolation frequently lack clear and semantically consistent\nfunctional boundaries. To address this bottleneck, we propose a\nsemantic-grounded learning paradigm that transfers rich semantic knowledge from\nlarge-scale 2D Vision Foundation Models (VFMs) into the 3D domain.\nSpecifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-training\nstrategy that aligns a 3D encoder with lifted 2D semantics and jointly\noptimizes reconstruction, affinity, and diversity to yield semantically\norganized representations. Building on this backbone, we further design the\nCross-modal Affordance Segmentation Transformer (CAST), which integrates\nmulti-modal prompts with CMAT-pretrained features to generate precise,\nprompt-aware segmentation maps. Extensive experiments on standard benchmarks\ndemonstrate that our framework establishes new state-of-the-art results for 3D\naffordance segmentation.", "authors": ["Yu Huang", "Zelin Peng", "Changsong Wen", "Xiaokang Yang", "Wei Shen"], "categories": ["cs.CV"], "published": "2025-10-09T15:01:26Z", "pdf": "https://arxiv.org/pdf/2510.08316v1", "abs": "https://arxiv.org/abs/2510.08316v1", "comment": "Work in process"}
{"id": "2510.08304v1", "title": "Bayesian Profile Regression with Linear Mixed Models (Profile-LMM) applied to Longitudinal Exposome Data", "summary": "Exposure to diverse non-genetic factors, known as the exposome, is a critical\ndeterminant of health outcomes. However, analyzing the exposome presents\nsignificant methodological challenges, including: high collinearity among\nexposures, the longitudinal nature of repeated measurements, and potential\ncomplex interactions with individual characteristics. In this paper, we address\nthese challenges by proposing a novel statistical framework that extends\nBayesian profile regression. Our method integrates profile regression, which\nhandles collinearity by clustering exposures into latent profiles, into a\nlinear mixed model (LMM), a framework for longitudinal data analysis. This\nprofile-LMM approach effectively accounts for within-person variability over\ntime while also incorporating interactions between the latent exposure clusters\nand individual characteristics. We validate our method using simulated data,\ndemonstrating its ability to accurately identify model parameters and recover\nthe true latent exposure cluster structure. Finally, we apply this approach to\na large longitudinal data set from the Lifelines cohort to identify\ncombinations of exposures that are significantly associated with diastolic\nblood pressure.", "authors": ["Matteo Amestoy", "Mark van de Wiel", "Jeroen Lakerveld", "Wessel van Wieringen"], "categories": ["stat.ME"], "published": "2025-10-09T14:55:31Z", "pdf": "https://arxiv.org/pdf/2510.08304v1", "abs": "https://arxiv.org/abs/2510.08304v1", "comment": null}
{"id": "2510.08278v1", "title": "A Multimodal Depth-Aware Method For Embodied Reference Understanding", "summary": "Embodied Reference Understanding requires identifying a target object in a\nvisual scene based on both language instructions and pointing cues. While prior\nworks have shown progress in open-vocabulary object detection, they often fail\nin ambiguous scenarios where multiple candidate objects exist in the scene. To\naddress these challenges, we propose a novel ERU framework that jointly\nleverages LLM-based data augmentation, depth-map modality, and a depth-aware\ndecision module. This design enables robust integration of linguistic and\nembodied cues, improving disambiguation in complex or cluttered environments.\nExperimental results on two datasets demonstrate that our approach\nsignificantly outperforms existing baselines, achieving more accurate and\nreliable referent detection.", "authors": ["Fevziye Irem Eyiokur", "Dogucan Yaman", "Hazım Kemal Ekenel", "Alexander Waibel"], "categories": ["cs.CV", "cs.HC", "cs.RO"], "published": "2025-10-09T14:32:21Z", "pdf": "https://arxiv.org/pdf/2510.08278v1", "abs": "https://arxiv.org/abs/2510.08278v1", "comment": null}
{"id": "2510.08270v1", "title": "Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots", "summary": "This study evaluates the performance of classical and modern control methods\nfor real-world Cable-Driven Parallel Robots (CDPRs), focusing on\nunderconstrained systems with limited time discretization. A comparative\nanalysis is conducted between classical PID controllers and modern\nreinforcement learning algorithms, including Deep Deterministic Policy Gradient\n(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy\nOptimization (TRPO). The results demonstrate that TRPO outperforms other\nmethods, achieving the lowest root mean square (RMS) errors across various\ntrajectories and exhibiting robustness to larger time intervals between control\nupdates. TRPO's ability to balance exploration and exploitation enables stable\ncontrol in noisy, real-world environments, reducing reliance on high-frequency\nsensor feedback and computational demands. These findings highlight TRPO's\npotential as a robust solution for complex robotic control tasks, with\nimplications for dynamic environments and future applications in sensor fusion\nor hybrid control strategies.", "authors": ["Damir Nurtdinov", "Aliaksei Korshuk", "Alexei Kornaev", "Alexander Maloletov"], "categories": ["cs.RO"], "published": "2025-10-09T14:28:58Z", "pdf": "https://arxiv.org/pdf/2510.08270v1", "abs": "https://arxiv.org/abs/2510.08270v1", "comment": null}
{"id": "2510.08238v1", "title": "Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness", "summary": "The rapid deployment of large language model (LLM)-based agents in real-world\napplications has raised serious concerns about their trustworthiness. In this\nwork, we reveal the security and robustness vulnerabilities of these agents\nthrough backdoor attacks. Distinct from traditional backdoors limited to\nsingle-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a\nmulti-step backdoor attack designed for long-horizon agentic control. CoTri\nrelies on an ordered sequence. It starts with an initial trigger, and\nsubsequent ones are drawn from the environment, allowing multi-step\nmanipulation that diverts the agent from its intended task. Experimental\nresults show that CoTri achieves a near-perfect attack success rate (ASR) while\nmaintaining a near-zero false trigger rate (FTR). Due to training data modeling\nthe stochastic nature of the environment, the implantation of CoTri\nparadoxically enhances the agent's performance on benign tasks and even\nimproves its robustness against environmental distractions. We further validate\nCoTri on vision-language models (VLMs), confirming its scalability to\nmultimodal agents. Our work highlights that CoTri achieves stable, multi-step\ncontrol within agents, improving their inherent robustness and task\ncapabilities, which ultimately makes the attack more stealthy and raises\npotential safty risks.", "authors": ["Jiyang Qiu", "Xinbei Ma", "Yunqing Xu", "Zhuosheng Zhang", "Hai Zhao"], "categories": ["cs.AI"], "published": "2025-10-09T14:01:43Z", "pdf": "https://arxiv.org/pdf/2510.08238v1", "abs": "https://arxiv.org/abs/2510.08238v1", "comment": null}
{"id": "2510.08227v1", "title": "Practicing a Second Language Without Fear: Mixed Reality Agents for Interactive Group Conversation", "summary": "Developing speaking proficiency in a second language can be cognitively\ndemanding and emotionally taxing, often triggering fear of making mistakes or\nbeing excluded from larger groups. While current learning tools show promise\nfor speaking practice, most focus on dyadic, scripted scenarios, limiting\nopportunities for dynamic group interactions. To address this gap, we present\nConversAR, a Mixed Reality system that leverages Generative AI and XR to\nsupport situated and personalized group conversations. It integrates embodied\nAI agents, scene recognition, and generative 3D props anchored to real-world\nsurroundings. Based on a formative study with experts in language acquisition,\nwe developed and tested this system with a user study with 21 second-language\nlearners. Results indicate that the system enhanced learner engagement,\nincreased willingness to communicate, and offered a safe space for speaking. We\ndiscuss the implications for integrating Generative AI and XR into the design\nof future language learning applications.", "authors": ["Mariana Fernandez-Espinosa", "Kai Zhang", "Jad Bendarkawi", "Ashley Ponce", "Sean Chidozie Mata", "Aminah Aliu", "Lei Zhang", "Francisco Fernandez Medina", "Elena Mangione-Lora", "Andres Monroy-Hernandez", "Diego Gomez-Zara"], "categories": ["cs.HC"], "published": "2025-10-09T13:50:42Z", "pdf": "https://arxiv.org/pdf/2510.08227v1", "abs": "https://arxiv.org/abs/2510.08227v1", "comment": "22 pages"}
{"id": "2510.08218v1", "title": "Expressive Value Learning for Scalable Offline Reinforcement Learning", "summary": "Reinforcement learning (RL) is a powerful paradigm for learning to make\nsequences of decisions. However, RL has yet to be fully leveraged in robotics,\nprincipally due to its lack of scalability. Offline RL offers a promising\navenue by training agents on large, diverse datasets, avoiding the costly\nreal-world interactions of online RL. Scaling offline RL to increasingly\ncomplex datasets requires expressive generative models such as diffusion and\nflow matching. However, existing methods typically depend on either\nbackpropagation through time (BPTT), which is computationally prohibitive, or\npolicy distillation, which introduces compounding errors and limits scalability\nto larger base policies. In this paper, we consider the question of how to\ndevelop a scalable offline RL approach without relying on distillation or\nbackpropagation through time. We introduce Expressive Value Learning for\nOffline Reinforcement Learning (EVOR): a scalable offline RL approach that\nintegrates both expressive policies and expressive value functions. EVOR learns\nan optimal, regularized Q-function via flow matching during training. At\ninference-time, EVOR performs inference-time policy extraction via rejection\nsampling against the expressive value function, enabling efficient\noptimization, regularization, and compute-scalable search without retraining.\nEmpirically, we show that EVOR outperforms baselines on a diverse set of\noffline RL tasks, demonstrating the benefit of integrating expressive value\nlearning into offline RL.", "authors": ["Nicolas Espinosa-Dice", "Kiante Brantley", "Wen Sun"], "categories": ["cs.LG", "cs.AI", "I.2.6"], "published": "2025-10-09T13:42:20Z", "pdf": "https://arxiv.org/pdf/2510.08218v1", "abs": "https://arxiv.org/abs/2510.08218v1", "comment": "24 pages, 5 figures"}
{"id": "2510.08202v1", "title": "Sentiment Matters: An Analysis of 200 Human-SAV Interactions", "summary": "Shared Autonomous Vehicles (SAVs) are likely to become an important part of\nthe transportation system, making effective human-SAV interactions an important\narea of research. This paper introduces a dataset of 200 human-SAV interactions\nto further this area of study. We present an open-source human-SAV\nconversational dataset, comprising both textual data (e.g., 2,136 human-SAV\nexchanges) and empirical data (e.g., post-interaction survey results on a range\nof psychological factors). The dataset's utility is demonstrated through two\nbenchmark case studies: First, using random forest modeling and chord diagrams,\nwe identify key predictors of SAV acceptance and perceived service quality,\nhighlighting the critical influence of response sentiment polarity (i.e.,\nperceived positivity). Second, we benchmark the performance of an LLM-based\nsentiment analysis tool against the traditional lexicon-based TextBlob method.\nResults indicate that even simple zero-shot LLM prompts more closely align with\nuser-reported sentiment, though limitations remain. This study provides novel\ninsights for designing conversational SAV interfaces and establishes a\nfoundation for further exploration into advanced sentiment modeling, adaptive\nuser interactions, and multimodal conversational systems.", "authors": ["Lirui Guo", "Michael G. Burke", "Wynita M. Griggs"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.ET"], "published": "2025-10-09T13:30:23Z", "pdf": "https://arxiv.org/pdf/2510.08202v1", "abs": "https://arxiv.org/abs/2510.08202v1", "comment": "Accepted for presentation at IEEE ITSC 2025 and for publication in\n  its Proceedings. \\c{opyright} 2025 IEEE. Personal use permitted; other uses\n  require permission from IEEE, including reprinting, republishing, or reuse of\n  any copyrighted component of this work"}
{"id": "2510.08186v1", "title": "Multimodal Topological Textures Arising from Coupled Structural Orders in SrTiO$_3$", "summary": "Magnetic spin topological textures recently found their electrical\ncounterparts in polar topologies emerging from the condensation of\ninhomogeneous polar atomic distortions. Here, we further extend the concept to\nother non-polar atomic degrees of freedom. Taking SrTiO$_3$ as a prototypical\nexample, we investigate from second-principles atomistic simulations, the\nequilibrium domain structures and topological textures associated with the\nnatural antiferrodistortive rotations of its oxygen octahedra. % Besides the\ncommon 90$^\\circ$ antiferrodistortive domain walls (twin boundaries), we\nidentify new metastable 180$^\\circ$ domain walls oriented along the\n$\\lbrace100\\rbrace_\\mathrm{pc}$ direction, when compressive epitaxial strain is\napplied. These domains exhibit complex antiferrodistortive Bloch- and\nN\\'eel-like configurations with the later being the most favorable. We also\nstabilize antiferrodistortive vortex and antivortex structures which are\naccompanied by co-localized polarization vortices and a complex pattern of the\nlocal strain field, giving rise to a trimodal topological structures. Our\nresults extends the concept of topological ordering to non-polar structural\ndegrees of freedom and highlights the role of lattice-mediated couplings in\nstabilizing complex textures in perovskite oxides.", "authors": ["Fernando Gómez-Ortiz", "Louis Bastogne", "Philippe Ghosez"], "categories": ["cond-mat.mtrl-sci"], "published": "2025-10-09T13:14:07Z", "pdf": "https://arxiv.org/pdf/2510.08186v1", "abs": "https://arxiv.org/abs/2510.08186v1", "comment": null}
{"id": "2510.08176v1", "title": "Leveraging Whisper Embeddings for Audio-based Lyrics Matching", "summary": "Audio-based lyrics matching can be an appealing alternative to other\ncontent-based retrieval approaches, but existing methods often suffer from\nlimited reproducibility and inconsistent baselines. In this work, we introduce\nWEALY, a fully reproducible pipeline that leverages Whisper decoder embeddings\nfor lyrics matching tasks. WEALY establishes robust and transparent baselines,\nwhile also exploring multimodal extensions that integrate textual and acoustic\nfeatures. Through extensive experiments on standard datasets, we demonstrate\nthat WEALY achieves a performance comparable to state-of-the-art methods that\nlack reproducibility. In addition, we provide ablation studies and analyses on\nlanguage robustness, loss functions, and embedding strategies. This work\ncontributes a reliable benchmark for future research, and underscores the\npotential of speech technologies for music information retrieval tasks.", "authors": ["Eleonora Mancini", "Joan Serrà", "Paolo Torroni", "Yuki Mitsufuji"], "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "published": "2025-10-09T13:03:34Z", "pdf": "https://arxiv.org/pdf/2510.08176v1", "abs": "https://arxiv.org/abs/2510.08176v1", "comment": null}
{"id": "2510.08173v1", "title": "NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions", "summary": "Instruction-following navigation is a key step toward embodied intelligence.\nPrior benchmarks mainly focus on semantic understanding but overlook\nsystematically evaluating navigation agents' spatial perception and reasoning\ncapabilities. In this work, we introduce the NavSpace benchmark, which contains\nsix task categories and 1,228 trajectory-instruction pairs designed to probe\nthe spatial intelligence of navigation agents. On this benchmark, we\ncomprehensively evaluate 22 navigation agents, including state-of-the-art\nnavigation models and multimodal large language models. The evaluation results\nlift the veil on spatial intelligence in embodied navigation. Furthermore, we\npropose SNav, a new spatially intelligent navigation model. SNav outperforms\nexisting navigation agents on NavSpace and real robot tests, establishing a\nstrong baseline for future work.", "authors": ["Haolin Yang", "Yuxing Long", "Zhuoyuan Yu", "Zihan Yang", "Minghan Wang", "Jiapeng Xu", "Yihan Wang", "Ziyan Yu", "Wenzhe Cai", "Lei Kang", "Hao Dong"], "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "published": "2025-10-09T12:59:19Z", "pdf": "https://arxiv.org/pdf/2510.08173v1", "abs": "https://arxiv.org/abs/2510.08173v1", "comment": null}
{"id": "2510.08163v1", "title": "ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code", "summary": "Large Reasoning Models (LRMs) often suffer from the ``over-thinking''\nproblem, generating unnecessarily long reasoning on simple tasks. Some\nstrategies have been proposed to mitigate this issue, such as length penalties\nor routing mechanisms, but they are typically heuristic and task-specific,\nlacking a general framework for adaptive reasoning. In this paper, we present\nARM2, a unified model that adaptively balances reasoning performance and\nefficiency across multiple formats through a reinforcement learning framework\naugmented with length-aware optimization. Beyond conventional natural language\ninference, ARM2 integrates vision understanding, extending its applicability to\nmultimodal. Moreover, ARM2 integrates executable code into reasoning, enabling\nsubstantial reductions in token cost while preserving task performance compared\nto long CoT. Experiments demonstrate that ARM2 achieves performance on par with\ntraditional reasoning models trained with GRPO, while reducing token usage by\nover 70% on average. We further conduct extensive analyses to validate the\neffectiveness of ARM2 and the soundness of its design.", "authors": ["Jian Xie", "Zhendong Chu", "Aoxiao Zhong", "Kai Zhang", "Mingzhe Han", "Xin Fang", "Jialie Shen", "Qingsong Wen"], "categories": ["cs.CL"], "published": "2025-10-09T12:49:34Z", "pdf": "https://arxiv.org/pdf/2510.08163v1", "abs": "https://arxiv.org/abs/2510.08163v1", "comment": "Work in Progress"}
{"id": "2510.08157v1", "title": "Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing", "summary": "Image editing with natural language has gained significant popularity, yet\nexisting methods struggle with intricate object intersections and fine-grained\nspatial relationships due to the lack of an explicit reasoning process. While\nChain-of-Thought (CoT) has been explored to enhance reasoning, purely textual\nCoT or CoT augmented with coordinate information is fundamentally limited in\nits ability to represent intricate visual layouts and lacks the necessary\nvisual cues to guide the generation of fine-grained, pixel-level details. To\naddress these challenges, we propose Multimodal Reasoning Edit (MURE), a novel\nframework that shifts the visual editing process from purely text-based\nreasoning to a series of interleaved textual and visual rationales. Our\nframework performs image editing using a natively multimodal, interleaved\ntext-image CoT. This approach generates a step-by-step chain of reasoning where\na textual description is followed by a corresponding visual cue, such as a\npositional mask that defined intended edited regions or a representation of new\ncontent. Furthermore, to mitigate the hallucination phenomenon of large\nlanguage models, we introduce Multimodal Deep Confidence (MMDC) reasoning\nparadigm. This paradigm explores a tree of visual reasoning paths at each step.\nBy pruning low-quality branches using a deep confidence score from a reward\nmodel, it ensures the model consistently follows a high-quality trajectory\ntowards the final edited result. The proposed method decomposes complex editing\ntasks into interdependent sub-tasks, achieving greater precision at each stage\nand yielding high-fidelity edited results. We define the formulation for\ninterleaved text-image chains and release the first CoT-Edit-14K dataset,\ncomprising 14K high-quality editing examples. Extensive experiments show that\nour method yields significant improvements across three image editing\nbenchmarks.", "authors": ["Zhentao Zou", "Zhengrong Yue", "Kunpeng Du", "Binlei Bao", "Hanting Li", "Haizhen Xie", "Guozheng Xu", "Yue Zhou", "Yali Wang", "Jie Hu", "Xue Jiang", "Xinghao Chen"], "categories": ["cs.CV"], "published": "2025-10-09T12:36:51Z", "pdf": "https://arxiv.org/pdf/2510.08157v1", "abs": "https://arxiv.org/abs/2510.08157v1", "comment": "25pages,20figures"}
{"id": "2510.08118v1", "title": "Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)", "summary": "Robotic Process Mining focuses on the identification of the routine types\nperformed by human resources through a User Interface. The ultimate goal is to\ndiscover routine-type models to enable robotic process automation. The\ndiscovery of routine-type models requires the provision of a routine log.\nUnfortunately, the vast majority of existing works do not directly focus on\nenabling the model discovery, limiting themselves to extracting the set of\nactions that are part of the routines. They were also not evaluated in\nscenarios characterized by inconsistent routine execution, hereafter referred\nto as noise, which reflects natural variability and occasional errors in human\nperformance. This paper presents a clustering-based technique that aims to\nextract routine logs. Experiments were conducted on nine UI logs from the\nliterature with different levels of injected noise. Our technique was compared\nwith existing techniques, most of which are not meant to discover routine logs\nbut were adapted for the purpose. The results were evaluated through standard\nstate-of-the-art metrics, showing that we can extract more accurate routine\nlogs than what the state of the art could, especially in the presence of noise.", "authors": ["Massimiliano de Leoni", "Faizan Ahmed Khan", "Simone Agostinelli"], "categories": ["cs.RO", "cs.SE"], "published": "2025-10-09T12:02:28Z", "pdf": "https://arxiv.org/pdf/2510.08118v1", "abs": "https://arxiv.org/abs/2510.08118v1", "comment": "16 pages, 5 figures"}
{"id": "2510.08106v1", "title": "Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography", "summary": "Liver disease is a major global health burden. While ultrasound is the\nfirst-line diagnostic tool, liver sonography requires locating multiple\nnon-continuous planes from positions where target structures are often not\nvisible, for biometric assessment and lesion detection, requiring significant\nexpertise. However, expert sonographers are severely scarce in resource-limited\nregions. Here, we develop an autonomous lightweight ultrasound robot comprising\nan AI agent that integrates multi-modal perception with memory attention for\nlocalization of unseen target structures, and a 588-gram 6-degrees-of-freedom\ncable-driven robot. By mounting on the abdomen, the system enhances robustness\nagainst motion. Our robot can autonomously acquire expert-level standard liver\nultrasound planes and detect pathology in patients, including two from Xining,\na 2261-meter-altitude city with limited medical resources. Our system performs\neffectively on rapid-motion individuals and in wilderness environments. This\nwork represents the first demonstration of autonomous sonography across\nmultiple challenging scenarios, potentially transforming access to expert-level\ndiagnostics in underserved regions.", "authors": ["Zihan Li", "Yixiao Xu", "Lei Zhang", "Taiyu Han", "Xinshan Yang", "Yingni Wang", "Mingxuan Liu", "Shenghai Xin", "Linxun Liu", "Hongen Liao", "Guochen Ning"], "categories": ["cs.RO"], "published": "2025-10-09T11:43:29Z", "pdf": "https://arxiv.org/pdf/2510.08106v1", "abs": "https://arxiv.org/abs/2510.08106v1", "comment": null}
