{"id": "2510.12801v1", "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search", "summary": "Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search.", "authors": ["Kartik Narayan", "Yang Xu", "Tian Cao", "Kavya Nerella", "Vishal M. Patel", "Navid Shiee", "Peter Grasch", "Chao Jia", "Yinfei Yang", "Zhe Gan"], "categories": ["cs.CV", "cs.IR"], "published": "2025-10-14T17:59:58Z", "pdf": "https://arxiv.org/pdf/2510.12801v1", "abs": "https://arxiv.org/abs/2510.12801v1", "comment": null}
{"id": "2510.12798v1", "title": "Detect Anything via Next Point Prediction", "summary": "Object detection has long been dominated by traditional coordinate\nregression-based models, such as YOLO, DETR, and Grounding DINO. Although\nrecent efforts have attempted to leverage MLLMs to tackle this task, they face\nchallenges like low recall rate, duplicate predictions, coordinate\nmisalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a\n3B-scale MLLM that achieves state-of-the-art object perception performance. On\nbenchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or\nexceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot\nsetting. This is enabled by three key designs: 1) Task Formulation: we use\nspecial tokens to represent quantized coordinates from 0 to 999, reducing the\nmodel's learning difficulty and improving token efficiency for coordinate\nprediction; 2) Data Engines: we construct multiple data engines to generate\nhigh-quality grounding, referring, and pointing data, providing semantically\nrich supervision for training; \\3) Training Pipelines: we employ a two-stage\ntraining process, combining supervised fine-tuning on 22 million data with\nGRPO-based reinforcement post-training. This RL post-training leverages\ngeometry-aware rewards to effectively bridge the discrete-to-continuous\ncoordinate prediction gap, improve box accuracy, and mitigate undesirable\nbehaviors like duplicate predictions that stem from the teacher-guided nature\nof the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent\nlanguage understanding enables versatile capabilities such as object referring,\npointing, visual prompting, GUI grounding, spatial referring, OCR and\nkey-pointing, all systematically evaluated on dedicated benchmarks. We believe\nthat Rex-Omni paves the way for more versatile and language-aware visual\nperception systems.", "authors": ["Qing Jiang", "Junan Huo", "Xingyu Chen", "Yuda Xiong", "Zhaoyang Zeng", "Yihao Chen", "Tianhe Ren", "Junzhi Yu", "Lei Zhang"], "categories": ["cs.CV"], "published": "2025-10-14T17:59:54Z", "pdf": "https://arxiv.org/pdf/2510.12798v1", "abs": "https://arxiv.org/abs/2510.12798v1", "comment": "homepage: https://rex-omni.github.io/"}
{"id": "2510.12793v1", "title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution", "summary": "Existing Multimodal Large Language Models (MLLMs) suffer from increased\ninference costs due to the additional vision tokens introduced by image inputs.\nIn this work, we propose Visual Consistency Learning (ViCO), a novel training\nalgorithm that enables the model to represent images of varying semantic\ncomplexities using different numbers of vision tokens. The key idea behind our\nmethod is to employ multiple MLP connectors, each with a different image\ncompression ratio, to downsample the vision tokens based on the semantic\ncomplexity of the image. During training, we minimize the KL divergence between\nthe responses conditioned on different MLP connectors. At inference time, we\nintroduce an image router, termed Visual Resolution Router (ViR), that\nautomatically selects the appropriate compression rate for each image patch.\nCompared with existing dynamic high-resolution strategies, which adjust the\nnumber of visual tokens based on image resolutions, our method dynamically\nadapts the number of visual tokens according to semantic complexity.\nExperimental results demonstrate that our method can reduce the number of\nvision tokens by up to 50% while maintaining the model's perception, reasoning,\nand OCR capabilities. We hope this work will contribute to the development of\nmore efficient MLLMs. The code and models will be released to facilitate future\nresearch.", "authors": ["Long Cui", "Weiyun Wang", "Jie Shao", "Zichen Wen", "Gen Luo", "Linfeng Zhang", "Yanting Zhang", "Yu Qiao", "Wenhai Wang"], "categories": ["cs.CV"], "published": "2025-10-14T17:58:10Z", "pdf": "https://arxiv.org/pdf/2510.12793v1", "abs": "https://arxiv.org/abs/2510.12793v1", "comment": null}
{"id": "2510.12789v1", "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation", "summary": "Although recent advances in visual generation have been remarkable, most\nexisting architectures still depend on distinct encoders for images and text.\nThis separation constrains diffusion models' ability to perform cross-modal\nreasoning and knowledge transfer. Prior attempts to bridge this gap often use\nthe last layer information from VLM, employ multiple visual encoders, or train\nlarge unified models jointly for text and image generation, which demands\nsubstantial computational resources and large-scale data, limiting its\naccessibility.We present UniFusion, a diffusion-based generative model\nconditioned on a frozen large vision-language model (VLM) that serves as a\nunified multimodal encoder. At the core of UniFusion is the Layerwise Attention\nPooling (LAP) mechanism that extracts both high level semantics and low level\ndetails from text and visual tokens of a frozen VLM to condition a diffusion\ngenerative model. We demonstrate that LAP outperforms other shallow fusion\narchitectures on text-image alignment for generation and faithful transfer of\nvisual information from VLM to the diffusion model which is key for editing. We\npropose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),\nwhich conditions a diffusion transformer (DiT) only on the text tokens\ngenerated by the VLM during in-model prompt rewriting. VERIFI combines the\nalignment of the conditioning distribution with the VLM's reasoning\ncapabilities for increased capabilities and flexibility at inference. In\naddition, finetuning on editing task not only improves text-image alignment for\ngeneration, indicative of cross-modality knowledge transfer, but also exhibits\ntremendous generalization capabilities. Our model when trained on single image\nediting, zero-shot generalizes to multiple image references further motivating\nthe unified encoder design of UniFusion.", "authors": ["Kevin Li", "Manuel Brack", "Sudeep Katakol", "Hareesh Ravi", "Ajinkya Kale"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-10-14T17:57:56Z", "pdf": "https://arxiv.org/pdf/2510.12789v1", "abs": "https://arxiv.org/abs/2510.12789v1", "comment": "Project page at https://thekevinli.github.io/unifusion/"}
{"id": "2510.12784v1", "title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models", "summary": "Recently, remarkable progress has been made in Unified Multimodal Models\n(UMMs), which integrate vision-language generation and understanding\ncapabilities within a single framework. However, a significant gap exists where\na model's strong visual understanding often fails to transfer to its visual\ngeneration. A model might correctly understand an image based on user\ninstructions, yet be unable to generate a faithful image from text prompts.\nThis phenomenon directly raises a compelling question: Can a model achieve\nself-improvement by using its understanding module to reward its generation\nmodule? To bridge this gap and achieve self-improvement, we introduce SRUM, a\nself-rewarding post-training framework that can be directly applied to existing\nUMMs of various designs. SRUM creates a feedback loop where the model's own\nunderstanding module acts as an internal ``evaluator'', providing corrective\nsignals to improve its generation module, without requiring additional\nhuman-labeled data. To ensure this feedback is comprehensive, we designed a\nglobal-local dual reward system. To tackle the inherent structural complexity\nof images, this system offers multi-scale guidance: a \\textbf{global reward}\nensures the correctness of the overall visual semantics and layout, while a\n\\textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads\nto powerful capabilities and shows strong generalization, boosting performance\non T2I-CompBench from 82.18 to \\textbf{88.37} and on T2I-ReasonBench from 43.82\nto \\textbf{46.75}. Overall, our work establishes a powerful new paradigm for\nenabling a UMMs' understanding module to guide and enhance its own generation\nvia self-rewarding.", "authors": ["Weiyang Jin", "Yuwei Niu", "Jiaqi Liao", "Chengqi Duan", "Aoxue Li", "Shenghua Gao", "Xihui Liu"], "categories": ["cs.CV", "cs.CL", "I.4.0"], "published": "2025-10-14T17:56:11Z", "pdf": "https://arxiv.org/pdf/2510.12784v1", "abs": "https://arxiv.org/abs/2510.12784v1", "comment": "20 pages, 8 figures, webpage can be seen in\n  https://waynejin0918.github.io/srum_web/"}
{"id": "2510.12750v1", "title": "VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural Heritage", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\ncapabilities in joint visual and linguistic tasks. However, existing Visual\nQuestion Answering (VQA) benchmarks often fail to evaluate deep semantic\nunderstanding, particularly in complex domains like visual art analysis.\nConfined to simple syntactic structures and surface-level attributes, these\nquestions fail to capture the diversity and depth of human visual inquiry. This\nlimitation incentivizes models to exploit statistical shortcuts rather than\nengage in visual reasoning. To address this gap, we introduce VQArt-Bench, a\nnew, large-scale VQA benchmark for the cultural heritage domain. This benchmark\nis constructed using a novel multi-agent pipeline where specialized agents\ncollaborate to generate nuanced, validated, and linguistically diverse\nquestions. The resulting benchmark is structured along relevant visual\nunderstanding dimensions that probe a model's ability to interpret symbolic\nmeaning, narratives, and complex visual relationships. Our evaluation of 14\nstate-of-the-art MLLMs on this benchmark reveals significant limitations in\ncurrent models, including a surprising weakness in simple counting tasks and a\nclear performance gap between proprietary and open-source models.", "authors": ["A. Alfarano", "L. Venturoli", "D. Negueruela del Castillo"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-10-14T17:29:52Z", "pdf": "https://arxiv.org/pdf/2510.12750v1", "abs": "https://arxiv.org/abs/2510.12750v1", "comment": null}
{"id": "2510.12749v1", "title": "SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding", "summary": "The scene perception, understanding, and simulation are fundamental\ntechniques for embodied-AI agents, while existing solutions are still prone to\nsegmentation deficiency, dynamic objects' interference, sensor data sparsity,\nand view-limitation problems. This paper proposes a novel framework, named\nSPORTS, for holistic scene understanding via tightly integrating Video Panoptic\nSegmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks into\nan iterative and unified perspective. Firstly, VPS designs an adaptive\nattention-based geometric fusion mechanism to align cross-frame features via\nenrolling the pose, depth, and optical flow modality, which automatically\nadjust feature maps for different decoding stages. And a post-matching strategy\nis integrated to improve identities tracking. In VO, panoptic segmentation\nresults from VPS are combined with the optical flow map to improve the\nconfidence estimation of dynamic objects, which enhances the accuracy of the\ncamera pose estimation and completeness of the depth map generation via the\nlearning-based paradigm. Furthermore, the point-based rendering of SR is\nbeneficial from VO, transforming sparse point clouds into neural fields to\nsynthesize high-fidelity RGB views and twin panoptic views. Extensive\nexperiments on three public datasets demonstrate that our attention-based\nfeature fusion outperforms most existing state-of-the-art methods on the\nodometry, tracking, segmentation, and novel view synthesis tasks.", "authors": ["Zhiliu Yang", "Jinyu Dai", "Jianyuan Zhang", "Zhu Yang"], "categories": ["cs.CV"], "published": "2025-10-14T17:28:19Z", "pdf": "https://arxiv.org/pdf/2510.12749v1", "abs": "https://arxiv.org/abs/2510.12749v1", "comment": "Accepted by IEEE Transactions on Multimedia"}
{"id": "2510.12733v1", "title": "HYPE: Hybrid Planning with Ego Proposal-Conditioned Predictions", "summary": "Safe and interpretable motion planning in complex urban environments needs to\nreason about bidirectional multi-agent interactions. This reasoning requires to\nestimate the costs of potential ego driving maneuvers. Many existing planners\ngenerate initial trajectories with sampling-based methods and refine them by\noptimizing on learned predictions of future environment states, which requires\na cost function that encodes the desired vehicle behavior. Designing such a\ncost function can be very challenging, especially if a wide range of complex\nurban scenarios has to be considered. We propose HYPE: HYbrid Planning with Ego\nproposal-conditioned predictions, a planner that integrates multimodal\ntrajectory proposals from a learned proposal model as heuristic priors into a\nMonte Carlo Tree Search (MCTS) refinement. To model bidirectional interactions,\nwe introduce an ego-conditioned occupancy prediction model, enabling\nconsistent, scene-aware reasoning. Our design significantly simplifies cost\nfunction design in refinement by considering proposal-driven guidance,\nrequiring only minimalistic grid-based cost terms. Evaluations on large-scale\nreal-world benchmarks nuPlan and DeepUrban show that HYPE effectively achieves\nstate-of-the-art performance, especially in safety and adaptability.", "authors": ["Hang Yu", "Julian Jordan", "Julian Schmidt", "Silvan Lindner", "Alessandro Canevaro", "Wilhelm Stork"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "published": "2025-10-14T17:11:04Z", "pdf": "https://arxiv.org/pdf/2510.12733v1", "abs": "https://arxiv.org/abs/2510.12733v1", "comment": null}
{"id": "2510.12724v1", "title": "T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial Transformation for Cross-Embodiment Dexterous Grasping", "summary": "Dexterous grasping remains a central challenge in robotics due to the\ncomplexity of its high-dimensional state and action space. We introduce T(R,O)\nGrasp, a diffusion-based framework that efficiently generates accurate and\ndiverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,\na unified representation that models spatial transformations between robotic\nhands and objects while encoding their geometric properties. A graph diffusion\nmodel, coupled with an efficient inverse kinematics solver, supports both\nunconditioned and conditioned grasp synthesis. Extensive experiments on a\ndiverse set of dexterous hands show that T(R,O) Grasp achieves average success\nrate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps per\nsecond on an NVIDIA A100 40GB GPU, substantially outperforming existing\nbaselines. In addition, our approach is robust and generalizable across\nembodiments while significantly reducing memory consumption. More importantly,\nthe high inference speed enables closed-loop dexterous manipulation,\nunderscoring the potential of T(R,O) Grasp to scale into a foundation model for\ndexterous grasping.", "authors": ["Xin Fei", "Zhixuan Xu", "Huaicong Fang", "Tianrui Zhang", "Lin Shao"], "categories": ["cs.RO"], "published": "2025-10-14T17:06:00Z", "pdf": "https://arxiv.org/pdf/2510.12724v1", "abs": "https://arxiv.org/abs/2510.12724v1", "comment": "12 pages, 14 figures"}
{"id": "2510.12721v1", "title": "CARVQ: Corrective Adaptor with Group Residual Vector Quantization for LLM Embedding Compression", "summary": "Large Language Models (LLMs) typically rely on a large number of parameters\nfor token embedding, leading to substantial storage requirements and memory\nfootprints. In particular, LLMs deployed on edge devices are memory-bound, and\nreducing the memory footprint by compressing the embedding layer not only frees\nup the memory bandwidth but also speeds up inference. To address this, we\nintroduce CARVQ, a post-training novel Corrective Adaptor combined with group\nResidual Vector Quantization. CARVQ relies on the composition of both linear\nand non-linear maps and mimics the original model embedding to compress to\napproximately 1.6 bits without requiring specialized hardware to support\nlower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B,\nLLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B\nand Phi-4, evaluating on common generative, discriminative, math and reasoning\ntasks. We show that in most cases, CARVQ can achieve lower average\nbitwidth-per-parameter while maintaining reasonable perplexity and accuracy\ncompared to scalar quantization. Our contributions include a novel compression\ntechnique that is compatible with state-of-the-art transformer quantization\nmethods and can be seamlessly integrated into any hardware supporting 4-bit\nmemory to reduce the model's memory footprint in memory-constrained devices.\nThis work demonstrates a crucial step toward the efficient deployment of LLMs\non edge devices.", "authors": ["Dayin Gou", "Sanghyun Byun", "Nilesh Malpeddi", "Gabrielle De Micheli", "Prathamesh Vaste", "Jacob Song", "Woo Seong Chung"], "categories": ["cs.LG"], "published": "2025-10-14T17:00:13Z", "pdf": "https://arxiv.org/pdf/2510.12721v1", "abs": "https://arxiv.org/abs/2510.12721v1", "comment": "Accepted at EMNLP Findings 2025"}
{"id": "2510.12720v1", "title": "Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception", "summary": "Fine-grained perception of multimodal information is critical for advancing\nhuman-AI interaction. With recent progress in audio-visual technologies, Omni\nLanguage Models (OLMs), capable of processing audio and video signals in\nparallel, have emerged as a promising paradigm for achieving richer\nunderstanding and reasoning. However, their capacity to capture and describe\nfine-grained details remains limited explored. In this work, we present a\nsystematic and comprehensive investigation of omni detailed perception from the\nperspectives of the data pipeline, models, and benchmark. We first identify an\ninherent \"co-growth\" between detail and hallucination in current OLMs. To\naddress this, we propose Omni-Detective, an agentic data generation pipeline\nintegrating tool-calling, to autonomously produce highly detailed yet minimally\nhallucinatory multimodal data. Based on the data generated with Omni-Detective,\nwe train two captioning models: Audio-Captioner for audio-only detailed\nperception, and Omni-Captioner for audio-visual detailed perception. Under the\ncascade evaluation protocol, Audio-Captioner achieves the best performance on\nMMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and\ndelivering performance comparable to Gemini 2.5 Pro. On existing detailed\ncaptioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and\nachieves the best trade-off between detail and hallucination on the\nvideo-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni\ndetailed perception, we design Omni-Cloze, a novel cloze-style evaluation for\ndetailed audio, visual, and audio-visual captioning that ensures stable,\nefficient, and reliable assessment. Experimental results and analysis\ndemonstrate the effectiveness of Omni-Detective in generating high-quality\ndetailed captions, as well as the superiority of Omni-Cloze in evaluating such\ndetailed captions.", "authors": ["Ziyang Ma", "Ruiyang Xu", "Zhenghao Xing", "Yunfei Chu", "Yuxuan Wang", "Jinzheng He", "Jin Xu", "Pheng-Ann Heng", "Kai Yu", "Junyang Lin", "Eng Siong Chng", "Xie Chen"], "categories": ["cs.CL", "cs.CV", "cs.MM", "cs.SD"], "published": "2025-10-14T17:00:09Z", "pdf": "https://arxiv.org/pdf/2510.12720v1", "abs": "https://arxiv.org/abs/2510.12720v1", "comment": "https://github.com/ddlBoJack/Omni-Captioner"}
{"id": "2510.12712v1", "title": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning", "summary": "Multimodal Large Language Models (MLLMs) are increasingly applied in\nreal-world scenarios where user-provided images are often imperfect, requiring\nactive image manipulations such as cropping, editing, or enhancement to uncover\nsalient visual cues. Beyond static visual perception, MLLMs must also think\nwith images: dynamically transforming visual content and integrating it with\nother tools to solve complex tasks. However, this shift from treating vision as\npassive context to a manipulable cognitive workspace remains underexplored.\nMost existing benchmarks still follow a think about images paradigm, where\nimages are regarded as static inputs. To address this gap, we introduce IRIS,\nan Interactive Reasoning with Images and Systems that evaluates MLLMs' ability\nto perceive, transform, and reason across complex visual-textual tasks under\nthe think with images paradigm. IRIS comprises 1,204 challenging, open-ended\nvision tasks (603 single-turn, 601 multi-turn) spanning across five diverse\ndomains, each paired with detailed rubrics to enable systematic evaluation. Our\nevaluation shows that current MLLMs struggle with tasks requiring effective\nintegration of vision and general-purpose tools. Even the strongest model\n(GPT-5-think) reaches only 18.68% pass rate. We further observe divergent\ntool-use behaviors, with OpenAI models benefiting from diverse image\nmanipulations while Gemini-2.5-pro shows no improvement. By introducing the\nfirst benchmark centered on think with images, IRIS offers critical insights\nfor advancing visual intelligence in MLLMs.", "authors": ["Xingang Guo", "Utkarsh Tyagi", "Advait Gosai", "Paula Vergara", "Ernesto Gabriel Hernández Montoya", "Chen Bo Calvin Zhang", "Bin Hu", "Yunzhong He", "Bing Liu", "Rakshith Sharma Srinivasa"], "categories": ["cs.CV", "cs.AI"], "published": "2025-10-14T16:50:49Z", "pdf": "https://arxiv.org/pdf/2510.12712v1", "abs": "https://arxiv.org/abs/2510.12712v1", "comment": null}
{"id": "2510.12710v1", "title": "Reflection-Based Task Adaptation for Self-Improving VLA", "summary": "Pre-trained Vision-Language-Action (VLA) models represent a major leap\ntowards general-purpose robots, yet efficiently adapting them to novel,\nspecific tasks in-situ remains a significant hurdle. While reinforcement\nlearning (RL) is a promising avenue for such adaptation, the process often\nsuffers from low efficiency, hindering rapid task mastery. We introduce\nReflective Self-Adaptation, a framework for rapid, autonomous task adaptation\nwithout human intervention. Our framework establishes a self-improving loop\nwhere the agent learns from its own experience to enhance both strategy and\nexecution.\n  The core of our framework is a dual-pathway architecture that addresses the\nfull adaptation lifecycle. First, a Failure-Driven Reflective RL pathway\nenables rapid learning by using the VLM's causal reasoning to automatically\nsynthesize a targeted, dense reward function from failure analysis. This\nprovides a focused learning signal that significantly accelerates policy\nexploration. However, optimizing such proxy rewards introduces a potential risk\nof \"reward hacking,\" where the agent masters the reward function but fails the\nactual task. To counteract this, our second pathway, Success-Driven\nQuality-Guided SFT, grounds the policy in holistic success. It identifies and\nselectively imitates high-quality successful trajectories, ensuring the agent\nremains aligned with the ultimate task goal. This pathway is strengthened by a\nconditional curriculum mechanism to aid initial exploration.\n  We conduct experiments in challenging manipulation tasks. The results\ndemonstrate that our framework achieves faster convergence and higher final\nsuccess rates compared to representative baselines. Our work presents a robust\nsolution for creating self-improving agents that can efficiently and reliably\nadapt to new environments.", "authors": ["Baicheng Li", "Dong Wu", "Zike Yan", "Xinchen Liu", "Zecui Zeng", "Lusong Li", "Hongbin Zha"], "categories": ["cs.RO"], "published": "2025-10-14T16:44:39Z", "pdf": "https://arxiv.org/pdf/2510.12710v1", "abs": "https://arxiv.org/abs/2510.12710v1", "comment": null}
{"id": "2510.12709v2", "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model", "summary": "Multimodal embedding models aim to yield informative unified representations\nthat empower diverse cross-modal tasks. Despite promising developments in the\nevolution from CLIP-based dual-tower architectures to large vision-language\nmodels, prior works still face unavoidable challenges in real-world\napplications and business scenarios, such as the limited modality support,\nunstable training mechanisms, and industrial domain gaps. In this work, we\nintroduce SAIL-Embedding, an omni-modal embedding foundation model that\naddresses these issues through tailored training strategies and architectural\ndesign. In the optimization procedure, we propose a multi-stage training scheme\nto boost the multifaceted effectiveness of representation learning.\nSpecifically, the content-aware progressive training aims to enhance the\nmodel's adaptability to diverse downstream tasks and master enriched\ncross-modal proficiency. The collaboration-aware recommendation enhancement\ntraining further adapts multimodal representations for recommendation scenarios\nby distilling knowledge from sequence-to-item and ID-to-item embeddings while\nmining user historical interests. Concurrently, we develop the stochastic\nspecialization and dataset-driven pattern matching to strengthen model training\nflexibility and generalizability. Experimental results show that SAIL-Embedding\nachieves SOTA performance compared to other methods in different retrieval\ntasks. In online experiments across various real-world scenarios integrated\nwith our model, we observe a significant increase in Lifetime (LT), which is a\ncrucial indicator for the recommendation experience. For instance, the model\ndelivers the 7-day LT gain of +0.5% in the Douyin-Selected scenario. For the\nDouyin feed rank model, the match features produced by SAIL-Embedding yield a\n+0.1% AUC gain.", "authors": ["Lin Lin", "Jiefeng Long", "Zhihe Wan", "Yuchi Wang", "Dingkang Yang", "Shuang Yang", "Yueyang Yao", "Xu Chen", "Zirui Guo", "Shengqiang Li", "Weiran Li", "Hanyu Li", "Yaling Mou", "Yan Qiu", "Haiyang Yu", "Xiao Liang", "Hongsheng Li", "Chao Feng"], "categories": ["cs.IR", "cs.CV"], "published": "2025-10-14T16:43:22Z", "pdf": "https://arxiv.org/pdf/2510.12709v2", "abs": "https://arxiv.org/abs/2510.12709v2", "comment": "Technical Report"}
{"id": "2510.12706v1", "title": "GKLO representations of twisted Yangians in type $\\mathsf{AI}$ and quantizations of symmetric quotients of the affine Grassmannian", "summary": "We construct an analogue of Gerasimov-Kharchev-Lebedev-Oblezin (GKLO)\nrepresentations for twisted Yangians of type $\\mathsf{AI}$, using the recently\nfound current presentation of these algebras due to Lu, Wang and Zhang. These\nnew representations allow us to define interesting truncations of twisted\nYangians, which, in the spirit of Ciccoli-Drinfeld-Gavarini quantum duality,\nreflect the Poisson geometry of homogeneous spaces. As our main result, we\nprove that a truncated twisted Yangian quantizes a scheme supported on\nquotients of transverse slices in the affine Grassmannian.", "authors": ["Robin Bartlett", "Tomasz Przezdziecki", "Lukas Tappeiner"], "categories": ["math.RT", "math.AG", "math.QA"], "published": "2025-10-14T16:40:18Z", "pdf": "https://arxiv.org/pdf/2510.12706v1", "abs": "https://arxiv.org/abs/2510.12706v1", "comment": null}
{"id": "2510.12693v1", "title": "ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning", "summary": "Recent advances in embodied AI highlight the potential of vision language\nmodels (VLMs) as agents capable of perception, reasoning, and interaction in\ncomplex environments. However, top-performing systems rely on large-scale\nmodels that are costly to deploy, while smaller VLMs lack the necessary\nknowledge and skills to succeed. To bridge this gap, we present\n\\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates\nprior knowledge learning and online reinforcement learning (RL). The first\nstage, \\textit{Embodied Prior Learning}, distills foundational knowledge from\nthree types of data: (1) Trajectory-Augmented Priors, which enrich existing\ntrajectory data with structured reasoning generated by stronger models; (2)\nEnvironment-Anchored Priors, which provide in-environment knowledge and\ngrounding supervision; and (3) External Knowledge Priors, which transfer\ngeneral knowledge from out-of-environment datasets. In the second stage, we\ndevelop an online RL pipeline that builds on these priors to further enhance\nagent performance. To overcome the inherent challenges in agent RL, including\nlong horizons, sparse rewards, and training instability, we introduce three key\ndesigns: self-summarization for context management, dense reward shaping, and\nturn-level policy optimization. Extensive experiments on both high-level\nplanning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate\nthat ERA-3B surpasses both prompting-based large models and previous\ntraining-based baselines. Specifically, it achieves overall improvements of\n8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits\nstrong generalization to unseen tasks. Overall, ERA offers a practical path\ntoward scalable embodied intelligence, providing methodological insights for\nfuture embodied AI systems.", "authors": ["Hanyang Chen", "Mark Zhao", "Rui Yang", "Qinwei Ma", "Ke Yang", "Jiarui Yao", "Kangrui Wang", "Hao Bai", "Zhenhailong Wang", "Rui Pan", "Mengchao Zhang", "Jose Barreiros", "Aykut Onol", "ChengXiang Zhai", "Heng Ji", "Manling Li", "Huan Zhang", "Tong Zhang"], "categories": ["cs.AI"], "published": "2025-10-14T16:25:46Z", "pdf": "https://arxiv.org/pdf/2510.12693v1", "abs": "https://arxiv.org/abs/2510.12693v1", "comment": null}
{"id": "2510.12684v1", "title": "Autonomous Legged Mobile Manipulation for Lunar Surface Operations via Constrained Reinforcement Learning", "summary": "Robotics plays a pivotal role in planetary science and exploration, where\nautonomous and reliable systems are crucial due to the risks and challenges\ninherent to space environments. The establishment of permanent lunar bases\ndemands robotic platforms capable of navigating and manipulating in the harsh\nlunar terrain. While wheeled rovers have been the mainstay for planetary\nexploration, their limitations in unstructured and steep terrains motivate the\nadoption of legged robots, which offer superior mobility and adaptability. This\npaper introduces a constrained reinforcement learning framework designed for\nautonomous quadrupedal mobile manipulators operating in lunar environments. The\nproposed framework integrates whole-body locomotion and manipulation\ncapabilities while explicitly addressing critical safety constraints, including\ncollision avoidance, dynamic stability, and power efficiency, in order to\nensure robust performance under lunar-specific conditions, such as reduced\ngravity and irregular terrain. Experimental results demonstrate the framework's\neffectiveness in achieving precise 6D task-space end-effector pose tracking,\nachieving an average positional accuracy of 4 cm and orientation accuracy of\n8.1 degrees. The system consistently respects both soft and hard constraints,\nexhibiting adaptive behaviors optimized for lunar gravity conditions. This work\neffectively bridges adaptive learning with essential mission-critical safety\nrequirements, paving the way for advanced autonomous robotic explorers for\nfuture lunar missions.", "authors": ["Alvaro Belmonte-Baeza", "Miguel Cazorla", "Gabriel J. García", "Carlos J. Pérez-Del-Pulgar", "Jorge Pomares"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "published": "2025-10-14T16:21:34Z", "pdf": "https://arxiv.org/pdf/2510.12684v1", "abs": "https://arxiv.org/abs/2510.12684v1", "comment": "This is the authors version of the paper accepted for publication in\n  The IEEE International Conference on Space Robotics 2025. The final version\n  link will be added here after conference proceedings are published"}
{"id": "2510.12677v1", "title": "Decoding Multimode Gottesman-Kitaev-Preskill Codes with Noisy Auxiliary States", "summary": "In order to achieve fault-tolerant quantum computing, we make use of quantum\nerror correction schemes designed to protect the logical information of the\nsystem from decoherence. A promising way to preserve such information is to use\nthe multimode Gottesman-Kitaev-Preskill (GKP) encoding, which encodes logical\nqubits into several harmonic oscillators. In this work, we focus on decoding\nthe measurements obtained from Steane-type quantum error correction protocols\nfor multimode GKP codes. We propose a decoder that considers the noise present\non the auxiliary states, more specifically by tracking the correlations between\nerrors on different modes spreading throughout the error-correction circuit. We\nshow that leveraging the correlations between measurement results and the\nactual error affecting the multimode GKP state can decrease the logical error\nprobability by at least an order of magnitude, yielding more robust quantum\ncomputation.", "authors": ["Marc-Antoine Roy", "Thomas Pousset", "Baptiste Royer"], "categories": ["quant-ph"], "published": "2025-10-14T16:14:33Z", "pdf": "https://arxiv.org/pdf/2510.12677v1", "abs": "https://arxiv.org/abs/2510.12677v1", "comment": "40 pages, 10 figures"}
{"id": "2510.12662v1", "title": "Maximal Adaptation, Minimal Guidance: Permissive Reactive Robot Task Planning with Humans in the Loop", "summary": "We present a novel framework for human-robot \\emph{logical} interaction that\nenables robots to reliably satisfy (infinite horizon) temporal logic tasks\nwhile effectively collaborating with humans who pursue independent and unknown\ntasks. The framework combines two key capabilities: (i) \\emph{maximal\nadaptation} enables the robot to adjust its strategy \\emph{online} to exploit\nhuman behavior for cooperation whenever possible, and (ii) \\emph{minimal\ntunable feedback} enables the robot to request cooperation by the human online\nonly when necessary to guarantee progress. This balance minimizes human-robot\ninterference, preserves human autonomy, and ensures persistent robot task\nsatisfaction even under conflicting human goals. We validate the approach in a\nreal-world block-manipulation task with a Franka Emika Panda robotic arm and in\nthe Overcooked-AI benchmark, demonstrating that our method produces rich,\n\\emph{emergent} cooperative behaviors beyond the reach of existing approaches,\nwhile maintaining strong formal guarantees.", "authors": ["Oz Gitelson", "Satya Prakash Nayak", "Ritam Raha", "Anne-Kathrin Schmuck"], "categories": ["cs.RO"], "published": "2025-10-14T15:58:42Z", "pdf": "https://arxiv.org/pdf/2510.12662v1", "abs": "https://arxiv.org/abs/2510.12662v1", "comment": null}
{"id": "2510.12630v1", "title": "Designing Tools with Control Confidence", "summary": "Prehistoric humans invented stone tools for specialized tasks by not just\nmaximizing the tool's immediate goal-completion accuracy, but also increasing\ntheir confidence in the tool for later use under similar settings. This factor\ncontributed to the increased robustness of the tool, i.e., the least\nperformance deviations under environmental uncertainties. However, the current\nautonomous tool design frameworks solely rely on performance optimization,\nwithout considering the agent's confidence in tool use for repeated use. Here,\nwe take a step towards filling this gap by i) defining an optimization\nframework for task-conditioned autonomous hand tool design for robots, where\nii) we introduce a neuro-inspired control confidence term into the optimization\nroutine that helps the agent to design tools with higher robustness. Through\nrigorous simulations using a robotic arm, we show that tools designed with\ncontrol confidence as the objective function are more robust to environmental\nuncertainties during tool use than a pure accuracy-driven objective. We further\nshow that adding control confidence to the objective function for tool design\nprovides a balance between the robustness and goal accuracy of the designed\ntools under control perturbations. Finally, we show that our CMAES-based\nevolutionary optimization strategy for autonomous tool design outperforms other\nstate-of-the-art optimizers by designing the optimal tool within the fewest\niterations. Code: https://github.com/ajitham123/Tool_design_control_confidence.", "authors": ["Ajith Anil Meera", "Abian Torres", "Pablo Lanillos"], "categories": ["cs.RO", "cs.AI"], "published": "2025-10-14T15:27:27Z", "pdf": "https://arxiv.org/pdf/2510.12630v1", "abs": "https://arxiv.org/abs/2510.12630v1", "comment": null}
{"id": "2510.12604v1", "title": "SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through Rate Prediction in E-commerce SEarch", "summary": "With the rise of modern search and recommendation platforms, insufficient\ncollaborative information of cold-start items exacerbates the Matthew effect of\nexisting platform items, challenging platform diversity and becoming a\nlongstanding issue. Existing methods align items' side content with\ncollaborative information to transfer collaborative signals from\nhigh-popularity items to cold-start items. However, these methods fail to\naccount for the asymmetry between collaboration and content, nor the\nfine-grained differences among items. To address these issues, we propose\nSMILE, an item representation enhancement approach based on fused alignment of\nsemantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and\ncollaborative information, followed by a two-step alignment: RQ encoding\ntransfers shared collaborative signals across items, while OPQ encoding learns\ndifferentiated information of items. Comprehensive offline experiments on\nlarge-scale industrial datasets demonstrate superiority of SMILE, and rigorous\nonline A/B tests confirm statistically significant improvements: item CTR\n+1.66%, buyers +1.57%, and order volume +2.17%.", "authors": ["Qihang Zhao", "Zhongbo Sun", "Xiaoyang Zheng", "Xian Guo", "Siyuan Wang", "Zihan Liang", "Mingcan Peng", "Ben Chen", "Chenyi Lei"], "categories": ["cs.IR", "cs.AI"], "published": "2025-10-14T14:58:50Z", "pdf": "https://arxiv.org/pdf/2510.12604v1", "abs": "https://arxiv.org/abs/2510.12604v1", "comment": null}
{"id": "2510.12603v1", "title": "Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space", "summary": "Multimodal reasoning aims to enhance the capabilities of MLLMs by\nincorporating intermediate reasoning steps before reaching the final answer. It\nhas evolved from text-only reasoning to the integration of visual information,\nenabling the thought process to be conveyed through both images and text.\nDespite its effectiveness, current multimodal reasoning methods depend on\nexplicit reasoning steps that require labor-intensive vision-text annotations\nand inherently introduce significant inference latency. To address these\nissues, we introduce multimodal latent reasoning with the advantages of\nmultimodal representation, reduced annotation, and inference efficiency. To\nfacilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),\nwhich injects both visual and textual information in the reasoning process\nwithin the latent space. Specifically, IVT-LR represents each reasoning step by\ncombining two implicit parts: latent text (the hidden states from the previous\nstep) and latent vision (a set of selected image embeddings). We further\nintroduce a progressive multi-stage training strategy to enable MLLMs to\nperform the above multimodal latent reasoning steps. Experiments on M3CoT and\nScienceQA demonstrate that our IVT-LR method achieves an average performance\nincrease of 5.45% in accuracy, while simultaneously achieving a speed increase\nof over 5 times compared to existing approaches. Code available at\nhttps://github.com/FYYDCC/IVT-LR.", "authors": ["Chao Chen", "Zhixin Ma", "Yongqi Li", "Yupeng Hu", "Yinwei Wei", "Wenjie Li", "Liqiang Nie"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "published": "2025-10-14T14:58:25Z", "pdf": "https://arxiv.org/pdf/2510.12603v1", "abs": "https://arxiv.org/abs/2510.12603v1", "comment": null}
{"id": "2510.12580v1", "title": "Anonymous leadership and stochastic resonance in collectives of self-propelled robots", "summary": "We investigate the influence of an anonymous leader on a collective of\nself-propelled robots using Kilobot experiments and numerical simulations. A\nsingle leader alternated deterministically between clockwise and\ncounterclockwise motion, while the other robots followed a stochastic majority\nrule. Although the leader does not change global order, it induces correlations\nwith the collective response that peak at intermediate perturbation levels,\nresembling stochastic resonance. Simulations confirm that this resonance occurs\nwhen the leader's reversal period matches the mean residence time of the\nunperturbed system. Our results contribute to understanding decision-making in\nactive matter and suggesting principles for steering robotic swarms with\nminimal leadership input.", "authors": ["Manuel Dizenhaus", "Franco De Simone", "German A. Patterson"], "categories": ["cond-mat.soft", "cond-mat.other"], "published": "2025-10-14T14:39:03Z", "pdf": "https://arxiv.org/pdf/2510.12580v1", "abs": "https://arxiv.org/abs/2510.12580v1", "comment": null}
{"id": "2510.12549v1", "title": "Privacy-Preserving Distributed Estimation with Limited Data Rate", "summary": "This paper focuses on the privacy-preserving distributed estimation problem\nwith a limited data rate, where the observations are the sensitive information.\nSpecifically, a binary-valued quantizer-based privacy-preserving distributed\nestimation algorithm is developed, which improves the algorithm's\nprivacy-preserving capability and simultaneously reduces the communication\ncosts. The algorithm's privacy-preserving capability, measured by the Fisher\ninformation matrix, is dynamically enhanced over time. Notably, the Fisher\ninformation matrix of the output signals with respect to the sensitive\ninformation converges to zero at a polynomial rate, and the improvement in\nprivacy brought by the quantizers is quantitatively characterized as a\nmultiplicative effect. Regarding the communication costs, each sensor transmits\nonly 1 bit of information to its neighbours at each time step. Additionally,\nthe assumption on the negligible quantization error for real-valued messages is\nnot required. While achieving the requirements of privacy preservation and\nreducing communication costs, the algorithm ensures that its estimates converge\nalmost surely to the true value of the unknown parameter by establishing a\nco-design guideline for the time-varying privacy noises and step-sizes. A\npolynomial almost sure convergence rate is obtained, and then the trade-off\nbetween privacy and convergence rate is established. Numerical examples\ndemonstrate the main results.", "authors": ["Jieming Ke", "Jimin Wang", "Ji-Feng Zhang"], "categories": ["eess.SY", "cs.SY"], "published": "2025-10-14T14:13:32Z", "pdf": "https://arxiv.org/pdf/2510.12549v1", "abs": "https://arxiv.org/abs/2510.12549v1", "comment": null}
{"id": "2510.12528v1", "title": "Two-stream network-driven vision-based tactile sensor for object feature extraction and fusion perception", "summary": "Tactile perception is crucial for embodied intelligent robots to recognize\nobjects. Vision-based tactile sensors extract object physical attributes\nmultidimensionally using high spatial resolution; however, this process\ngenerates abundant redundant information. Furthermore, single-dimensional\nextraction, lacking effective fusion, fails to fully characterize object\nattributes. These challenges hinder the improvement of recognition accuracy. To\naddress this issue, this study introduces a two-stream network feature\nextraction and fusion perception strategy for vision-based tactile systems.\nThis strategy employs a distributed approach to extract internal and external\nobject features. It obtains depth map information through three-dimensional\nreconstruction while simultaneously acquiring hardness information by measuring\ncontact force data. After extracting features with a convolutional neural\nnetwork (CNN), weighted fusion is applied to create a more informative and\neffective feature representation. In standard tests on objects of varying\nshapes and hardness, the force prediction error is 0.06 N (within a 12 N\nrange). Hardness recognition accuracy reaches 98.0%, and shape recognition\naccuracy reaches 93.75%. With fusion algorithms, object recognition accuracy in\nactual grasping scenarios exceeds 98.5%. Focused on object physical attributes\nperception, this method enhances the artificial tactile system ability to\ntransition from perception to cognition, enabling its use in embodied\nperception applications.", "authors": ["Muxing Huang", "Zibin Chen", "Weiliang Xu", "Zilan Li", "Yuanzhi Zhou", "Guoyuan Zhou", "Wenjing Chen", "Xinming Li"], "categories": ["cs.RO", "physics.app-ph"], "published": "2025-10-14T13:53:38Z", "pdf": "https://arxiv.org/pdf/2510.12528v1", "abs": "https://arxiv.org/abs/2510.12528v1", "comment": null}
{"id": "2510.12509v1", "title": "Automated Behavior Planning for Fruit Tree Pruning via Redundant Robot Manipulators: Addressing the Behavior Planning Challenge", "summary": "Pruning is an essential agricultural practice for orchards. Proper pruning\ncan promote healthier growth and optimize fruit production throughout the\norchard's lifespan. Robot manipulators have been developed as an automated\nsolution for this repetitive task, which typically requires seasonal labor with\nspecialized skills. While previous research has primarily focused on the\nchallenges of perception, the complexities of manipulation are often\noverlooked. These challenges involve planning and control in both joint and\nCartesian spaces to guide the end-effector through intricate, obstructive\nbranches. Our work addresses the behavior planning challenge for a robotic\npruning system, which entails a multi-level planning problem in environments\nwith complex collisions. In this paper, we formulate the planning problem for a\nhigh-dimensional robotic arm in a pruning scenario, investigate the system's\nintrinsic redundancies, and propose a comprehensive pruning workflow that\nintegrates perception, modeling, and holistic planning. In our experiments, we\ndemonstrate that more comprehensive planning methods can significantly enhance\nthe performance of the robotic manipulator. Finally, we implement the proposed\nworkflow on a real-world robot. As a result, this work complements previous\nefforts on robotic pruning and motivates future research and development in\nplanning for pruning applications.", "authors": ["Gaoyuan Liu", "Bas Boom", "Naftali Slob", "Yuri Durodié", "Ann Nowé", "Bram Vanderborght"], "categories": ["cs.RO"], "published": "2025-10-14T13:40:40Z", "pdf": "https://arxiv.org/pdf/2510.12509v1", "abs": "https://arxiv.org/abs/2510.12509v1", "comment": null}
{"id": "2510.12498v1", "title": "Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation", "summary": "Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable,\ndecision-relevant models of cell state from multimodal, multiscale\nmeasurements. Recent studies have introduced single-cell and spatial foundation\nmodels, improved cross-modality alignment, scaled perturbation atlases, and\nexplored pathway-level readouts. Nevertheless, although held-out validation is\nstandard practice, evaluations remain predominantly within single datasets and\nsettings; evidence indicates that transport across laboratories and platforms\nis often limited, that some data splits are vulnerable to leakage and coverage\nbias, and that dose, time and combination effects are not yet systematically\nhandled. Cross-scale coupling also remains constrained, as anchors linking\nmolecular, cellular and tissue levels are sparse, and alignment to scientific\nor clinical readouts varies across studies. We propose a model-agnostic\nCell-State Latent (CSL) perspective that organizes learning via an operator\ngrammar: measurement, lift/project for cross-scale coupling, and intervention\nfor dosing and scheduling. This view motivates a decision-aligned evaluation\nblueprint across modality, scale, context and intervention, and emphasizes\nfunction-space readouts such as pathway activity, spatial neighborhoods and\nclinically relevant endpoints. We recommend operator-aware data design,\nleakage-resistant partitions, and transparent calibration and reporting to\nenable reproducible, like-for-like comparisons.", "authors": ["Chengpeng Hu", "Calvin Yu-Chian Chen"], "categories": ["cs.AI"], "published": "2025-10-14T13:31:40Z", "pdf": "https://arxiv.org/pdf/2510.12498v1", "abs": "https://arxiv.org/abs/2510.12498v1", "comment": null}
{"id": "2510.12483v1", "title": "Fast Visuomotor Policy for Robotic Manipulation", "summary": "We present a fast and effective policy framework for robotic manipulation,\nnamed Energy Policy, designed for high-frequency robotic tasks and\nresource-constrained systems. Unlike existing robotic policies, Energy Policy\nnatively predicts multimodal actions in a single forward pass, enabling\nhigh-precision manipulation at high speed. The framework is built upon two core\ncomponents. First, we adopt the energy score as the learning objective to\nfacilitate multimodal action modeling. Second, we introduce an energy MLP to\nimplement the proposed objective while keeping the architecture simple and\nefficient. We conduct comprehensive experiments in both simulated environments\nand real-world robotic tasks to evaluate the effectiveness of Energy Policy.\nThe results show that Energy Policy matches or surpasses the performance of\nstate-of-the-art manipulation methods while significantly reducing\ncomputational overhead. Notably, on the MimicGen benchmark, Energy Policy\nachieves superior performance with at a faster inference compared to existing\napproaches.", "authors": ["Jingkai Jia", "Tong Yang", "Xueyao Chen", "Chenhuan Liu", "Wenqiang Zhang"], "categories": ["cs.RO", "cs.CV"], "published": "2025-10-14T13:18:45Z", "pdf": "https://arxiv.org/pdf/2510.12483v1", "abs": "https://arxiv.org/abs/2510.12483v1", "comment": null}
{"id": "2510.12482v1", "title": "A Text-Image Fusion Method with Data Augmentation Capabilities for Referring Medical Image Segmentation", "summary": "Deep learning relies heavily on data augmentation to mitigate limited data,\nespecially in medical imaging. Recent multimodal learning integrates text and\nimages for segmentation, known as referring or text-guided image segmentation.\nHowever, common augmentations like rotation and flipping disrupt spatial\nalignment between image and text, weakening performance. To address this, we\npropose an early fusion framework that combines text and visual features before\naugmentation, preserving spatial consistency. We also design a lightweight\ngenerator that projects text embeddings into visual space, bridging semantic\ngaps. Visualization of generated pseudo-images shows accurate region\nlocalization. Our method is evaluated on three medical imaging tasks and four\nsegmentation frameworks, achieving state-of-the-art results. Code is publicly\navailable on GitHub: https://github.com/11yxk/MedSeg_EarlyFusion.", "authors": ["Shurong Chai", "Rahul Kumar JAIN", "Rui Xu", "Shaocong Mo", "Ruibo Hou", "Shiyu Teng", "Jiaqing Liu", "Lanfen Lin", "Yen-Wei Chen"], "categories": ["cs.CV", "cs.AI"], "published": "2025-10-14T13:18:34Z", "pdf": "https://arxiv.org/pdf/2510.12482v1", "abs": "https://arxiv.org/abs/2510.12482v1", "comment": null}
{"id": "2510.12477v1", "title": "A Task-Efficient Reinforcement Learning Task-Motion Planner for Safe Human-Robot Cooperation", "summary": "In a Human-Robot Cooperation (HRC) environment, safety and efficiency are the\ntwo core properties to evaluate robot performance. However, safety mechanisms\nusually hinder task efficiency since human intervention will cause backup\nmotions and goal failures of the robot. Frequent motion replanning will\nincrease the computational load and the chance of failure. In this paper, we\npresent a hybrid Reinforcement Learning (RL) planning framework which is\ncomprised of an interactive motion planner and a RL task planner. The RL task\nplanner attempts to choose statistically safe and efficient task sequences\nbased on the feedback from the motion planner, while the motion planner keeps\nthe task execution process collision-free by detecting human arm motions and\ndeploying new paths when the previous path is not valid anymore. Intuitively,\nthe RL agent will learn to avoid dangerous tasks, while the motion planner\nensures that the chosen tasks are safe. The proposed framework is validated on\nthe cobot in both simulation and the real world, we compare the planner with\nhard-coded task motion planning methods. The results show that our planning\nframework can 1) react to uncertain human motions at both joint and task\nlevels; 2) reduce the times of repeating failed goal commands; 3) reduce the\ntotal number of replanning requests.", "authors": ["Gaoyuan Liu", "Joris de Winter", "Kelly Merckaert", "Denis Steckelmacher", "Ann Nowe", "Bram Vanderborght"], "categories": ["cs.RO"], "published": "2025-10-14T13:11:09Z", "pdf": "https://arxiv.org/pdf/2510.12477v1", "abs": "https://arxiv.org/abs/2510.12477v1", "comment": null}
{"id": "2510.12474v1", "title": "SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression", "summary": "Large language models (LLMs) generate high-dimensional embeddings that\ncapture rich semantic and syntactic information. However, high-dimensional\nembeddings exacerbate computational complexity and storage requirements,\nthereby hindering practical deployment. To address these challenges, we propose\na novel training framework named Sequential Matryoshka Embedding Compression\n(SMEC). This framework introduces the Sequential Matryoshka Representation\nLearning(SMRL) method to mitigate gradient variance during training, the\nAdaptive Dimension Selection (ADS) module to reduce information degradation\nduring dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module\nto enhance unsupervised learning between high- and low-dimensional embeddings.\nExperiments on image, text, and multimodal datasets demonstrate that SMEC\nachieves significant dimensionality reduction while maintaining performance.\nFor instance, on the BEIR dataset, our approach improves the performance of\ncompressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points\ncompared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.", "authors": ["Biao Zhang", "Lixin Chen", "Tong Liu", "Bo Zheng"], "categories": ["cs.CL", "cs.LG"], "published": "2025-10-14T13:04:22Z", "pdf": "https://arxiv.org/pdf/2510.12474v1", "abs": "https://arxiv.org/abs/2510.12474v1", "comment": "Accepted by EMNLP2025"}
{"id": "2510.12448v1", "title": "Marginally deformed AdS$_5$/CFT$_4$ and spindle-like orbifolds", "summary": "In this thesis, the AdS/CFT correspondence is used as a tool to explore novel\nAdS$_5$ Supergravity backgrounds (containing five-dimensional Anti-de Sitter\nspacetime) and their dual (four dimensional) Conformal Field Theory\ndescriptions. In order to obtain precise results, both conformal symmetry and\nsupersymmetry play an important role. However, in order to align with\nexperimental observation, supersymmetry must be broken at low energies. In the\nabsence of supersymmetry, finding deformations of a CFT which are marginal in\nnature (preserving conformal symmetry) is currently not well understood.\nNevertheless, the solutions presented in this work may offer the best evidence\nto date of such deformations.\n  Multi-parameter families of non-supersymmetric type IIA and type IIB AdS$_5$\nsolutions are presented, promoting to $\\mathcal{N} = 1$ supersymmetry in some\nspecial cases. Contained within these solutions is an existing class of\n$\\mathcal{N} = 2$ type IIA solutions, recovered in one example when both\ndeformation parameters are fixed to zero. The supersymmetry is studied using\nthe method of G-structures, with the boundaries of the solutions carefully\ninvestigated -- uncovering orbifold singularities within some solutions. In the\ntype IIA backgrounds, both the spindle and its higher dimensional analogue play\nan important role, giving rise to rational quantization of charge. All\nparameters drop out of a quantity called the holographic central charge,\npointing to marginal deformations of the existing $d=4$ $\\mathcal{N} = 2$ long\nlinear quiver CFT. These marginal deformations are proposed to correspond to\nsoft-SUSY breaking, with the Lagrangian nature of the CFT broken in some cases.", "authors": ["Paul Merrikin"], "categories": ["hep-th"], "published": "2025-10-14T12:28:28Z", "pdf": "https://arxiv.org/pdf/2510.12448v1", "abs": "https://arxiv.org/abs/2510.12448v1", "comment": "PhD Thesis"}
{"id": "2510.12427v1", "title": "Phase Transitions of the Additive Uniform Noise Channel with Peak Amplitude and Cost Constraint", "summary": "Under which condition is quantization optimal? We address this question in\nthe context of the additive uniform noise channel under peak amplitude and cost\nconstraints. We compute analytically the capacity-achieving input distribution\nas a function of the noise level, the average cost constraint, and the\ncurvature of the cost function. We find that when the cost function is concave,\nthe capacity-achieving input distribution is discrete, whereas when the cost\nfunction is convex and the cost constraint is active, the support of the\ncapacity-achieving input distribution spans the entire interval. For the cases\nof a discrete capacity-achieving input distribution, we derive the analytical\nexpressions for the capacity of the channel.", "authors": ["Jonas Stapmanns", "Catarina Dias", "Luke Eilers", "Tobias Kühn", "Jean-Pascal Pfister"], "categories": ["cs.IT", "math.IT", "94A15, 94A40"], "published": "2025-10-14T12:04:58Z", "pdf": "https://arxiv.org/pdf/2510.12427v1", "abs": "https://arxiv.org/abs/2510.12427v1", "comment": "This work was presented in part at the IEEE International Symposium\n  on Information Theory (ISIT) 2025. 18 pages, 8 figures"}
{"id": "2510.12419v1", "title": "M3D-skin: Multi-material 3D-printed Tactile Sensor with Hierarchical Infill Structures for Pressure Sensing", "summary": "Tactile sensors have a wide range of applications, from utilization in\nrobotic grippers to human motion measurement. If tactile sensors could be\nfabricated and integrated more easily, their applicability would further\nexpand. In this study, we propose a tactile sensor-M3D-skin-that can be easily\nfabricated with high versatility by leveraging the infill patterns of a\nmulti-material fused deposition modeling (FDM) 3D printer as the sensing\nprinciple. This method employs conductive and non-conductive flexible filaments\nto create a hierarchical structure with a specific infill pattern. The flexible\nhierarchical structure deforms under pressure, leading to a change in\nelectrical resistance, enabling the acquisition of tactile information. We\nmeasure the changes in characteristics of the proposed tactile sensor caused by\nmodifications to the hierarchical structure. Additionally, we demonstrate the\nfabrication and use of a multi-tile sensor. Furthermore, as applications, we\nimplement motion pattern measurement on the sole of a foot, integration with a\nrobotic hand, and tactile-based robotic operations. Through these experiments,\nwe validate the effectiveness of the proposed tactile sensor.", "authors": ["Shunnosuke Yoshimura", "Kento Kawaharazuka", "Kei Okada"], "categories": ["cs.RO"], "published": "2025-10-14T11:56:01Z", "pdf": "https://arxiv.org/pdf/2510.12419v1", "abs": "https://arxiv.org/abs/2510.12419v1", "comment": "Accepted to IROS2025, Website:\n  https://ssk-yoshimura.github.io/M3D-skin/"}
{"id": "2510.12403v1", "title": "Robot Learning: A Tutorial", "summary": "Robot learning is at an inflection point, driven by rapid advancements in\nmachine learning and the growing availability of large-scale robotics data.\nThis shift from classical, model-based methods to data-driven, learning-based\nparadigms is unlocking unprecedented capabilities in autonomous systems. This\ntutorial navigates the landscape of modern robot learning, charting a course\nfrom the foundational principles of Reinforcement Learning and Behavioral\nCloning to generalist, language-conditioned models capable of operating across\ndiverse tasks and even robot embodiments. This work is intended as a guide for\nresearchers and practitioners, and our goal is to equip the reader with the\nconceptual understanding and practical tools necessary to contribute to\ndevelopments in robot learning, with ready-to-use examples implemented in\n$\\texttt{lerobot}$.", "authors": ["Francesco Capuano", "Caroline Pascal", "Adil Zouitine", "Thomas Wolf", "Michel Aractingi"], "categories": ["cs.RO", "cs.LG"], "published": "2025-10-14T11:36:46Z", "pdf": "https://arxiv.org/pdf/2510.12403v1", "abs": "https://arxiv.org/abs/2510.12403v1", "comment": "Tutorial on Robot Learning using LeRobot, the end-to-end robot\n  learning library developed by Hugging Face"}
{"id": "2510.12392v1", "title": "Improving Generative Behavior Cloning via Self-Guidance and Adaptive Chunking", "summary": "Generative Behavior Cloning (GBC) is a simple yet effective framework for\nrobot learning, particularly in multi-task settings. Recent GBC methods often\nemploy diffusion policies with open-loop (OL) control, where actions are\ngenerated via a diffusion process and executed in multi-step chunks without\nreplanning. While this approach has demonstrated strong success rates and\ngeneralization, its inherent stochasticity can result in erroneous action\nsampling, occasionally leading to unexpected task failures. Moreover, OL\ncontrol suffers from delayed responses, which can degrade performance in noisy\nor dynamic environments. To address these limitations, we propose two novel\ntechniques to enhance the consistency and reactivity of diffusion policies: (1)\nself-guidance, which improves action fidelity by leveraging past observations\nand implicitly promoting future-aware behavior; and (2) adaptive chunking,\nwhich selectively updates action sequences when the benefits of reactivity\noutweigh the need for temporal consistency. Extensive experiments show that our\napproach substantially improves GBC performance across a wide range of\nsimulated and real-world robotic manipulation tasks. Our code is available at\nhttps://github.com/junhyukso/SGAC", "authors": ["Junhyuk So", "Chiwoong Lee", "Shinyoung Lee", "Jungseul Ok", "Eunhyeok Park"], "categories": ["cs.RO", "cs.LG"], "published": "2025-10-14T11:16:34Z", "pdf": "https://arxiv.org/pdf/2510.12392v1", "abs": "https://arxiv.org/abs/2510.12392v1", "comment": "Accepted at NeurIPS25"}
{"id": "2510.12386v1", "title": "Hey Dashboard!: Supporting Voice, Text, and Pointing Modalities in Dashboard Onboarding", "summary": "Visualization dashboards are regularly used for data exploration and\nanalysis, but their complex interactions and interlinked views often require\ntime-consuming onboarding sessions from dashboard authors. Preparing these\nonboarding materials is labor-intensive and requires manual updates when\ndashboards change. Recent advances in multimodal interaction powered by large\nlanguage models (LLMs) provide ways to support self-guided onboarding. We\npresent DIANA (Dashboard Interactive Assistant for Navigation and Analysis), a\nmultimodal dashboard assistant that helps users for navigation and guided\nanalysis through chat, audio, and mouse-based interactions. Users can choose\nany interaction modality or a combination of them to onboard themselves on the\ndashboard. Each modality highlights relevant dashboard features to support user\norientation. Unlike typical LLM systems that rely solely on text-based chat,\nDIANA combines multiple modalities to provide explanations directly in the\ndashboard interface. We conducted a qualitative user study to understand the\nuse of different modalities for different types of onboarding tasks and their\ncomplexities.", "authors": ["Vaishali Dhanoa", "Gabriela Molina León", "Eve Hoggan", "Eduard Gröller", "Marc Streit", "Niklas Elmqvist"], "categories": ["cs.HC"], "published": "2025-10-14T11:10:35Z", "pdf": "https://arxiv.org/pdf/2510.12386v1", "abs": "https://arxiv.org/abs/2510.12386v1", "comment": null}
{"id": "2510.12370v1", "title": "Controlling Intent Expressiveness in Robot Motion with Diffusion Models", "summary": "Legibility of robot motion is critical in human-robot interaction, as it\nallows humans to quickly infer a robot's intended goal. Although traditional\ntrajectory generation methods typically prioritize efficiency, they often fail\nto make the robot's intentions clear to humans. Meanwhile, existing approaches\nto legible motion usually produce only a single \"most legible\" trajectory,\noverlooking the need to modulate intent expressiveness in different contexts.\nIn this work, we propose a novel motion generation framework that enables\ncontrollable legibility across the full spectrum, from highly legible to highly\nambiguous motions. We introduce a modeling approach based on an Information\nPotential Field to assign continuous legibility scores to trajectories, and\nbuild upon it with a two-stage diffusion framework that first generates paths\nat specified legibility levels and then translates them into executable robot\nactions. Experiments in both 2D and 3D reaching tasks demonstrate that our\napproach produces diverse and controllable motions with varying degrees of\nlegibility, while achieving performance comparable to SOTA. Code and project\npage: https://legibility-modulator.github.io.", "authors": ["Wenli Shi", "Clemence Grislain", "Olivier Sigaud", "Mohamed Chetouani"], "categories": ["cs.RO"], "published": "2025-10-14T10:38:15Z", "pdf": "https://arxiv.org/pdf/2510.12370v1", "abs": "https://arxiv.org/abs/2510.12370v1", "comment": "Using diffusion models trained on quality diversity datasets for\n  generating robot motions with adjustable legibility levels"}
{"id": "2510.12369v1", "title": "A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph Representation Learning", "summary": "Recent progress in language and vision foundation models demonstrates the\nimportance of discrete token interfaces that transform complex inputs into\ncompact sequences for large-scale modeling. Extending this paradigm to graphs\nrequires a tokenization scheme that handles non-Euclidean structures and\nmulti-scale dependencies efficiently. Existing approaches to graph\ntokenization, linearized, continuous, and quantized, remain limited in\nadaptability and efficiency. In particular, most current quantization-based\ntokenizers organize hierarchical information in fixed or task-agnostic ways,\nwhich may either over-represent or under-utilize structural cues, and lack the\nability to dynamically reweight contributions from different levels without\nretraining the encoder. This work presents a hierarchical quantization\nframework that introduces a self-weighted mechanism for task-adaptive\naggregation across multiple scales. The proposed method maintains a frozen\nencoder while modulating information flow through a lightweight gating process,\nenabling parameter-efficient adaptation to diverse downstream tasks.\nExperiments on benchmark datasets for node classification and link prediction\ndemonstrate consistent improvements over strong baselines under comparable\ncomputational budgets.", "authors": ["Yang Xiang", "Li Fan", "Chenke Yin", "Chengtao Ji"], "categories": ["cs.IR"], "published": "2025-10-14T10:36:43Z", "pdf": "https://arxiv.org/pdf/2510.12369v1", "abs": "https://arxiv.org/abs/2510.12369v1", "comment": null}
{"id": "2510.12363v1", "title": "Pretraining in Actor-Critic Reinforcement Learning for Robot Motion Control", "summary": "The pretraining-finetuning paradigm has facilitated numerous transformative\nadvancements in artificial intelligence research in recent years. However, in\nthe domain of reinforcement learning (RL) for robot motion control, individual\nskills are often learned from scratch despite the high likelihood that some\ngeneralizable knowledge is shared across all task-specific policies belonging\nto a single robot embodiment. This work aims to define a paradigm for\npretraining neural network models that encapsulate such knowledge and can\nsubsequently serve as a basis for warm-starting the RL process in classic\nactor-critic algorithms, such as Proximal Policy Optimization (PPO). We begin\nwith a task-agnostic exploration-based data collection algorithm to gather\ndiverse, dynamic transition data, which is then used to train a Proprioceptive\nInverse Dynamics Model (PIDM) through supervised learning. The pretrained\nweights are loaded into both the actor and critic networks to warm-start the\npolicy optimization of actual tasks. We systematically validated our proposed\nmethod on seven distinct robot motion control tasks, showing significant\nbenefits to this initialization strategy. Our proposed approach on average\nimproves sample efficiency by 40.1% and task performance by 7.5%, compared to\nrandom initialization. We further present key ablation studies and empirical\nanalyses that shed light on the mechanisms behind the effectiveness of our\nmethod.", "authors": ["Jiale Fan", "Andrei Cramariuc", "Tifanny Portela", "Marco Hutter"], "categories": ["cs.RO", "cs.LG"], "published": "2025-10-14T10:25:40Z", "pdf": "https://arxiv.org/pdf/2510.12363v1", "abs": "https://arxiv.org/abs/2510.12363v1", "comment": "Submitted to ICLR 2026"}
{"id": "2510.12346v1", "title": "PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing", "summary": "Recently, biped robot walking technology has been significantly developed,\nmainly in the context of a bland walking scheme. To emulate human walking,\nrobots need to step on the positions they see in unknown spaces accurately. In\nthis paper, we present PolyMap, a perception-based locomotion planning\nframework for humanoid robots to climb stairs. Our core idea is to build a\nreal-time polygonal staircase plane semantic map, followed by a footstep planar\nusing these polygonal plane segments. These plane segmentation and visual\nodometry are done by multi-sensor fusion(LiDAR, RGB-D camera and IMUs). The\nproposed framework is deployed on a NVIDIA Orin, which performs 20-30 Hz\nwhole-body motion planning output. Both indoor and outdoor real-scene\nexperiments indicate that our method is efficient and robust for humanoid robot\nstair climbing.", "authors": ["Bingquan Li", "Ning Wang", "Tianwei Zhang", "Zhicheng He", "Yucong Wu"], "categories": ["cs.RO"], "published": "2025-10-14T10:00:05Z", "pdf": "https://arxiv.org/pdf/2510.12346v1", "abs": "https://arxiv.org/abs/2510.12346v1", "comment": null}
{"id": "2510.12340v1", "title": "Achieving Meaningful Collaboration: Worker-centered Design of a Physical Human-Robot Collaborative Blending Task", "summary": "The use of robots in industrial settings continues to grow, driven by the\nneed to address complex societal challenges such as labor shortages, aging\npopulations, and ever-increasing production demands. In this abstract, we\nadvocate for (and demonstrate) a transdisciplinary approach when considering\nrobotics in the workplace. Transdisciplinarity emphasizes the integration of\nacademic research with pragmatic expertise and embodied experiential knowledge,\nthat prioritize values such as worker wellbeing and job attractiveness. In the\nfollowing, we describe an ongoing multi-pronged effort to explore the potential\nof collaborative robots in the context of airplane engine repair and\nmaintenance operations.", "authors": ["Nicky Mol", "Luka Peternel", "Alessandro Ianniello", "Denis Zatyagov", "Auke Nachenius", "Stephan Balvert", "J. Micah Prendergast", "Sara Muscolo", "Olger Siebinga", "Eva Verhoef", "Deborah Forster", "David A. Abbink"], "categories": ["cs.RO"], "published": "2025-10-14T09:51:22Z", "pdf": "https://arxiv.org/pdf/2510.12340v1", "abs": "https://arxiv.org/abs/2510.12340v1", "comment": "3 pages, 1 figure, ICRA@40 (Extended abstract)"}
{"id": "2510.12332v1", "title": "Shape-Aware Whole-Body Control for Continuum Robots with Application in Endoluminal Surgical Robotics", "summary": "This paper presents a shape-aware whole-body control framework for\ntendon-driven continuum robots with direct application to endoluminal surgical\nnavigation. Endoluminal procedures, such as bronchoscopy, demand precise and\nsafe navigation through tortuous, patient-specific anatomy where conventional\ntip-only control often leads to wall contact, tissue trauma, or failure to\nreach distal targets. To address these challenges, our approach combines a\nphysics-informed backbone model with residual learning through an Augmented\nNeural ODE, enabling accurate shape estimation and efficient Jacobian\ncomputation. A sampling-based Model Predictive Path Integral (MPPI) controller\nleverages this representation to jointly optimize tip tracking, backbone\nconformance, and obstacle avoidance under actuation constraints. A task manager\nfurther enhances adaptability by allowing real-time adjustment of objectives,\nsuch as wall clearance or direct advancement, during tele-operation. Extensive\nsimulation studies demonstrate millimeter-level accuracy across diverse\nscenarios, including trajectory tracking, dynamic obstacle avoidance, and\nshape-constrained reaching. Real-robot experiments on a bronchoscopy phantom\nvalidate the framework, showing improved lumen-following accuracy, reduced wall\ncontacts, and enhanced adaptability compared to joystick-only navigation and\nexisting baselines. These results highlight the potential of the proposed\nframework to increase safety, reliability, and operator efficiency in minimally\ninvasive endoluminal surgery, with broader applicability to other confined and\nsafety-critical environments.", "authors": ["Mohammadreza Kasaei", "Mostafa Ghobadi", "Mohsen Khadem"], "categories": ["cs.RO"], "published": "2025-10-14T09:43:47Z", "pdf": "https://arxiv.org/pdf/2510.12332v1", "abs": "https://arxiv.org/abs/2510.12332v1", "comment": null}
{"id": "2510.12325v1", "title": "Causal Inspired Multi Modal Recommendation", "summary": "Multimodal recommender systems enhance personalized recommendations in\ne-commerce and online advertising by integrating visual, textual, and user-item\ninteraction data. However, existing methods often overlook two critical biases:\n(i) modal confounding, where latent factors (e.g., brand style or product\ncategory) simultaneously drive multiple modalities and influence user\npreference, leading to spurious feature-preference associations; (ii)\ninteraction bias, where genuine user preferences are mixed with noise from\nexposure effects and accidental clicks. To address these challenges, we propose\na Causal-inspired multimodal Recommendation framework. Specifically, we\nintroduce a dual-channel cross-modal diffusion module to identify hidden modal\nconfounders, utilize back-door adjustment with hierarchical matching and\nvector-quantized codebooks to block confounding paths, and apply front-door\nadjustment combined with causal topology reconstruction to build a deconfounded\ncausal subgraph. Extensive experiments on three real-world e-commerce datasets\ndemonstrate that our method significantly outperforms state-of-the-art\nbaselines while maintaining strong interpretability.", "authors": ["Jie Yang", "Chenyang Gu", "Zixuan Liu"], "categories": ["cs.IR", "cs.AI"], "published": "2025-10-14T09:29:07Z", "pdf": "https://arxiv.org/pdf/2510.12325v1", "abs": "https://arxiv.org/abs/2510.12325v1", "comment": null}
{"id": "2510.12323v1", "title": "RAG-Anything: All-in-One RAG Framework", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm\nfor expanding Large Language Models beyond their static training limitations.\nHowever, a critical misalignment exists between current RAG capabilities and\nreal-world information environments. Modern knowledge repositories are\ninherently multimodal, containing rich combinations of textual content, visual\nelements, structured tables, and mathematical expressions. Yet existing RAG\nframeworks are limited to textual content, creating fundamental gaps when\nprocessing multimodal documents. We present RAG-Anything, a unified framework\nthat enables comprehensive knowledge retrieval across all modalities. Our\napproach reconceptualizes multimodal content as interconnected knowledge\nentities rather than isolated data types. The framework introduces dual-graph\nconstruction to capture both cross-modal relationships and textual semantics\nwithin a unified representation. We develop cross-modal hybrid retrieval that\ncombines structural knowledge navigation with semantic matching. This enables\neffective reasoning over heterogeneous content where relevant evidence spans\nmultiple modalities. RAG-Anything demonstrates superior performance on\nchallenging multimodal benchmarks, achieving significant improvements over\nstate-of-the-art methods. Performance gains become particularly pronounced on\nlong documents where traditional approaches fail. Our framework establishes a\nnew paradigm for multimodal knowledge access, eliminating the architectural\nfragmentation that constrains current systems. Our framework is open-sourced\nat: https://github.com/HKUDS/RAG-Anything.", "authors": ["Zirui Guo", "Xubin Ren", "Lingrui Xu", "Jiahao Zhang", "Chao Huang"], "categories": ["cs.AI"], "published": "2025-10-14T09:25:35Z", "pdf": "https://arxiv.org/pdf/2510.12323v1", "abs": "https://arxiv.org/abs/2510.12323v1", "comment": null}
{"id": "2510.12299v1", "title": "An Empirical Study for Representations of Videos in Video Question Answering via MLLMs", "summary": "Multimodal large language models have recently achieved remarkable progress\nin video question answering (VideoQA) by jointly processing visual, textual,\nand audio information. However, it remains unclear which video representations\nare most effective for MLLMs, and how different modalities balance task\naccuracy against computational efficiency. In this work, we present a\ncomprehensive empirical study of video representation methods for VideoQA with\nMLLMs. We systematically evaluate single modality inputs question only,\nsubtitles, visual frames, and audio signals as well as multimodal combinations,\non two widely used benchmarks: VideoMME and LongVideoBench. Our results show\nthat visual frames substantially enhance accuracy but impose heavy costs in GPU\nmemory and inference latency, while subtitles provide a lightweight yet\neffective alternative, particularly for long videos. These findings highlight\nclear trade-offs between effectiveness and efficiency and provide practical\ninsights for designing resource-aware MLLM-based VideoQA systems.", "authors": ["Zhi Li", "Yanan Wang", "Hao Niu", "Julio Vizcarra", "Masato Taya"], "categories": ["cs.IR", "I.2.10"], "published": "2025-10-14T09:02:22Z", "pdf": "https://arxiv.org/pdf/2510.12299v1", "abs": "https://arxiv.org/abs/2510.12299v1", "comment": "6 pages, 3 figures"}
{"id": "2510.12287v1", "title": "Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector", "summary": "Vision Language Models (VLMs) have achieved impressive progress in multimodal\nreasoning; yet, they remain vulnerable to hallucinations, where outputs are not\ngrounded in visual evidence. In this paper, we investigate a previously\noverlooked setting: logo hallucination, where models generate brand names or\ntextual content despite logos containing no visible words. Using curated splits\nof pure symbols, hybrids, and text-bearing logos, as well as the challenging\nHard-60 subset, we systematically measure hallucination across leading VLMs. We\nfurther probe robustness through nine structured perturbations and show that\nhallucinations persist even under strong distortions, with occlusion exposing\nthe sharpest weaknesses. Embedding-level analysis with open-weight LLaVA\ndemonstrates that hallucination is tied to a small subset of projector\ndimensions, and targeted ablation substantially reduces errors while preserving\nOCR accuracy. Together, these findings reveal that VLMs often rely on symbolic\npriors rather than genuine glyph perception, particularly for iconic circular\nlogos, and that projector subspaces play a decisive role in this failure mode.\nOur work contributes both a novel diagnostic lens and actionable mitigation\ninsights, highlighting projector disentanglement and OCR-guided decoding as\npromising directions for building more trustworthy multimodal systems.", "authors": ["Sifan Li", "Hongkai Chen", "Yujun Cai", "Qingwen Ye", "Liyang Chen", "Junsong Yuan", "Yiwei Wang"], "categories": ["cs.CV", "cs.CL"], "published": "2025-10-14T08:42:58Z", "pdf": "https://arxiv.org/pdf/2510.12287v1", "abs": "https://arxiv.org/abs/2510.12287v1", "comment": null}
{"id": "2510.12276v1", "title": "Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model", "summary": "Vision-language-action (VLA) models have recently shown strong potential in\nenabling robots to follow language instructions and execute precise actions.\nHowever, most VLAs are built upon vision-language models pretrained solely on\n2D data, which lack accurate spatial awareness and hinder their ability to\noperate in the 3D physical world. Existing solutions attempt to incorporate\nexplicit 3D sensor inputs such as depth maps or point clouds, but these\napproaches face challenges due to sensor noise, hardware heterogeneity, and\nincomplete depth coverage in existing datasets. Alternative methods that\nestimate 3D cues from 2D images also suffer from the limited performance of\ndepth estimators.We propose Spatial Forcing (SF), a simple yet effective\nalignment strategy that implicitly forces VLA models to develop spatial\ncomprehension capabilities without relying on explicit 3D inputs or depth\nestimators. SF aligns intermediate visual embeddings of VLAs with geometric\nrepresentations produced by pretrained 3D foundation models. By enforcing\nalignment at intermediate layers, SF guides VLAs to encode richer spatial\nrepresentations that enhance action precision.Extensive experiments in\nsimulation and real-world environments demonstrate that SF achieves\nstate-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further\naccelerates training by up to 3.8x and improves data efficiency across diverse\nrobotic tasks. Project page is at https://spatial-forcing.github.io/", "authors": ["Fuhao Li", "Wenxuan Song", "Han Zhao", "Jingbo Wang", "Pengxiang Ding", "Donglin Wang", "Long Zeng", "Haoang Li"], "categories": ["cs.RO"], "published": "2025-10-14T08:27:10Z", "pdf": "https://arxiv.org/pdf/2510.12276v1", "abs": "https://arxiv.org/abs/2510.12276v1", "comment": null}
{"id": "2510.12267v1", "title": "SpineBench: Benchmarking Multimodal LLMs for Spinal Pathology Analysis", "summary": "With the increasing integration of Multimodal Large Language Models (MLLMs)\ninto the medical field, comprehensive evaluation of their performance in\nvarious medical domains becomes critical. However, existing benchmarks\nprimarily assess general medical tasks, inadequately capturing performance in\nnuanced areas like the spine, which relies heavily on visual input. To address\nthis, we introduce SpineBench, a comprehensive Visual Question Answering (VQA)\nbenchmark designed for fine-grained analysis and evaluation of MLLMs in the\nspinal domain. SpineBench comprises 64,878 QA pairs from 40,263 spine images,\ncovering 11 spinal diseases through two critical clinical tasks: spinal disease\ndiagnosis and spinal lesion localization, both in multiple-choice format.\nSpineBench is built by integrating and standardizing image-label pairs from\nopen-source spinal disease datasets, and samples challenging hard negative\noptions for each VQA pair based on visual similarity (similar but not the same\ndisease), simulating real-world challenging scenarios. We evaluate 12 leading\nMLLMs on SpineBench. The results reveal that these models exhibit poor\nperformance in spinal tasks, highlighting limitations of current MLLM in the\nspine domain and guiding future improvements in spinal medicine applications.\nSpineBench is publicly available at\nhttps://zhangchenghanyu.github.io/SpineBench.github.io/.", "authors": ["Chenghanyu Zhang", "Zekun Li", "Peipei Li", "Xing Cui", "Shuhan Xia", "Weixiang Yan", "Yiqiao Zhang", "Qianyu Zhuang"], "categories": ["cs.CV"], "published": "2025-10-14T08:19:22Z", "pdf": "https://arxiv.org/pdf/2510.12267v1", "abs": "https://arxiv.org/abs/2510.12267v1", "comment": "Proceedings of the 33rd ACM International Conference on\n  Multimedia,ACMMM 2025 Dataset Track"}
{"id": "2510.12260v1", "title": "AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive Multi-Modality Image Fusion", "summary": "Visible-infrared image fusion is crucial in key applications such as\nautonomous driving and nighttime surveillance. Its main goal is to integrate\nmultimodal information to produce enhanced images that are better suited for\ndownstream tasks. Although deep learning based fusion methods have made\nsignificant progress, mainstream unsupervised approaches still face serious\nchallenges in practical applications. Existing methods mostly rely on manually\ndesigned loss functions to guide the fusion process. However, these loss\nfunctions have obvious limitations. On one hand, the reference images\nconstructed by existing methods often lack details and have uneven brightness.\nOn the other hand, the widely used gradient losses focus only on gradient\nmagnitude. To address these challenges, this paper proposes an angle-based\nperception framework for spatial-sensitive image fusion (AngularFuse). At\nfirst, we design a cross-modal complementary mask module to force the network\nto learn complementary information between modalities. Then, a fine-grained\nreference image synthesis strategy is introduced. By combining Laplacian edge\nenhancement with adaptive histogram equalization, reference images with richer\ndetails and more balanced brightness are generated. Last but not least, we\nintroduce an angle-aware loss, which for the first time constrains both\ngradient magnitude and direction simultaneously in the gradient domain.\nAngularFuse ensures that the fused images preserve both texture intensity and\ncorrect edge orientation. Comprehensive experiments on the MSRS, RoadScene, and\nM3FD public datasets show that AngularFuse outperforms existing mainstream\nmethods with clear margin. Visual comparisons further confirm that our method\nproduces sharper and more detailed results in challenging scenes, demonstrating\nsuperior fusion capability.", "authors": ["Xiaopeng Liu", "Yupei Lin", "Sen Zhang", "Xiao Wang", "Yukai Shi", "Liang Lin"], "categories": ["cs.CV", "cs.LG", "eess.IV"], "published": "2025-10-14T08:13:15Z", "pdf": "https://arxiv.org/pdf/2510.12260v1", "abs": "https://arxiv.org/abs/2510.12260v1", "comment": "For the first time, angle-based perception was introduced into the\n  multi-modality image fusion task"}
